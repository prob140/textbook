<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Search the site</title>
  <meta name="description" content="This is an example book built with Jupyter Books.">

  <link rel="canonical" href="http://prob140.org/textbook/search">
  <link rel="alternate" type="application/rss+xml" title="Prob 140 Textbook" href="http://prob140.org/textbook/feed.xml">

  <meta property="og:url"         content="http://prob140.org/textbook/search" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Search the site" />
<meta property="og:description" content="This is an example book built with Jupyter Books." />
<meta property="og:image"       content="http://prob140.org/textbook/content/images/logo/logo.png" />

<meta name="twitter:card" content="summary">


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "http://prob140.org/textbook/search",
  "headline":
    "Search the site",
  "datePublished":
    "2019-09-02T11:27:06-07:00",
  "dateModified":
    "2019-09-02T11:27:06-07:00",
  "description":
    "This is an example book built with Jupyter Books.",
  "author": {
    "@type": "Person",
    "name": "Ani Adhikari"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "http://prob140.org/textbook",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "http://prob140.org/textbook",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/textbook/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/textbook/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<!-- (mostly) copied from nbconvert configuration -->
<!-- https://github.com/jupyter/nbconvert/blob/master/nbconvert/templates/html/mathjax.tpl -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true },
    }
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/textbook';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/textbook/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/textbook/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Selectors for elements on the page -->
  <script>
/**
 * Select various elements on the page for later use
 */

// IDs we'll attach to cells
const codeCellId = index => `codecell${index}`
const inputCellId = index => `inputcell${index}`

pageElements = {}

// All code cells
findCodeCells = function() {
    var codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre, div.text_cell_render div.highlight pre')
    pageElements['codeCells'] = codeCells;

    codeCells.forEach((codeCell, index) => {
      const id = codeCellId(index)
      codeCell.setAttribute('id', id)
    })
};

initFunction(findCodeCells);

// All cells in general
findInputCells = function() {
    var inputCells = document.querySelectorAll('div.jb_cell')
    pageElements['inputCells'] = inputCells;

    inputCells.forEach((inputCell, index) => {
        const id = inputCellId(index)
        inputCell.setAttribute('id', id)
    })
};

initFunction(findInputCells);
</script>

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->



  <!-- Load the auto-generating TOC -->
  <script src="/textbook/assets/js/tocbot.min.js"  type="text/javascript"></script>
<script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/textbook/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load custom website scripts -->
  <script src="/textbook/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/textbook/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/textbook/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("http://prob140.org") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("https://prob140.datahub.berkeley.edu", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/textbook/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/textbook/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  pageElements['codeCells'].forEach((codeCell) => {
    const id = codeCell.getAttribute('id')
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  <script>
    /**
    Add buttons to hide code cells
    */


    var setCodeCellVisibility = function (inputField, kind) {
        // Update the image and class for hidden
        var id = inputField.getAttribute('data-id');
        var codeCell = document.querySelector(`#${id} div.highlight`);

        if (kind === "visible") {
            codeCell.classList.remove('hidden');
            inputField.checked = true;
        } else {
            codeCell.classList.add('hidden');
            inputField.checked = false;
        }
    }

    var toggleCodeCellVisibility = function (event) {
        // The label is clicked, and now we decide what to do based on the input field's clicked status
        if (event.target.tagName === "LABEL") {
            var inputField = event.target.previousElementSibling;
        } else {
            // It is the span inside the target
            var inputField = event.target.parentElement.previousElementSibling;
        }

        if (inputField.checked === true) {
            setCodeCellVisibility(inputField, "visible");
        } else {
            setCodeCellVisibility(inputField, "hidden");
        }
    }


    // Button constructor
    const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

    var addHideButton = function () {
        // If a hide button is already added, don't add another
        if (document.querySelector('div.tag_hide_input input') !== null) {
            return;
        }

        // Find the input cells and add a hide button
        pageElements['inputCells'].forEach(function (inputCell) {
            if (!inputCell.classList.contains("tag_hide_input")) {
                // Skip the cell if it doesn't have a hidecode class
                return;
            }

            const id = inputCell.getAttribute('id')

            // Insert the button just inside the end of the next div
            inputCell.querySelector('div.input').insertAdjacentHTML('beforeend', hideCodeButton(id))

            // Set up the visibility toggle
            hideLink = document.querySelector(`#${id} div.inner_cell + input + label`);
            hideLink.addEventListener('click', toggleCodeCellVisibility)
        });
    }


    // Initialize the hide buttos
    var initHiddenCells = function () {
        // Add hide buttons to the cells
        addHideButton();

        // Toggle the code cells that should be hidden
        document.querySelectorAll('div.tag_hide_input input').forEach(function (item) {
            setCodeCellVisibility(item, 'hidden');
            item.checked = true;
        })
    }

    initFunction(initHiddenCells);

</script>

  <!-- Printing the screen -->
  <!-- Include nbinteract for interactive widgets -->
<script src="/textbook/assets/js/print.min.js"  type="text/javascript"></script>
<script>
printContent = () => {
    // MathJax displays a second version of any math for assistive devices etc.
    // This prevents double-rendering in the PDF output.
    var ignoreAssistList = [];
    assistives = document.querySelectorAll('.MathJax_Display span.MJX_Assistive_MathML').forEach((element, index) => {
        var thisId = 'MathJax-assistive-' + index.toString();
        element.setAttribute('id', thisId);
        ignoreAssistList.push(thisId)
    });

    // Print the actual content object
    printJS({
        printable: 'textbook_content',
        type: 'html',
        css: "/textbook/assets/css/styles.css",
        scanStyles: false,
        targetStyles: ["*"],
        ignoreElements: ignoreAssistList
    })
};

initPrint = () => {
    document.querySelector('#interact-button-print').addEventListener('click', printContent)
}

initFunction(initPrint)

</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="http://prob140.org/textbook/README.html"><img src="/textbook/content/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">Prob 140 Textbook</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="http://prob140.org"
        >
          
          Course Home
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/README.html"
        >
          
          Authors and License
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/To_the_Student.html"
        >
          
          To the Student
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
            1.
          
          Search
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__divider"></li>
        
      
      
        <li><h2 class="c-sidebar__title">Textbook</li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_01/00_Fundamentals.html"
        >
          
            2.
          
          Chapter 1: Fundamentals
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_01/01_Outcome_Space_and_Events.html"
                >
                  
                    2.1
                  
                  Outcome Space and Events
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_01/02_Equally_Likely_Outcomes.html"
                >
                  
                    2.2
                  
                  Equally Likely Outcomes
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_01/03_Collisions_in_Hashing.html"
                >
                  
                    2.3
                  
                  Collisions in Hashing
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_01/04_Birthday_Problem.html"
                >
                  
                    2.4
                  
                  The Birthday Problem
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_01/05_An_Exponential_Approximation.html"
                >
                  
                    2.5
                  
                  An Exponential Approximation
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_02/00_Calculating_Chances.html"
        >
          
            3.
          
          Chapter 2: Calculating Chances
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_02/01_Addition.html"
                >
                  
                    3.1
                  
                  Addition
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_02/02_Examples.html"
                >
                  
                    3.2
                  
                  Examples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_02/03_Multiplication.html"
                >
                  
                    3.3
                  
                  Multiplication
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_02/04_More_Examples.html"
                >
                  
                    3.4
                  
                  More Examples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_02/05_Updating_Probabilities.html"
                >
                  
                    3.5
                  
                  Updating Probabilities
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_03/00_Random_Variables.html"
        >
          
            4.
          
          Chapter 3: Random Variables
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_03/01_Functions_on_an_Outcome_Space.html"
                >
                  
                    4.1
                  
                  Functions on an Outcome Space
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_03/02_Distributions.html"
                >
                  
                    4.2
                  
                  Distributions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_03/03_Equality.html"
                >
                  
                    4.3
                  
                  Equality
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_04/00_Relations_Between_Variables.html"
        >
          
            5.
          
          Chapter 4: Relations Between Variables
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_04/01_Joint_Distributions.html"
                >
                  
                    5.1
                  
                  Joint Distributions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_04/02_Examples.html"
                >
                  
                    5.2
                  
                  Examples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_04/03_Marginal_Distributions.html"
                >
                  
                    5.3
                  
                  Marginal Distributions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_04/04_Conditional_Distributions.html"
                >
                  
                    5.4
                  
                  Conditional Distributions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_04/05_Dependence_and_Independence.html"
                >
                  
                    5.5
                  
                  Dependence and Independence
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_05/00_Collections_of_Events.html"
        >
          
            6.
          
          Chapter 5: Collections of Events
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_05/01_Bounding_the_Chance_of_a_Union.html"
                >
                  
                    6.1
                  
                  Bounding the Chance of a Union
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_05/02_Inclusion_Exclusion.html"
                >
                  
                    6.2
                  
                  Inclusion-Exclusion
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_05/03_The_Matching_Problem.html"
                >
                  
                    6.3
                  
                  The Matching Problem
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_05/04_Sampling_Without_Replacement.html"
                >
                  
                    6.4
                  
                  Sampling Without Replacement
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_05/05_Review_Problems_Set_1.html"
        >
          
          Review Problem Set 1
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_06/00_Random_Counts.html"
        >
          
            7.
          
          Chapter 6: Random Counts
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_06/01_Binomial_Distribution.html"
                >
                  
                    7.1
                  
                  The Binomial Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_06/02_Examples.html"
                >
                  
                    7.2
                  
                  Examples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_06/03_Hypergeometric_Distribution.html"
                >
                  
                    7.3
                  
                  The Hypergeometric Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_06/04_Odds_Ratios.html"
                >
                  
                    7.4
                  
                  Odds Ratios
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_06/05_Law_of_Small_Numbers.html"
                >
                  
                    7.5
                  
                  The Law of Small Numbers
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_07/00_Poissonization.html"
        >
          
            8.
          
          Chapter 7: Poissonization
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_07/01_Poissonizing_the_Binomial.html"
                >
                  
                    8.1
                  
                  Poissonizing the Binomial
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_07/02_Poissonizing_the_Multinomial.html"
                >
                  
                    8.2
                  
                  Poissonizing the Multinomial
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_08/00_Expectation.html"
        >
          
            9.
          
          Chapter 8: Expectation
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_08/01_Definition.html"
                >
                  
                    9.1
                  
                  Definition
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_08/02_Additivity.html"
                >
                  
                    9.2
                  
                  Additivity
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_08/03_Expectations_of_Functions.html"
                >
                  
                    9.3
                  
                  Expectations of Functions
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_08/04_Review_Problems_Set_2.html"
        >
          
          Review Problems: Set 2
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_09/00_Conditioning_Revisited.html"
        >
          
            10.
          
          Chapter 9: Conditioning, Revisited
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_09/01_Probability_by_Conditioning.html"
                >
                  
                    10.1
                  
                  Probability by Conditioning
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_09/02_Expectation_by_Conditioning.html"
                >
                  
                    10.2
                  
                  Expectation by Conditioning
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_09/03_Expected_Waiting_Times.html"
                >
                  
                    10.3
                  
                  Expected Waiting Times
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_10/00_Markov_Chains.html"
        >
          
            11.
          
          Chapter 10: Markov Chains
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_10/01_Transitions.html"
                >
                  
                    11.1
                  
                  Transitions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_10/02_Deconstructing_Chains.html"
                >
                  
                    11.2
                  
                  Deconstructing Chains
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_10/03_Long_Run_Behavior.html"
                >
                  
                    11.3
                  
                  Long Run Behavior
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_10/04_Examples.html"
                >
                  
                    11.4
                  
                  Examples
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_11/00_Reversing_Markov_Chains.html"
        >
          
            12.
          
          Chapter 11: Reversing Markov Chains
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_11/01_Detailed_Balance.html"
                >
                  
                    12.1
                  
                  Detailed Balance
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_11/02_Reversibility.html"
                >
                  
                    12.2
                  
                  Reversibility
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_11/03_Code_Breaking.html"
                >
                  
                    12.3
                  
                  Code Breaking
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_11/04_Markov_Chain_Monte_Carlo.html"
                >
                  
                    12.4
                  
                  Markov Chain Monte Carlo
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_11/05_Review_Conditioning_and_MC.html"
        >
          
          Review Set on Conditioning and Markov Chains
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_12/00_Standard_Deviation.html"
        >
          
            13.
          
          Chapter 12: Standard Deviation
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_12/01_Definition.html"
                >
                  
                    13.1
                  
                  Definition
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_12/02_Prediction_and_Estimation.html"
                >
                  
                    13.2
                  
                  Prediction and Estimation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_12/03_Bounds.html"
                >
                  
                    13.3
                  
                  Tail Bounds
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_12/04_Heavy_Tails.html"
                >
                  
                    13.4
                  
                  Heavy Tails
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_13/00_Variance_Via_Covariance.html"
        >
          
            14.
          
          Chapter 13: Variance Via Covariance
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_13/01_Properties_of_Covariance.html"
                >
                  
                    14.1
                  
                  Properties of Covariance
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_13/02_Sums_of_IID_Samples.html"
                >
                  
                    14.2
                  
                  Sums of IID Samples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_13/03_Sums_of_Simple_Random_Samples.html"
                >
                  
                    14.3
                  
                  Sums of Simple Random Samples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_13/04_Finite_Population_Correction.html"
                >
                  
                    14.4
                  
                  Finite Population Correction
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_14/00_The_Central_Limit_Theorem.html"
        >
          
            15.
          
          Chapter 14: The Central Limit Theorem
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_14/01_Exact_Distribution.html"
                >
                  
                    15.1
                  
                  Exact Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_14/02_PGFs_in_NumPy.html"
                >
                  
                    15.2
                  
                  PGFs in NumPy
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_14/03_Central_Limit_Theorem.html"
                >
                  
                    15.3
                  
                  Central Limit Theorem
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_14/04_The_Sample_Mean.html"
                >
                  
                    15.4
                  
                  The Sample Mean
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_14/05_Confidence_Intervals.html"
                >
                  
                    15.5
                  
                  Confidence Intervals
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_15/00_Continuous_Distributions.html"
        >
          
            16.
          
          Chapter 15: Continuous Distributions
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_15/01_Density_and_CDF.html"
                >
                  
                    16.1
                  
                  Density and CDF
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_15/02_The_Meaning_of_Density.html"
                >
                  
                    16.2
                  
                  The Meaning of Density
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_15/03_Expectation.html"
                >
                  
                    16.3
                  
                  Expectation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_15/04_Exponential_Distribution.html"
                >
                  
                    16.4
                  
                  Exponential Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_15/05_Calculus_in_SymPy.html"
                >
                  
                    16.5
                  
                  Calculus in SymPy
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_15/06_Review_Problems_Set_3.html"
        >
          
          Review Problems: Set 3
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_16/00_Transformations.html"
        >
          
            17.
          
          Chapter 16: Transformations
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_16/01_Linear_Transformations.html"
                >
                  
                    17.1
                  
                  Linear Transformations
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_16/02_Monotone_Functions.html"
                >
                  
                    17.2
                  
                  Monotone Functions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_16/03_Two_to_One_Functions.html"
                >
                  
                    17.3
                  
                  Two-to-One Functions
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_17/00_Joint_Densities.html"
        >
          
            18.
          
          Chapter 17: Joint Densities
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_17/01_Probabilities_and_Expectations.html"
                >
                  
                    18.1
                  
                  Probabilities and Expectations
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_17/02_Independence.html"
                >
                  
                    18.2
                  
                  Independence
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_17/03_Marginal_and_Conditional_Densities.html"
                >
                  
                    18.3
                  
                  Marginal and Conditional Densities
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_17/04_Beta_Densities_with_Integer_Parameters.html"
                >
                  
                    18.4
                  
                  Beta Densities with Integer Parameters
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_18/00_The_Normal_and_Gamma_Families.html"
        >
          
            19.
          
          Chapter 18: The Normal and Gamma Families
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_18/01_Standard_Normal_Basics.html"
                >
                  
                    19.1
                  
                  Standard Normal: The Basics
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_18/02_Sums_of_Independent_Normal_Variables.html"
                >
                  
                    19.2
                  
                  Sums of Independent Normal Variables
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_18/03_The_Gamma_Family.html"
                >
                  
                    19.3
                  
                  The Gamma Family
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_18/04_Chi_Squared_Distributions.html"
                >
                  
                    19.4
                  
                  Chi-Squared Distributions
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_18/05_Review_Problems_Set_4.html"
        >
          
          Review Problems: Set 4
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_19/00_Distributions_of_Sums.html"
        >
          
            20.
          
          Chapter 19: Distributions of Sums
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_19/01_Convolution_Formula.html"
                >
                  
                    20.1
                  
                  The Convolution Formula
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_19/02_Moment_Generating_Functions.html"
                >
                  
                    20.2
                  
                  Moment Generating Functions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_19/03_MGFs_Normal_and_the_CLT.html"
                >
                  
                    20.3
                  
                  MGFs, the Normal, and the CLT
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_19/04_Chernoff_Bound.html"
                >
                  
                    20.4
                  
                  Chernoff Bound
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_20/00_Approaches_to_Estimation.html"
        >
          
            21.
          
          Chapter 20: Approaches to Estimation
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_20/01_Maximum_Likelihood.html"
                >
                  
                    21.1
                  
                  Maximum Likelihood
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_20/02_Prior_and_Posterior.html"
                >
                  
                    21.2
                  
                  Prior and Posterior
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_20/03_Independence_Revisited.html"
                >
                  
                    21.3
                  
                  Independence, Revisited
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_21/00_The_Beta_and_the_Binomial.html"
        >
          
            22.
          
          Chapter 21: The Beta and the Binomial
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_21/01_Updating_and_Prediction.html"
                >
                  
                    22.1
                  
                  Updating and Prediction
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_21/02_Beta_Binomial_Distribution.html"
                >
                  
                    22.2
                  
                  The Beta-Binomial Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_21/03_Long_Run_Proportion_of_Heads.html"
                >
                  
                    22.3
                  
                  Long Run Proportion of Heads
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_22/00_Prediction.html"
        >
          
            23.
          
          Chapter 22: Prediction
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_22/01_Conditional_Expectation_Projection.html"
                >
                  
                    23.1
                  
                  Conditional Expectation As a Projection
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_22/02_Variance_by_Conditioning.html"
                >
                  
                    23.2
                  
                  Variance by Conditioning
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_22/03_Examples.html"
                >
                  
                    23.3
                  
                  Examples
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_22/04_Least_Squares_Predictor.html"
                >
                  
                    23.4
                  
                  Least Squares Predictor
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_23/00_Multivariate_Normal_RVs.html"
        >
          
            24.
          
          Chapter 23: Jointly Normal Random Variables
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_23/01_Random_Vectors.html"
                >
                  
                    24.1
                  
                  Random Vectors
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_23/02_Multivariate_Normal_Distribution.html"
                >
                  
                    24.2
                  
                  Multivariate Normal Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_23/03_Linear_Combinations.html"
                >
                  
                    24.3
                  
                  Linear Combinations
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_23/04_Independence.html"
                >
                  
                    24.4
                  
                  Independence
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_24/00_Simple_Linear_Regression.html"
        >
          
            25.
          
          Chapter 24: Simple Linear Regression
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_24/01_Bivariate_Normal_Distribution.html"
                >
                  
                    25.1
                  
                  Bivariate Normal Distribution
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_24/02_Linear_Least_Squares.html"
                >
                  
                    25.2
                  
                  Least Squares Linear Predictor
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_24/03_Regression_and_Bivariate_Normal.html"
                >
                  
                    25.3
                  
                  Regression and the Bivariate Normal
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_24/04_Regression_Equation.html"
                >
                  
                    25.4
                  
                  The Regression Equation
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_25/00_Multiple_Regression.html"
        >
          
            26.
          
          Chapter 25: Multiple Regression
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_25/01_Bilinearity_in_Matrix_Notation.html"
                >
                  
                    26.1
                  
                  Bilinearity in Matrix Notation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_25/02_Best_Linear_Predictor.html"
                >
                  
                    26.2
                  
                  Best Linear Predictor
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_25/03_Multivariate_Normal_Conditioning.html"
                >
                  
                    26.3
                  
                  Conditioning and the Multivariate Normal
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/textbook/Chapter_25/04_Multiple_Regression.html"
                >
                  
                    26.4
                  
                  Multiple Regression
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/textbook/Chapter_25/05_Further_Review_Exercises.html"
        >
          
          Further Review Exercises
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <div class="c-topbar" id="top-navbar">
  <!-- We show the sidebar by default so we use .is-active -->
  <div class="c-topbar__buttons">
    <button
      id="js-sidebar-toggle"
      class="hamburger hamburger--arrowalt is-active"
    >
      <span class="hamburger-box">
        <span class="hamburger-inner"></span>
      </span>
    </button>
    <div class="buttons">
<div class="download-buttons-dropdown">
    <button id="dropdown-button-trigger" class="interact-button"><i class="fa fa-download"></i></button>
    <div class="download-buttons">
        
        <a id="interact-button-print"><button id="interact-button-download" class="interact-button">PDF</button></a>
    </div>
</div>


  </div>
  <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
  <aside class="sidebar__right">
    <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
    <nav class="onthispage">
    </nav>
  </aside>
  <a href="/textbook/search.html" class="topbar-right-button" id="search-button"><i class="fa fa-search"></i></a>
</div>

      <main class="c-textbook__page" tabindex="-1">
            <div class="c-textbook__content" id="textbook_content">
              <div class="search-content__inner-wrap">
    <input type="text" id="lunr_search" class="search-input" tabindex="-1" placeholder="'Enter your search term...''" />
    <div id="results" class="results"></div>
</div>

<script>
    // Add the lunr store since we will now search it
    var store = [{
        "title": "Chapter 1: Fundamentals",
        
        "excerpt":
            "Fundamentals Exactly what probabilities are has been the subject of contentious debate. Some people think that probabilities are long-run frequencies that only apply to events that can happen over and over again under identical conditions. Others think that probabilities quantify an individual's subjective degree of uncertainty about events of any kind and can vary across individuals. Still others don't fall rigidly into either of those groups. Arguments about the meaning of probability led the great probabilist Jimmie Savage (1917-1971) to observe caustically that \"[A]s to what probability is ... there has seldom been such complete disagreement and breakdown of communication...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_01/00_Fundamentals.html",
        "teaser":null},{
        "title": "Outcome Space and Events",
        
        "excerpt":
            "Outcome Space and Events Any experiment involving randomness results in one of a number of possible outcomes. The outcome space is the collection of all such outcomes. Formally, an outcome space is just a set, usually denoted by $\\Omega$. That's the upper case Greek letter Omega. We will assume for now that $\\Omega$ is finite. In a sense this is not restrictive, as even the largest data sets are finite and the most powerful computers perform finitely many operations per task. However, we will soon see that allowing infinitely many possible outcomes not only results in a rich and elegant...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_01/01_Outcome_Space_and_Events.html",
        "teaser":null},{
        "title": "Equally Likely Outcomes",
        
        "excerpt":
            "Equally Likely Outcomes \"If a coin is tossed, what is the chance that it lands heads?\" Ask this question and the most common answer that you will get is $1/2$. If you press for a reason, don't be surprised to hear, \"Because the coin has two faces.\" A coin does indeed have two faces, but notice an assumption hidden inside the \"reasoning\" you have been given: that each of the two faces has the same chance as the other. The assumption of equally likely outcomes is a simple and ancient model of randomness. It defines probabilities as proportions. The assumption...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_01/02_Equally_Likely_Outcomes.html",
        "teaser":null},{
        "title": "Collisions in Hashing",
        
        "excerpt":
            "Collisions in Hashing In computer science, hash functions assign a code called a hash value to each member of a set of individuals. It's important that each individual be assigned a unique value. If two individuals are assigned the same value, there is a collision, and this causes trouble in identification. Yet it is cumbersome to keep track of which hash values have and have not been assigned, as the numbers of hash values and individuals can be very large. What if the hash values were just assigned at random, without taking into account which of them have already been...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_01/03_Collisions_in_Hashing.html",
        "teaser":null},{
        "title": "The Birthday Problem",
        
        "excerpt":
            "The Birthday Problem A classical problem in probability is about \"collisions\" of birthdays. This birthday problem was posed by Richard von Mises and other mathematicians  its origin has not been well established. The main question is, \"If there are $n$ people in a room, what is the chance that some pair among them have the same birthday?\" Assumptions of Randomness The problem is commonly solved under the assumptions that each year consists of 365 days and that each person is equally likely to be born on any of the 365 days regardless of the birthdays of others. You can...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_01/04_Birthday_Problem.html",
        "teaser":null},{
        "title": "An Exponential Approximation",
        
        "excerpt":
            "An Exponential Approximation The goal of this section is to understand how the chance of at least one collision behaves as a function of the number of individuals $n$, when there are $N$ hash values and $N$ is large compared to $n$. We know that chance is $$ P(\\text{at least one collision}) ~=~ 1 ~-~ \\prod_{i=0}^{n-1} \\frac{N-i}{N} $$ While this gives an exact formula for the chance, it doesn't give us a sense of how the function grows. Let's see if we can develop an approximation that has a simpler form and is therefore easier to study. The main steps...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_01/05_An_Exponential_Approximation.html",
        "teaser":null},{
        "title": "Chapter 2: Calculating Chances",
        
        "excerpt":
            "Calculating Chances Once you start working with probabilities, you quickly realize that the assumption of all possible outcomes being equally likely isn't always reasonable. For example, if you think a coin is biased then you won't want to assume that it lands heads with the same chance as tails. To deal with settings in which some outcomes have a higher chance than others, a more general theory is needed. In the 1930's, the Russian mathematician Andrey Kolmogorov (1903-1987) formulated some ground rules, known as axioms, that covered a rich array of settings and became the foundation of modern probability theory....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_02/00_Calculating_Chances.html",
        "teaser":null},{
        "title": "Addition",
        
        "excerpt":
            "Addition The third axiom is about events that are mutually exclusive. Two events $A$ and $B$ are mutually exclusive if at most one of them can happen; in other words, they can't both happen. For example, suppose you are selecting one student at random from a class in which 40% of the students are freshmen and 20% are sophomores. Each student is either a freshman or a sophomore or neither; but no student is both a freshman and a sophomore. So if $A$ is the event \"the student selected is a freshman\" and $B$ is the event \"the student selected...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_02/01_Addition.html",
        "teaser":null},{
        "title": "Examples",
        
        "excerpt":
            "Examples Let's see if we can use the results we've developed to calculate some chances. Some of the steps will be clear without calculation; others will require more work. Example 1: Both Heads and Tails in $n$ Tosses A coin is tossed $n$ times so that all $2^n$ possible sequences of heads and tails are equally likely. Question. What is the chance of getting at least one head and at least one tail? Answer. There are many sequences in which each face appears at least once. For example, if $n=4$, such sequences include HTTT, HTHT, TTHT, and so on. Method:...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_02/02_Examples.html",
        "teaser":null},{
        "title": "Multiplication",
        
        "excerpt":
            "Multiplication The main axiom of probability is about mutually exclusive events, and it turns out that we don't need any further axioms to deal with events that intersect. Let $A$ and $B$ be two events. The intersection $A \\cap B$ is the event that both $A$ and $B$ occur, and is shown in bright blue in the Venn diagram on the right. Because we will encounter intersections all the time, we'll be a little lazy in our notation: we will use $AB$ to denote the intersection and not bother writing the intersection symbol $\\cap$. You will have to keep in...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_02/03_Multiplication.html",
        "teaser":null},{
        "title": "More Examples",
        
        "excerpt":
            "More Examples Just a plus rule and a times rule  that's all it takes to get going. Here are some examples of standard problem solving techniques. Example 1: A Fundamental Method &#8211; Start by Partitioning A box contains 6 dark chocolates and 4 milk chocolates. I pick two at random without replacement. Question. What is the chance that I get one of each kind? Answer. You will have noticed that the problem doesn't specify whether the dark has to come first, or the milk. Either could happen. So list the distinct ways the event can happen, that is, partition...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_02/04_More_Examples.html",
        "teaser":null},{
        "title": "Updating Probabilities",
        
        "excerpt":
            "Updating Probabilities Data changes minds. We might start out with a set of assumptions about how the world works, but as we gather more data, we may have to update our opinions based on what we see in the data. Opinions can be reflected by probabilities, and these too can be updated as information comes in. In this section we will set up a method of updating probabilities given some data. We will start with an example and then we will state the method in greater generality. Example: True Positives In a population there is a rare disease: only 0.4%...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_02/05_Updating_Probabilities.html",
        "teaser":null},{
        "title": "Chapter 3: Random Variables",
        
        "excerpt":
            "Random Variables Much of data science involves numerical variables whose observed values depend on chance. The predicted value of one variable given the values of others, the number of different classes of individuals observed in a random sample, and the median of a bootstrapped sample are just a few examples. You saw many more in Data 8. In probability theory, a random variable is a numerical function defined on an outcome space. That is, the domain of the function is $\\Omega$ and its range is the real number line. Random variables are typically denoted by late letters of the alphabet,...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_03/00_Random_Variables.html",
        "teaser":null},{
        "title": "Functions on an Outcome Space",
        
        "excerpt":
            "Functions on an Outcome Space Random sampling can be thought of as repeated random trials, and therefore many outcome spaces consist of sequences. An outcome space representing two tosses of a coin is $$ \\Omega = \\{ \\text{HH, HT, TH, TT} \\} $$If you were tossing 10 times, the outcome space would consist of the $2^{10}$ sequences of 10 elements where each element is H or T. The outcomes are a pain to list by hand, but computers are good at saving us that kind of pain. Product Spaces The product of two sets $A$ and $B$ is the set...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_03/01_Functions_on_an_Outcome_Space.html",
        "teaser":null},{
        "title": "Distributions",
        
        "excerpt":
            "Distributions Our space is the outcomes of five rolls of a die, and our random variable $S$ is the total number of spots on the five rolls. five_rolls_sum omega S(omega) P(omega) [1 1 1 1 1] 5 0.000128601 [1 1 1 1 2] 6 0.000128601 [1 1 1 1 3] 7 0.000128601 [1 1 1 1 4] 8 0.000128601 [1 1 1 1 5] 9 0.000128601 [1 1 1 1 6] 10 0.000128601 [1 1 1 2 1] 6 0.000128601 [1 1 1 2 2] 7 0.000128601 [1 1 1 2 3] 8 0.000128601 [1 1 1 2 4] 9...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_03/02_Distributions.html",
        "teaser":null},{
        "title": "Equality",
        
        "excerpt":
            "Equality We know what it means for two numbers to be equal: they are at the same spot on the number line. Equality of random variables, however, can be of more than one kind. Equal Two random variables $X$ and $Y$ defined on the same outcome space are equal if their values are the same for every outcome in the space. The notation is $X = Y$ and it means that $$ X(\\omega) = Y(\\omega) \\text{ for all } \\omega \\in \\Omega $$Informally, this says that no matter what the outcome, if $X$ is 10 then $Y$ must be 10...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_03/03_Equality.html",
        "teaser":null},{
        "title": "Chapter 4: Relations Between Variables",
        
        "excerpt":
            "Relations Between Variables Many questions in data science have at their center a question about the relation between variables. How can attributes be used to tell whether or not a cell is cancerous? What is the relation between average income and water usage in Californian counties? Do car buyers pay more for good mileage or for rapid acceleration? You encountered these and many other such questions in Data 8. Probability theory helps us pose and answer precise questions about the relation between random variables. In particular, it helps us understand the conditional behavior of one set of random variables given...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_04/00_Relations_Between_Variables.html",
        "teaser":null},{
        "title": "Joint Distributions",
        
        "excerpt":
            "Joint Distributions Suppose $X$ and $Y$ are two random variables defined on the same outcome space. We will use the notation $P(X = x, Y = y)$ for the probability that $X$ has the value $x$ and $Y$ has the value $y$. The joint distribution of $X$ and $Y$ consists of all the probabilities $P(X=x, Y=y)$ where $(x, y)$ ranges over all the possible values of $(X, Y)$. Example In three tosses of a coin, let $X$ be the number of heads in the first two tosses and $Y$ the number of heads in the last two tosses. Then $$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_04/01_Joint_Distributions.html",
        "teaser":null},{
        "title": "Examples",
        
        "excerpt":
            "Examples This section is a workout in finding probabilities of events determined by two jointly distributed random variables. We will apply the computational methods of the previous section and also the mathematical framework for finding probabilities that was developed there. In some of the examples you may find yourself wondering why we are bothering to write out math notation for numerical answers that we have already obtained using Python. It is because the Python visualizations help us understand the math. That understanding then helps us answer questions in generality, not just in particular numerical settings, as you will see in...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_04/02_Examples.html",
        "teaser":null},{
        "title": "Marginal Distributions",
        
        "excerpt":
            "Marginal Distributions What does the joint distribution of $X$ and $Y$ tell us about the distribution of $X$ alone? Everything, of course. Let's see how. Here is the joint distribution table of two random variables $X$ and $Y$. joint_table X=0 X=1 X=2 Y=5 0.00000 0.0000 0.03125 Y=4 0.00000 0.0625 0.09375 Y=3 0.03125 0.1875 0.09375 Y=2 0.09375 0.1875 0.03125 Y=1 0.09375 0.0625 0.00000 Y=0 0.03125 0.0000 0.00000 To find the distribution of $X$ we need the possible values of $X$ and all their probabilities. At a glance, you can see that the possible values of $X$ are 0, 1, and 2....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_04/03_Marginal_Distributions.html",
        "teaser":null},{
        "title": "Conditional Distributions",
        
        "excerpt":
            "Conditional Distributions To understand the relation between two variables we must examine the conditional behavior of each of them given the value of the other. Towards this goal, we will start by examining the example of the previous section and then develop the general theory. In our example, the joint distribution of $X$ and $Y$ is given by joint_table. Here we also display the marginal distribution of $X$. joint_table.marginal(&#39;X&#39;) X=0 X=1 X=2 Y=5 0.00000 0.0000 0.03125 Y=4 0.00000 0.0625 0.09375 Y=3 0.03125 0.1875 0.09375 Y=2 0.09375 0.1875 0.03125 Y=1 0.09375 0.0625 0.00000 Y=0 0.03125 0.0000 0.00000 Sum: Marginal of X...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_04/04_Conditional_Distributions.html",
        "teaser":null},{
        "title": "Dependence and Independence",
        
        "excerpt":
            "Dependence and Independence Conditional distributions help us formalize our intuitive ideas about whether two random variables are independent of each other. Let $X$ and $Y$ be two random variables, and suppose we are given the value of $X$. Does that change our opinion about $Y$? If the answer is yes, then we will say that $X$ and $Y$ are dependent. If the answer is no regardless of the given value of $X$, then we will say that $X$ and $Y$ are independent. Let's start with some examples and then move to precise definitions and results. Dependence Here is the joint...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_04/05_Dependence_and_Independence.html",
        "teaser":null},{
        "title": "Chapter 5: Collections of Events",
        
        "excerpt":
            "    Collections of Events            Most questions in data science involve multiple variables and events. Random variables and their joint distributions give us a way to set up probabilistic models for how our data originate. Some techniques are particularly useful for working with large collections of variables and events. These include:   Using bounds when exact values are difficult to calculate Noticing patterns when working with small collections and then generalizing to larger ones Using symmetry, both for insight and for simplifying calculation  In this chapter we will study powerful examples of all these techniques.           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_05/00_Collections_of_Events.html",
        "teaser":null},{
        "title": "Bounding the Chance of a Union",
        
        "excerpt":
            "Bounding the Chance of a Union Before we get to larger collections of events, let's consider the union of two events that are not mutually exclusive. The diagram below shows two such events. The union is the entire colored region: the blue, the gold, as well as the intersection. show_intersection() We can find $P(A \\cup B)$ by partitioning. A partition of $A \\cup B$ is \"all of $A$ union the part of $B$ that is not also in $A$\": $$ A \\cup B = A \\cup (B \\backslash AB) $$Therefore $$ \\begin{align*} P(A \\cup B) &amp;= P(A) + P(B \\backslash...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_05/01_Bounding_the_Chance_of_a_Union.html",
        "teaser":null},{
        "title": "Inclusion-Exclusion",
        
        "excerpt":
            "Inclusion-Exclusion While we have bounds on the probability of the union of $n$ events, we don't yet have a formula for the exact chance except in the case $n = 2$. For $n = 2$ we have $$ P(A \\cup B) = P(A) + P(B) - P(AB) $$ Union of $n$ Events Let's see if we can guess the formula for larger $n$, by applying what we know about the union of two events. For $n = 3$, the event $A_1 \\cup A_2 \\cup A_3$ can be written as $B \\cup A_3$ where $B = A_1 \\cup A_2$. So by...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_05/02_Inclusion_Exclusion.html",
        "teaser":null},{
        "title": "The Matching Problem",
        
        "excerpt":
            "The Matching Problem This famous problem has been stated variously in terms of hats and people, letters and envelopes, tea cups and saucers  indeed, any situation in which you might want to match two kinds of items seems to have appeared somewhere as a setting for the matching problem. In the letter-envelope setting there are $n$ letters labeled 1 through $n$ and also $n$ envelopes labeled 1 through $n$. The letters are permuted randomly into the envelopes, one letter per envelope (a mishap usually blamed on an unfortunate hypothetical secretary), so that all permutations are equally likely. The main...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_05/03_The_Matching_Problem.html",
        "teaser":null},{
        "title": "Sampling Without Replacement",
        
        "excerpt":
            "Sampling Without Replacement Consider a set of $n$ individuals labeled $1, 2 \\ldots , n$. The results of $n$ draws made at random without replacement is a random permutation of all the elements. You used random permutations in Data 8 when you were trying to assess whether two samples came from the same underlying distribution. Let's call such a permutation $(X_1, X_2, \\ldots , X_n)$. For any permutation $i_1, i_2, \\ldots , i_n$ of the integers 1 through $n$, $$ P(X_1 = i_1, X_2 = i_2, \\ldots, X_n = i_n) = \\frac{1}{n!} $$Notice that the right hand side doesn't depend...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_05/04_Sampling_Without_Replacement.html",
        "teaser":null},{
        "title": "Review Problem Set 1",
        
        "excerpt":
            "Review Problems: Set 1 These problems can be solved using the main ideas of Chapters 1 through 5. It is divided into two parts: The Basics, and Additional Practice. The first part will remind you of the fundamental concepts and some typical calculations. The rest are for you to further develop your problem solving skills and your fluency with the notation and ideas. The Basics 1. A procedure for estimating a parameter is based on random sampling and has a 95% chance of producing a good estimate. Suppose I run this procedure $n$ times such that the results of all...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_05/05_Review_Problems_Set_1.html",
        "teaser":null},{
        "title": "Chapter 6: Random Counts",
        
        "excerpt":
            "Random Counts These form a class of random variables that are of fundamental importance in probability theory. You have seen some examples already: the number of matches (fixed points) in a random permutation of $n$ elements is an example of a \"random count\", as is the number of good elements in a simple random sample. The general setting is that there are a number of trials, each of which can be a success or a failure. The random count is the number of successes among all the trials. The distribution of the number of successes depends on the underlying assumptions...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_06/00_Random_Counts.html",
        "teaser":null},{
        "title": "The Binomial Distribution",
        
        "excerpt":
            "The Binomial Distribution Let $X_1, X_2, \\ldots , X_n$ be i.i.d. Bernoulli $(p)$ random variables and let $S_n = X_1 + X_2 \\ldots + X_n$. That's a formal way of saying: Suppose you have a fixed number $n$ of success/failure trials; and the trials are independent; and on each trial, the probability of success is $p$. Let $S_n$ be the total number of successes. The first goal of this section is to find the distribution of $S_n$. In the example that we fixed our minds on earlier, we are counting the number of sixes in 7 rolls of a die....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_06/01_Binomial_Distribution.html",
        "teaser":null},{
        "title": "Examples",
        
        "excerpt":
            "Examples In this section we will work with the binomial distribution in a variety of settings. The formula and code are straightforward enough. What you have to keep in mind are the conditions under which the binomial distribution is valid. It is the distribution of: the number of successes in a known number of independent trials with the same probability of success on each trial. So don't use it for the number of aces in a hand of cards dealt without replacement. Example 1: Random Number Generator A random number generator draws repeatedly at random with replacement from the 10...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_06/02_Examples.html",
        "teaser":null},{
        "title": "The Hypergeometric Distribution",
        
        "excerpt":
            "The Hypergeometric Distribution You have seen the hypergeometric probabilities earlier. In this section we will use them to define the distribution of a random count, and study the relation with the binomial distribution. As a review of the hypergeometric setting, suppose you have a population of a fixed size $N$, and suppose you are interested in a particular group of those $N$ individuals. Let's call them \"successes\" or \"good elements\". For example, you might be interested in: a population of voters, and among them the group who will vote for a particular candidate a population of households, and among them...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_06/03_Hypergeometric_Distribution.html",
        "teaser":null},{
        "title": "Odds Ratios",
        
        "excerpt":
            "Odds Ratios Binomial $(n, p)$ probabilities involve powers and factorials, both of which are difficult to compute when $n$ is large. This section is about simplifying the computation of the entire distribution. The result also helps us understand the shape of binomial histograms. Consecutive Odds Ratios Fix $n$ and $p$, and let $P(k)$ be the binomial $(n, p)$ probability of $k$. That is, let $P(k)$ be the chance of getting $k$ successes in $n$ independent trials with probability $p$ of success on each trial. For $k \\ge 1$, define the $k$th consecutive odds ratio $$ R(k) = \\frac{P(k)}{P(k-1)} $$To see...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_06/04_Odds_Ratios.html",
        "teaser":null},{
        "title": "The Law of Small Numbers",
        
        "excerpt":
            "The Law of Small Numbers The consecutive odds ratios of the binomial $(n, p)$ distribution help us derive an approximation for the distribution when $n$ is large and $p$ is small. The approximation is sometimes called \"the law of small numbers\" because it approximates the distribution of the number of successes when the chance of success is small: you only expect a small number of successes. As an example, here is the binomial $(1000, 2/1000)$ distribution. Note that $1000$ is large, $2/1000$ is pretty small, and $1000 \\times (2/1000) = 2$ is the natural number of successes to be thinking...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_06/05_Law_of_Small_Numbers.html",
        "teaser":null},{
        "title": "Chapter 7: Poissonization",
        
        "excerpt":
            "Poissonization A binomial $(n, p)$ random variable has a finite number of values: it can only be between 0 and $n$. But now that we are studying the behavior of binomial probabilities as $n$ gets large, it is time to move from finite outcome spaces to spaces that are infinite. Our first example of a probability distribution on infinitely many values is motivated by the approximation we have developed for the binomial $(n, p)$ distribution when $n$ is large and $p$ is small. Under those assumptions we saw that the chance of $k$ successes in $n$ i.i.d. Bernoulli $(p)$ trials...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_07/00_Poissonization.html",
        "teaser":null},{
        "title": "Poissonizing the Binomial",
        
        "excerpt":
            "Poissonizing the Binomial The families of distributions that data scientists tend to use most are those that arise out of natural assumptions about the randomness in the data. These distribution families also have mathematical properties that give rise to illuminating answers to questions about the data. The binomial and Poisson families are among these. In this section we will study some more properties of the Poisson family, including another remarkable connection that it has with the binomial family. Sums of Independent Poisson Variables Let $X$ have the Poisson ($\\mu$) distribution, and let $Y$ independent of $X$ have the Poisson ($\\lambda$)...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_07/01_Poissonizing_the_Binomial.html",
        "teaser":null},{
        "title": "Poissonizing the Multinomial",
        
        "excerpt":
            "Poissonizing the Multinomial Bernoulli trials come out in one of two ways. But many trials come out in multiple different ways, all of which we might want to track. A die can land six different ways. A jury member can have one of several different educational levels. In general, an individual might belong to one of several classes. The multinomial distribution is an extension of the binomial to the case where each repeated trial has more than two possible outcomes. Let's look at it first in an example, and then we will define it in general. A box contains 2...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_07/02_Poissonizing_the_Multinomial.html",
        "teaser":null},{
        "title": "Chapter 8: Expectation",
        
        "excerpt":
            "    Expectation            The distribution of a random variable gives us detailed information about how probability is distributed over all the possible values of the variable. But often we would just like a sense of roughly where the distribution is situated on the number line. In other words, we would like some idea of where the center of the distribution is.  As any student who has taken multiple \"midterm\" exams in a class knows, words like \"middle\" and \"center\" don't have a unique meaning. This chapter is about a particular kind of \"center\" of the distribution of a random variable.           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_08/00_Expectation.html",
        "teaser":null},{
        "title": "Definition",
        
        "excerpt":
            "Definition The expectation of a random variable $X$, denoted $E(X)$, is the average of the possible values of $X$ weighted by their probabilities. This can be calculated in two equivalent ways. On the domain of $X$: $$ E(X) = \\sum_{\\omega \\in \\Omega} X(\\omega)P(\\omega) $$On the range of $X$: $$ E(X) = \\sum_{\\text{all }x} xP(X=x) $$Technical Note: If $X$ has finitely many possible values, the sums above are always well defined and finite. If $X$ can have countably many values (that is, values indexed by 1, 2, 3, $\\ldots$), then more care is needed to make sure that the formulas result...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_08/01_Definition.html",
        "teaser":null},{
        "title": "Additivity",
        
        "excerpt":
            "Additivity Calculating expectation by plugging into the definition works in simple cases, but often it can be cumbersome or lack insight. The most powerful result for calculating expectation turns out not to be the definition. It looks rather innocuous: Additivity of Expectation Let $X$ and $Y$ be two random variables defined on the same probability space. Then $$ E(X+Y) = E(X) + E(Y) $$Before we look more closely at this result, note that we are assuming that all the expectations exist; we will do this throughout in this course. And now note that there are no assumptions about the relation...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_08/02_Additivity.html",
        "teaser":null},{
        "title": "Expectations of Functions",
        
        "excerpt":
            "Expectations of Functions Once we start using random variables as estimators, we will want to see how far the estimate is from a desired value. For example, we might want to see how far a random variable $X$ is from the number 10. That's a function of $X$. Let's call it $Y$. Then $$ Y = |X - 10| $$which is not a linear function. To find $E(Y)$, we need a bit more technique. Throughout, we will assume that all the expectations that we are discussing are well defined. This section is about finding the expectation of a function of...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_08/03_Expectations_of_Functions.html",
        "teaser":null},{
        "title": "Review Problems: Set 2",
        
        "excerpt":
            "Review Problems: Set 2 These problems can be solved using the main ideas of Chapters 6 through 8. The material in those chapters of course relies on all the previous chapters. The set of exercises is divided into two parts: The Basics, and Additional Practice. The first part will remind you of the fundamental concepts and some typical calculations. The rest are for you to further develop your problem solving skills and your fluency with the notation and ideas. The Basics 1. A coin is tossed 12 times. Find the chance that (a) there are six heads (b) there are...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_08/04_Review_Problems_Set_2.html",
        "teaser":null},{
        "title": "Chapter 9: Conditioning, Revisited",
        
        "excerpt":
            "    Conditioning, Revisited            In the next couple of chapters we are going to study random processes indexed by time. We have looked at some of these already  for example, a sequence of trials can be thought of as a random process indexed by time.  Naturally, we will be interested in what we can predict about a process in the future given that we know its present value. Techniques involving conditioning are going to be useful. We know some of these techniques already, and in this chapter we will develop them further.           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_09/00_Conditioning_Revisited.html",
        "teaser":null},{
        "title": "Probability by Conditioning",
        
        "excerpt":
            "Probability by Conditioning The theory in this section isn't new. It's the old familiar multiplication rule. We are just going to use it in the context of processes indexed by time, in a method that we are going to call conditioning on early moves. Winning a Game of Dice Suppose Jo and Bo play the following game. Jo rolls a die, then Bo rolls it, then Jo rolls again, and so on, until the first time one of them gets the face with six spots. That person is the winner. Question. What is the chance that Jo wins? Answer. Before...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_09/01_Probability_by_Conditioning.html",
        "teaser":null},{
        "title": "Expectation by Conditioning",
        
        "excerpt":
            "Expectation by Conditioning Let $T$ be a random variable, and let $S$ be a random variable defined on the same space as $T$. As we have seen, conditioning on $S$ might be a good way to find probabilities for $T$ if $S$ and $T$ are related. In this section we will see that conditioning on $S$ can also be a good way to find the expectation of $T$. We will start with a simple example to illustrate the ideas. Let the joint distribution of $T$ and $S$ be as in the table below. t = [3, 4] s = [5,...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_09/02_Expectation_by_Conditioning.html",
        "teaser":null},{
        "title": "Expected Waiting Times",
        
        "excerpt":
            "Expected Waiting Times Let's find some expectations by conditioning. All of the calculations below involve conditioning on early moves of a random process. Waiting till H A coin lands heads with chance $p$. Let's call it a $p$-coin for short. Let $W_H$ be the number of tosses of a $p$-coin till the first head appears. The use of $W$ in the notation is because the random variable is often called the waiting time till the first head. If as usual we write $q = 1-p$, the distribution of $W_H$ is given by $$ P(W_H = k) ~ = ~ q^{k-1}p,...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_09/03_Expected_Waiting_Times.html",
        "teaser":null},{
        "title": "Chapter 10: Markov Chains",
        
        "excerpt":
            "Markov Chains A stochastic process is a collection of random variables on a probability space. We will study a kind of process that evolves over discrete time, that is, random variables $X_0, X_1, X_2, \\ldots $. Our image is that the process starts with value $X_0$ at time 0, and then takes steps at times 1, 2, and so on, with $X_n$ representing the value at time $n$. We have already seen examples of such processes. For example, an i.i.d. sequence of Bernoulli $(p)$ trials forms such a process, going back and forth between the two values 0 and 1,...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_10/00_Markov_Chains.html",
        "teaser":null},{
        "title": "Transitions",
        
        "excerpt":
            "Transitions Let $X_0, X_1, X_2, \\ldots $ be a Markov chain with state space $S$. By the Markov property, the probability of a trajectory or path of finite length is $$ \\begin{align*} &amp; P(X_0 = i_0, X_1 = i_1, X_2 = i_2, \\ldots, X_n = i_n) \\\\ &amp; = ~ P(X_0 = i_0)P(X_1 = i_1 \\mid X_0 = i_0)P(X_2 = i_2 \\mid X_1 = i_1) \\cdots P(X_n = i_n \\mid X_{n-1} = i_{n-1}) \\end{align*} $$The distribution of $X_0$ is called the initial distribution of the chain. The conditional probabilities in the product are called transition probabilities. For states $i$ and...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_10/01_Transitions.html",
        "teaser":null},{
        "title": "Deconstructing Chains",
        
        "excerpt":
            "Deconstructing Chains Let $S$ be a finite or countably infinite set of states. Any stochastic matrix with rows and columns indexed by $S$ is the transition matrix of some Markov chain with state space $S$. The transition behaviors of Markov chains are thus as varied as the matrices. It is helpful to set up terminology to discuss some of these behaviors. Communication If it is possible for the chain to get from state $i$ to state $j$, we say that $i$ leads to $j$ and we write $i \\rightarrow j$. Usually you can decide whether $i$ leads to $j$ just...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_10/02_Deconstructing_Chains.html",
        "teaser":null},{
        "title": "Long Run Behavior",
        
        "excerpt":
            "Long Run Behavior Every irreducible and aperiodic Markov Chain on a finite state space exhibits astonishing regularity after it has run for a while. The proof of the convergence theorem below is beyond the scope of this course, but in examples you have seen the result by computation. All the results are true in greater generality for some classes of Markov Chains on infinitely many states. Convergence to Stationarity Let $X_0, X_1, \\ldots$ be an irreducible, aperiodic Markov chain on a finite state space $S$. Then for all states $i$ and $j$, $$ P_n(i, j) \\to \\pi(j) ~~~ \\text{as }...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_10/03_Long_Run_Behavior.html",
        "teaser":null},{
        "title": "Examples",
        
        "excerpt":
            "Examples Here are two examples to illustrate how to find the stationary distribution and how to use it. A Diffusion Model by Ehrenfest Paul Ehrenfest proposed a number of models for the diffusion of gas particles, one of which we will study here. The model says that there are two containers containing a total of $N$ particles. At each instant, a container is selected at random and a particle is selected at random independently of the container. Then the selected particle is placed in the selected container; if it was already in that container, it stays there. Let $X_n$ be...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_10/04_Examples.html",
        "teaser":null},{
        "title": "Chapter 11: Reversing Markov Chains",
        
        "excerpt":
            "Reversing Markov Chains What do stock markets, mutating viruses, and computer search engines have in common? They have all been analyzed using Markov Chain models. Understanding the long run behavior of Markov Chains helps us understand many different random phenomena. In data science, Markov Chains are also used for quite a different purpose. Markov Chains help data scientists draw random samples from distributions that are too complicated for standard sampling methods to handle. Markov Chains can also be used to approximate expectations of random quantities whose distributions are either very complicated or involve unknown quantities that are too difficult to...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_11/00_Reversing_Markov_Chains.html",
        "teaser":null},{
        "title": "Detailed Balance",
        
        "excerpt":
            "Detailed Balance The Markov Chains that we have been studying have stationary distributions that contain much information about the behavior of the chain. The stationary distribution of a chain is a probability distribution that solves the balance equations. For some chains it is easy to identify a distribution that solves the balance equations. But for other chains, the solution can be complicated or tedious. Let's see if we can find a simple way to solve the balance equations. Recall our earlier image of what is being balanced in the equations. Imagine a large number of independent replications of the chain....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_11/01_Detailed_Balance.html",
        "teaser":null},{
        "title": "Reversibility",
        
        "excerpt":
            "Reversibility The reflecting random walk of the previous section has states 0, 1, 2, 3, 4 arranged in sequence clockwise on a circle. At each step the chain stays in place with probability $s$, moves to its clockwise neighbor with probability $r$ and to its counterclockwise neighbor with probability $p$. The stationary distribution of the chain assigns chance 0.2 to each state. If $r &gt; p$, then the chain is more likely to be moving clockwise than counterclockwise. For example, in steady state, the probability of the path $0, 1, 2, 3$ is $$ P(X_0 = 0)P(0, 1)P(1, 2)P(2, 3)...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_11/02_Reversibility.html",
        "teaser":null},{
        "title": "Code Breaking",
        
        "excerpt":
            "Code Breaking While it is interesting that many Markov Chains are reversible, the examples that we have seen so far haven't explained what we get by reversing a chain. After all, if it looks the same running forwards as it does backwards, why not just run it forwards? Why bother with reversibility? It turns out that reversing Markov Chains can help solve a class of problems that are intractable by other methods. In this section we present an example of how such problems arise. In the next section we discuss a solution. Assumptions People have long been fascinated by encryption...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_11/03_Code_Breaking.html",
        "teaser":null},{
        "title": "Markov Chain Monte Carlo",
        
        "excerpt":
            "Markov Chain Monte Carlo The goal of Markov Chain Monte Carlo (MCMC) is to generate random samples from complicated high dimensional distributions about which we have incomplete information. For example, it might be that we don't know the normalizing constant of the distribution, as we saw in the code breaking example of the previous section. Suppose the distribution from which we want to generate a sample is called $\\pi$. We are going to assume that $\\pi$ is a probability distribution on a finite set, and you should imagine the set to be large. MCMC relies on a few observations. Let...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_11/04_Markov_Chain_Monte_Carlo.html",
        "teaser":null},{
        "title": "Review Set on Conditioning and Markov Chains",
        
        "excerpt":
            "Review Set on Conditioning and Markov Chains David Aldous' former student Takis Konstantopoulos created a great resource of exercises on Markov Chains. Selected problems are listed below along with some others. Resist the impulse to read the answers to the Konstantopoulos exercises before you try the exercises yourself. 1. Bridge Hands My bridge partner and I are playing a game of bridge against two other people. In the game, four hands of 13 cards each are dealt to the four players, at random without replacement from a standard deck. Let $X$ be the number of hearts in my hand and...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_11/05_Review_Conditioning_and_MC.html",
        "teaser":null},{
        "title": "Chapter 12: Standard Deviation",
        
        "excerpt":
            "Standard Deviation The expected value $\\mu_X$ of a random variable $X$ is a measure of the center of the distribution of $X$. But we know that $X$ need not be equal to $\\mu_X$; indeed, $\\mu_X$ need not even be a possible value of $X$. How far from $\\mu_X$ can $X$ be? This chapter develops an answer to that question. As a starting point, it is natural to look at the deviation from the mean $$ X - \\mu_X $$and try to get a sense of what we expect that to be. By the linear function rule, $$ E(X - \\mu_X)...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_12/00_Standard_Deviation.html",
        "teaser":null},{
        "title": "Definition",
        
        "excerpt":
            "Definition Measuring the rough size of the squared deviations has the advantage that it avoids cancellation between positive and negative errors. The disadvantage is that squared deviations have units that are difficult to understand. The measure of spread that we are about to define takes care of this problem. Root Mean Squared Deviation from the Mean Let $X$ be a random variable with expectation $\\mu_X$. The standard deviation of $X$, denoted $SD(X)$ or $\\sigma_X$, is the root mean square of deviations from the mean: $$ SD(X) = \\sigma_X = \\sqrt{ E\\big{(} (X-\\mu_X)^2 \\big{)} } $$$SD(X)$ has the same units as...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_12/01_Definition.html",
        "teaser":null},{
        "title": "Prediction and Estimation",
        
        "excerpt":
            "Prediction and Estimation One way to think about the SD is in terms of errors in prediction. Suppose I am going to generate a value of the random variable $X$, and I ask you to predict the value I am going to get. What should you use as your predictor? A natural choice is $\\mu_X$, the expectation of $X$. But you could choose any number $c$. The error that you will make is $X - c$. About how big is that? For most reasonable choices of $c$, the error will sometimes be positive and sometimes negative. To find the rough...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_12/02_Prediction_and_Estimation.html",
        "teaser":null},{
        "title": "Tail Bounds",
        
        "excerpt":
            "Tail Bounds If you know $E(X)$ and $SD(X)$ you can get some idea of how much probability there is in the tails of the distribution of $X$. In this section we are going to get upper bounds on probabilities such as the gold area in the graph below. That's $P(X \\ge 20)$ for the random variable $X$ whose distribution is displayed in the histogram. Monotonicity To do this, we will start with an observation about expectations of functions of $X$. Suppose $g$ and $h$ are functions such that $g(X) \\ge h(X)$, that is, $P(g(X) \\ge h(X)) = 1$. Then $E(g(X))...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_12/03_Bounds.html",
        "teaser":null},{
        "title": "Heavy Tails",
        
        "excerpt":
            "Heavy Tails This short section shows an example of how expectations and SDs, though very useful in many situations, aren't quite adequate when distributions have long, fat tails. Here is one such distribution. N = 1000 n = np.arange(1, N+1, 1) probs = (1/n)*(1/np.sum(1/n)) dist = Table().values(n).probability(probs) Plot(dist) plt.xlim(0, N/10); You can see that the tail stretches out quite far. If we sample independently from this population, how does the sample average behave? Averages are affected by values out in the tails. Let's simulate the distribution of the average of a random sample of size 500 from this distribution. We'll...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_12/04_Heavy_Tails.html",
        "teaser":null},{
        "title": "Chapter 13: Variance Via Covariance",
        
        "excerpt":
            "Variance Via Covariance In this chapter we return to random sampling and study the variability in the sum of a random sample. It is worth taking some time to understand how sample sums behave, because many interesting random variables like counts of successes can be written as sums. Binomial and hypergeometric random variables are such sums. Also, the mean of a random sample is a straightforward function of the sample sum. Let $X$ be a random variable. We will use some familiar shorthand: $\\mu_X = E(X)$ $\\sigma_X = SD(X)$ Let $D_X = X - \\mu_X$ denote the deviation of $X$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_13/00_Variance_Via_Covariance.html",
        "teaser":null},{
        "title": "Properties of Covariance",
        
        "excerpt":
            "Properties of Covariance Let's examine how covariance behaves. In the next two sections we will use our observations to calculate variances of sample sums. Establishing properties of covariance involves simple observations and routine algebra. We have done some of it below, and we expect that you can fill in the rest. Recall that the covariance of $X$ and $Y$ is $$ Cov(X, Y) ~ = ~ E(D_XD_Y) ~ = ~ E[(X - \\mu_X)(Y - \\mu_Y)] $$ Constants Don't Vary That title has a \"duh\" quality. But it's still worth noting that for any constant $c$, $$ Cov(X, c) = 0...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_13/01_Properties_of_Covariance.html",
        "teaser":null},{
        "title": "Sums of IID Samples",
        
        "excerpt":
            "Sums of IID Samples After the dry, algebraic discussion of the previous section it is a relief to finally be able to compute some variances. Let $X_1, X_2, \\ldots X_n$ be random variables with sum $$ S_n = \\sum_{i=1}^n X_i $$ The variance of the sum is $$ \\begin{align*} Var(S_n) &amp;= Cov(S_n, S_n) \\\\ &amp;= \\sum_{i=1}^n\\sum_{j=1}^n Cov(X_i, X_j) ~~~~ \\text{(bilinearity)} \\\\ &amp;= \\sum_{i=1}^n Var(X_i) + \\mathop{\\sum \\sum}_{1 \\le i \\ne j \\le n} Cov(X_i, X_j) \\end{align*} $$We say that the variance of the sum is the sum of all the variances and all the covariances. If $X_1, X_2 \\ldots ,...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_13/02_Sums_of_IID_Samples.html",
        "teaser":null},{
        "title": "Sums of Simple Random Samples",
        
        "excerpt":
            "Sums of Simple Random Samples When the random variables that are being added are not independent, finding the variance of the sum does involve finding covariances. As before, let $X_1, X_2, \\ldots X_n$ be random variables with sum $$ S_n = \\sum_{i=1}^n X_i $$The variance of the sum is $$ Var(S_n) ~ = ~ \\sum_{i=1}^n Var(X_i) + \\mathop{\\sum \\sum}_{1 \\le i \\ne j \\le n} Cov(X_i, X_j) $$ Before we apply this formula, let's start out by finding a simple covariance. Indicators Let $A$ and $B$ be two events. Let $I_A$ be the indicator of $A$ and let $I_B$ be...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_13/03_Sums_of_Simple_Random_Samples.html",
        "teaser":null},{
        "title": "Finite Population Correction",
        
        "excerpt":
            "Finite Population Correction Data scientists often have to work with a relatively small sample from an enormous population. So suppose we are drawing at random $n$ times from a population of size $N$ where $N$ is large and $n$ is small relative to $N$. Just go with the flow for now  all of this will become more precise as this section develops. Suppose the population mean is $\\mu$ and the population SD is $\\sigma$. Let $S_n$ be the sample sum. Then, regardless of whether the sample is drawn with replacement or without, $$ E(S_n) = n\\mu $$ The variance...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_13/04_Finite_Population_Correction.html",
        "teaser":null},{
        "title": "Chapter 14: The Central Limit Theorem",
        
        "excerpt":
            "The Central Limit Theorem The standard deviation is one measure of spread, but you can imagine many others. Why choose the SD over all of them? The main reason is the connection between the SD and the normal curve. Why is the normal curve so important? This chapter contains an answer to that question. We will start by looking at the exact distribution of the sum of an i.i.d. sample. We already know its mean and SD. In this chapter, we will study the shape of the distribution: the exact shape if we can calculate it, and a remarkable approximation...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_14/00_The_Central_Limit_Theorem.html",
        "teaser":null},{
        "title": "Exact Distribution",
        
        "excerpt":
            "Exact Distribution We already know how to find the distribution of the sum of any two discrete random variables. $$ P(X+Y = k) = \\sum_j P(X=j, Y=k-j) $$If $X$ and $Y$ are independent, this simplifies to become the discrete convolution formula: $$ P(X+Y = k) = \\sum_j P(X=j)P(Y=k-j) $$By induction, we can extend this to the sum of any finite number of independent variables. So in principle, we know how to find the distribution of the sum of $n$ independent random variables for $n &gt; 1$. However, this method can be hard to put into practice for large $n$. In...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_14/01_Exact_Distribution.html",
        "teaser":null},{
        "title": "PGFs in NumPy",
        
        "excerpt":
            "PGFs in NumPy Recall our algorithm to find the distribution of $S_n$, the sum of $n$ i.i.d. copies of a random variable $X_1$ that has values in a finite set of non-negative integers. Start with the pgf of $X_1$. Raise it to the power $n$. That's the pgf of $S_n$. Read the distribution of $S_n$ off the pgf. In this section we will use NumPy to carry out this algorithm. Let's start with an example. Suppose the distribution of $X_1$ is given by $p_0 = 0.1$, $p_1 = 0.5$, $p_2 = 0.4$. Let probs_X1 be an array containing the probabilities...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_14/02_PGFs_in_NumPy.html",
        "teaser":null},{
        "title": "Central Limit Theorem",
        
        "excerpt":
            "Central Limit Theorem As its name implies, this theorem is central to the fields of probability, statistics, and data science. It explains the normal curve that kept appearing in the previous section. Before we get to the theorem let's recall some facts from Data 8 and from earlier in this course. Standard Units As we have seen earlier, a random variable $X$ converted to standard units becomes $$ Z = \\frac{X - \\mu_X}{\\sigma_X} $$$Z$ measures how far $X$ is from the mean, in units of the SD. In other words $Z$ measures how many SDs above average the value of...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_14/03_Central_Limit_Theorem.html",
        "teaser":null},{
        "title": "The Sample Mean",
        
        "excerpt":
            "The Sample Mean What's central about the Central Limit Theorem? One answer is that it allows us to make inferences based on random samples when we don't know much about the distribution of the population. For data scientists, that's very valuable. In Data 8 you saw that if we want estimate the mean of a population, we can construct confidence intervals for the parameter based on the mean of a large random sample. In that course you used the bootstrap to generate an empirical distribution of the sample mean, and then used the empirical distribution to create the confidence interval....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_14/04_The_Sample_Mean.html",
        "teaser":null},{
        "title": "Confidence Intervals",
        
        "excerpt":
            "Confidence Intervals Suppose you have a large i.i.d. sample $X_1, X_2, \\ldots, X_n$, and let $\\bar{X}_n$ be the sample mean. The CLT implies that with chance about 95%, the sample mean is within 2 SDs of the population mean: $$ P\\big{(}\\bar{X}_n \\in (\\mu - 2\\frac{\\sigma}{\\sqrt{n}}, ~~~ \\mu + 2\\frac{\\sigma}{\\sqrt{n}}) \\big{)} ~ \\approx ~~ 0.95 $$ This can be expressed in a different way: $$ P\\big{(}\\vert \\bar{X}_n - \\mu \\vert &lt; 2\\frac{\\sigma}{\\sqrt{n}}\\big{)} ~ \\approx ~~ 0.95 $$Distance is symmetric, so this is the same as saying: $$ P\\big{(}\\mu \\in (\\bar{X}_n - 2\\frac{\\sigma}{\\sqrt{n}}, ~~~ \\bar{X}_n + 2\\frac{\\sigma}{\\sqrt{n}})\\big{)} ~ \\approx ~~ 0.95 $$That...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_14/05_Confidence_Intervals.html",
        "teaser":null},{
        "title": "Chapter 15: Continuous Distributions",
        
        "excerpt":
            "Continuous Distributions The normal curve, used by us as an approximation to discrete distributions, can be thought of as a distribution in its own right provided we allow the possible values to be the entire real line. In this chapter we will develop methods for working with random variables whose possible values are an interval of numbers. The interval of possible values might be bounded or unbounded, but even a bounded interval has uncountably many numbers in it. So we will use calculus to find probabilities and expectations. You will see that almost all the results we developed for discrete...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/00_Continuous_Distributions.html",
        "teaser":null},{
        "title": "Density and CDF",
        
        "excerpt":
            "Density and CDF Let $f$ be a non-negative function on the real number line and suppose $$ \\int_{-\\infty}^\\infty f(x)dx ~ = 1 $$Then $f$ is called a probability density function or just density for short. In the next section we will discuss the reason behind the name. For now, imagine the graph of $f$ as a kind of continuous probability histogram. We will soon make that precise. As an example, the function $f$ defined by $$ f(x) = \\begin{cases} 0 ~~~~~~~~~~~~~~~~~~ \\text{if } x \\le 0 \\\\ 6x(1-x) ~~~~~ \\text{if } 0 &lt; x &lt; 1 \\\\ 0 ~~~~~~~~~~~~~~~~~~ \\text{if...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/01_Density_and_CDF.html",
        "teaser":null},{
        "title": "The Meaning of Density",
        
        "excerpt":
            "The Meaning of Density When we work with a discrete random variable $X$, a natural component of our calculations is the chance that $X$ has a particular value $k$. That's the probability we denote by $P(X = k)$. What is the analog of $P(X = k)$ when $X$ has a density? If your answer is $P(X = x)$ for any number $x$, prepare to be disconcerted by the next paragraph. If $X$ Has a Density, Each Individual Value Has Probability 0 If $X$ has a density, then probabilities are defined as areas under the density curve. The area of a...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/02_The_Meaning_of_Density.html",
        "teaser":null},{
        "title": "Expectation",
        
        "excerpt":
            "Expectation Let $X$ have density $f$. Let $g$ be a real valued function on the real line, and suppose you want to find $E(g(X))$. Then you can follow a procedure analogous to the non-linear function rule we developed for finding expectations of functions of discrete random variables. Write a generic value of $X$: that's $x$. Apply the function $g$ to get $g(x)$. Weight $g(x)$ by the chance that $X$ is just around $x$, resulting in the product $g(x) \\cdot f(x)dx$. \"Sum\" over all $x$, that is, integrate. The expectation is $$ E(g(X)) ~ = ~ \\int_{-\\infty}^{\\infty} g(x)\\cdot f(x)dx $$ Technical...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/03_Expectation.html",
        "teaser":null},{
        "title": "Exponential Distribution",
        
        "excerpt":
            "Exponential Distribution A random variable $T$ has the exponential distribution with parameter $\\lambda$ if the density of $T$ is given by $$ f_T(t) ~ = \\lambda e^{-\\lambda t}, ~~~ t \\ge 0 $$The graph below shows the density $f_T$ for $\\lambda = 5$. CDF and Survival Function The exponential distribution is often used as a model for random lifetimes, in settings that we will study in greater detail below. For now, just think of $T$ as the lifetime of an object like a lightbulb, and note that the cdf at time $t$ can be thought of as the chance that...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/04_Exponential_Distribution.html",
        "teaser":null},{
        "title": "Calculus in SymPy",
        
        "excerpt":
            "Calculus in SymPy Working with densities involves calculus which can sometimes be time-consuming. This course gives you two ways of reducing the amount of calculus involved. Probabilistic methods can help reduce algebra and calculus. You've seen this with algebra in the discrete case. You'll see it with calculus as we learn more about densities. Python has a symbolic math module called SymPy that does algebra, calculus, and much other symbolic math. In this section we will show you how to do calculus using SymPy. We will demonstrate the methods in the context of an example. Suppose $X$ has density given...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/05_Calculus_in_SymPy.html",
        "teaser":null},{
        "title": "Review Problems: Set 3",
        
        "excerpt":
            "Review Problems: Set 3 These problems can be solved using the main ideas of Chapters 12 through 15. The Basics set will remind you of the fundamental concepts and some typical calculations. The rest are for you to further develop your problem solving skills and your fluency with the notation and ideas. The Basics 1. Let $X$ have the distribution given by $~~~~~~~~~~~~~~~~~~~~~~~~~x$ 1 2 3 $P(X=x)$ 0.2 0.5 0.3 Find $E(X)$ and $SD(X)$. 2. For random variables $X$ and $Y$, suppose $E(X) = 5$, $Var(X) = 2$ $E(Y) = -3$, $Var(Y) = 7$ Let $W = 8X - 9Y...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_15/06_Review_Problems_Set_3.html",
        "teaser":null},{
        "title": "Chapter 16: Transformations",
        
        "excerpt":
            "Transformations Let $X$ have a continuous distribution and let $Y = g(X)$ be a function of $X$. We know how to find $E(Y)$, assuming the expectation exists. In this chapter we will develop a method for finding the density of $Y$ in terms of $g$ and the density of $X$. The method is only valid for \"well behaved\" functions $g$. We will define what \"well behaved\" means in this context. It turns out that the class of well behaved functions is rich enough to cover a large set of interesting random variables. We will start by studying properties of the...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_16/00_Transformations.html",
        "teaser":null},{
        "title": "Linear Transformations",
        
        "excerpt":
            "Linear Transformations Let $T$ have the exponential $(\\lambda)$ distribution and let $T_1 = \\lambda T$. Then $T_1$ is a linear transformation of $T$. Therefore $$ E(T_1) = \\lambda E(T) = 1 ~~~ \\text{and} ~~~ SD(T_1) = \\lambda SD(T) = 1 $$The parameter $\\lambda$ has disappeared in these results. Let's see how that follows from the distribution of $T_1$. The cdf of $T_1$ is $$ F_{T_1}(t) = P(T_1 \\le t) = P(T \\le t/\\lambda) = 1 - e^{-\\lambda (t/\\lambda)} = 1 - e^{-t} $$That's the cdf of the exponential $(1)$ distribution, consistent with the expectation and SD we found above. To...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_16/01_Linear_Transformations.html",
        "teaser":null},{
        "title": "Monotone Functions",
        
        "excerpt":
            "Monotone Functions The method we have developed for finding the density of a linear function of a random variable can be extended to non-linear functions. We will start with a setting in which you have seen that applying non-linear functions to a random variable can have useful results. Simulation via the CDF In exercises, you have seen by simulation that you can generate a value of a random variable with a specified distribution by using the cdf of the distribution and a uniform (0, 1) random number. We will now establish the theory that underlies what you discovered by computation....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_16/02_Monotone_Functions.html",
        "teaser":null},{
        "title": "Two-to-One Functions",
        
        "excerpt":
            "Two-to-One Functions Let $X$ have density $f_X$. As you have seen, the random variable $Y = X^2$ comes up frequently in calculations. Thus far, all we have needed is $E(Y)$ which can be found by the formula for the expectation of a non-linear function of $X$. To find the density of $Y$, we can't directly use the change of variable formula of the previous section because the function $g(x) = x^2$ is not monotone. It is two-to-one because both $\\sqrt{x}$ and $-\\sqrt{x}$ have the same square. In this section we will find the density of $Y$ by developing a modification...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_16/03_Two_to_One_Functions.html",
        "teaser":null},{
        "title": "Chapter 17: Joint Densities",
        
        "excerpt":
            "    Joint Densities            We turn now to probability densities on the plane, that is, to joint distributions of two continuous random variables. The methods extend our calculations with densities of single variables, and are analogous to methods involving joint distribution tables in the discrete case.           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_17/00_Joint_Densities.html",
        "teaser":null},{
        "title": "Probabilities and Expectations",
        
        "excerpt":
            "Probabilities and Expectations A function $f$ on the plane is called a joint density if: $f(x, y) \\ge 0$ for all $x$, $y$ $\\int_x \\int_y f(x, y)dydx = 1$ If you think of $f$ as a surface, then the first condition says that the surface is on or above the plane. The second condition says that the total volume under the surface is 1. Think of probabilities as volumes under the surface, and define $f$ to be the joint density of random variables $X$ and $Y$ if $$ P((X, Y) \\in A) ~ = ~ \\mathop{\\int \\int}_A f(x,y)dydx ~~~~~ \\text{for...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_17/01_Probabilities_and_Expectations.html",
        "teaser":null},{
        "title": "Independence",
        
        "excerpt":
            "Independence Jointly distributed random variables $X$ and $Y$ are independent if $$ P(X \\in A, Y \\in B) = P(X \\in A)P(Y \\in B) $$ for all intervals $A$ and $B$. Let $X$ have density $f_X$, let $Y$ have density $f_Y$, and suppose $X$ and $Y$ are independent. Then if $f$ is the joint density of $X$ and $Y$, $$ \\begin{align*} f(x, y)dxdy &amp;\\sim P(X \\in dx, Y \\in dy) \\\\ &amp;= P(X \\in dx)P(Y \\in dy) ~~~~~ \\text{(independence)} \\\\ &amp;= f_X(x)dx f_Y(y)dy \\\\ &amp;= f_X(x)f_Y(y)dxdy \\end{align*} $$Thus if $X$ and $Y$ are independent then their joint density is given...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_17/02_Independence.html",
        "teaser":null},{
        "title": "Marginal and Conditional Densities",
        
        "excerpt":
            "Marginal and Conditional Densities Let random variables $X$ and $Y$ have the joint density defined by $$ f(x, y) ~ = ~ \\begin{cases} 30(y-x)^4, ~~~ 0 &lt; x &lt; y &lt; 1 \\\\ 0 ~~~~~~~~ \\text{otherwise} \\end{cases} $$ def jt_dens(x,y): if y &lt; x: return 0 else: return 30 * (y-x)**4 Plot_3d(x_limits=(0,1), y_limits=(0,1), f=jt_dens, cstride=4, rstride=4) Then the possible values of $(X, Y)$ are in the upper right hand triangle of the unit square. Here is a quick check by SymPy to see that the function $f$ is indeed a joint density. x = Symbol(&#39;x&#39;, positive=True) y = Symbol(&#39;y&#39;, positive=True)...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_17/03_Marginal_and_Conditional_Densities.html",
        "teaser":null},{
        "title": "Beta Densities with Integer Parameters",
        
        "excerpt":
            "Beta Densities with Integer Parameters In the previous section we learned how to work with joint densities, but many of the joint density functions seemed to appear out of nowhere. For example, we checked that the function $$ f(x, y) = 120x(y-x)(1-y), ~~~~ 0 &lt; x &lt; y &lt; 1 $$is a joint density, but there was no clue where it came from. In this section we will find its origin and go on to develop an important family of densities on the unit interval. Order Statistics of IID Uniform $(0, 1)$ Variables Let $U_1, U_2, \\ldots, U_n$ be i.i.d....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_17/04_Beta_Densities_with_Integer_Parameters.html",
        "teaser":null},{
        "title": "Chapter 18: The Normal and Gamma Families",
        
        "excerpt":
            "The Normal and Gamma Families The family of normal distributions has a uniquely important place in probability theory and data science. You started working with normal distributions in Data 8 long before you understood much formal probability theory. So you have taken several basic facts for granted without proof. We will start by establishing those facts and then study sums of independent normal variables. The gamma family of distributions, properties of which you have established in exercises, arises in numerous contexts. For example, you have seen that the exponential distribution is a member of the gamma family, as is the...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_18/00_The_Normal_and_Gamma_Families.html",
        "teaser":null},{
        "title": "Standard Normal: The Basics",
        
        "excerpt":
            "Standard Normal: The Basics Though we have accepted the formula for the standard normal density function since Data 8, we have never proved that it is indeed a density  that it integrates to 1. We have also not checked that its expectation exists, nor that its SD is 1. It's time to do all that and thereby ensure that our calculations involving normal densities are legitimate. We will start by recalling some facts about the apparently unrelated Rayleigh distribution, which we encountered as the distribution of the square root of an exponential variable. Let $T$ have the exponential $(1/2)$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_18/01_Standard_Normal_Basics.html",
        "teaser":null},{
        "title": "Sums of Independent Normal Variables",
        
        "excerpt":
            "Sums of Independent Normal Variables This section consists of examples based on one important fact: The sum of independent normal variables is normal. We will prove the fact in a later section using moment generating functions. For now, we will just run a quick simulation and then see how to use the fact in examples. mu_X = 10 sigma_X = 2 mu_Y = 15 sigma_Y = 3 x = stats.norm.rvs(mu_X, sigma_X, size=10000) y = stats.norm.rvs(mu_Y, sigma_Y, size=10000) s = x+y Table().with_column(&#39;S = X+Y&#39;, s).hist(bins=20) plt.title(&#39;$X$ is normal (10, $2^2$); $Y$ is normal (15, $3^2$) independent of $X$&#39;); The simulation above...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_18/02_Sums_of_Independent_Normal_Variables.html",
        "teaser":null},{
        "title": "The Gamma Family",
        
        "excerpt":
            "The Gamma Family You have seen in exercises that a non-negative random variable $X$ has the gamma $(r, \\lambda)$ distribution for two positive parameters $r$ and $\\lambda$ if the density of $X$ is given by $$ f_X(x) ~ = ~ \\frac{\\lambda^r}{\\Gamma(r)} x^{r-1}e^{-\\lambda x}, ~~~~~ x \\ge 0 $$Here $$ \\Gamma(r) ~ = ~ \\int_0^\\infty x^{r-1}e^{-x} dx $$is the Gamma function applied to $r$, and is part of the constant that makes the density integrate to 1. As you have shown, the key fact about the Gamma function is the recursion $$ \\Gamma(r+1) ~ = ~ r\\Gamma (r), ~~~~ r &gt;...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_18/03_The_Gamma_Family.html",
        "teaser":null},{
        "title": "Chi-Squared Distributions",
        
        "excerpt":
            "Chi-Squared Distributions Let $Z$ be a standard normal random variable and let $V = Z^2$. By the change of variable formula for densities, we found the density of $V$ to be $$ f_V(v) ~ = ~ \\frac{1}{\\sqrt{2\\pi}} v^{-\\frac{1}{2}} e^{-\\frac{1}{2} v}, ~~~~ v &gt; 0 $$That's the gamma $(1/2, 1/2)$ density. It is also called the chi-squared density with 1 degree of freedom, which we will abbreviate to chi-squared (1). From Chi-Squared $(1)$ to Chi-Squared $(n)$ When we were establishing the properties of the standard normal density, we discovered that if $Z_1$ and $Z_2$ are independent standard normal then $Z_1^2 +...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_18/04_Chi_Squared_Distributions.html",
        "teaser":null},{
        "title": "Review Problems: Set 4",
        
        "excerpt":
            "Review Problems: Set 4 These problems can be solved using the main ideas of Chapters 16 through 18. The Basics set will remind you of the fundamental concepts and some typical calculations. The rest are for you to further develop your problem solving skills and your fluency with the notation and ideas. You can leave answers in terms of the standard normal cdf $\\Phi$. You can also leave answers in terms of the Gamma function $\\Gamma$ unless you are asked for a simplification. The Basics 1. Let $X$ have density $f_X(x) = 2x$ for $0 &lt; x &lt; 1$. Find...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_18/05_Review_Problems_Set_4.html",
        "teaser":null},{
        "title": "Chapter 19: Distributions of Sums",
        
        "excerpt":
            "    Distributions of Sums            This chapter provides some general methods for working with sums of random variables, whether discrete or continuous.  We will start with the continuous analog of the convolution formula for the distribution of a sum of two independent discrete random variables.  We will then develop a more powerful version of the probability generating function that we defined earlier to study sums of independent random variables with finitely many non-negative integer values. The new version, called the moment generating function, will apply to discrete as well as continuous random variables, with finite or infinite sets of possible values.           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_19/00_Distributions_of_Sums.html",
        "teaser":null},{
        "title": "The Convolution Formula",
        
        "excerpt":
            "The Convolution Formula Let $X$ and $Y$ be discrete random variables and let $S = X+Y$. We know that a good way to find the distribution of $S$ is to partition the event $\\{ S = s\\}$ according to values of $X$. That is, $$ P(S = s) ~ = ~ \\sum_{\\text{all }x} P(X = x, Y = s-x) $$If $X$ and $Y$ are independent, this becomes the discrete convolution formula: $$ P(S = s) ~ = ~ \\sum_{\\text{all }x} P(X = x)P(Y = s-x) $$This formula has a straightforward continuous analog. Let $X$ and $Y$ be continuous random variables...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_19/01_Convolution_Formula.html",
        "teaser":null},{
        "title": "Moment Generating Functions",
        
        "excerpt":
            "Moment Generating Functions The probability mass function and probability density, cdf, and survival functions are all ways of specifying the probability distribution of a random variable. They are all defined as probabilities or as probability per unit length, and thus have natural interpretations and visualizations. But there are also more abstract ways of describing distributions. One that you have encountered is the probability generating function (pgf), which we defined for random variables with finitely many non-negative integer values. We now define another such transform of a distribution. More general than the pgf, it is a powerful tool for studying distributions....",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html",
        "teaser":null},{
        "title": "MGFs, the Normal, and the CLT",
        
        "excerpt":
            "MGFs, the Normal, and the CLT Let $Z$ be standard normal. Then the mgf of $Z$ is given by $$ M_Z(t) ~ = ~ e^{t^2/2} ~~~ \\text{for all } t $$To see this, just work out the integral: $$ \\begin{align*} M_Z(t) ~ &amp;= ~ \\int_{-\\infty}^\\infty e^{tz} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2} dz \\\\ \\\\ &amp;= ~ \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z^2 - 2tz)} dz \\\\ \\\\ &amp;= ~ e^{t^2/2} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z^2 - 2tz + t^2)} dz \\\\ \\\\ &amp;= ~ e^{t^2/2} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}(z- t)^2} dz \\\\ \\\\ &amp;= ~ e^{t^2/2} \\end{align*} $$because the integral is 1. It is the normal $(t, 1)$ density...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_19/03_MGFs_Normal_and_the_CLT.html",
        "teaser":null},{
        "title": "Chernoff Bound",
        
        "excerpt":
            "Chernoff Bound If the form of a distribution is intractable in that it is difficult to find exact probabilities by integration, then good estimates and bounds become important. Bounds on the tails of the distribution of a random variable help us quantify roughly how close to the mean the random variable is likely to be. We already know two such bounds. Let $X$ be a random variable with expectation $\\mu$ and SD $\\sigma$. Markov's Bound on the Right Hand Tail If $X$ is non-negative, $$ P(X \\ge c) ~ \\le ~ \\frac{\\mu}{c} $$Chebychev's Bound on Two Tails $$ P(\\lvert X...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_19/04_Chernoff_Bound.html",
        "teaser":null},{
        "title": "Chapter 20: Approaches to Estimation",
        
        "excerpt":
            "Approaches to Estimation In Data 8 we defined a parameter to be a number associated with a population or with a distribution in a model. In all of the inference we have done so far, we have assumed that a parameter is a fixed number, possibly unknown. We have developed methods of estimation that attempt to capture an unknown fixed parameter in a confidence interval based on the data in random draws from the population. We will start this chapter by developing a general method that allows us to come up with good estimates of fixed parameters. Essentially, it looks...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_20/00_Approaches_to_Estimation.html",
        "teaser":null},{
        "title": "Maximum Likelihood",
        
        "excerpt":
            "Maximum Likelihood Suppose you have an i.i.d. sample $X_1, X_2, \\ldots, X_n$ where the density of each $X_i$ depends on a parameter $\\theta$. Assume that $\\theta$ is fixed but unknown. The method of maximum likelihood estimates $\\theta$ by answering the following question: Among all the possible values of the parameter $\\theta$, which one maximizes the likeihood of getting our sample? That maximizing value of the parameter is called the maximum likelihood estimate or MLE for short. In this section we will develop a method for finding MLEs. Let's look at an example to illustrate the main idea. Suppose you know...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_20/01_Maximum_Likelihood.html",
        "teaser":null},{
        "title": "Prior and Posterior",
        
        "excerpt":
            "Prior and Posterior You are used to thinking of coin tosses as a sequence of i.i.d. Bernoulli $(p)$ variables for some fixed $p$. In an earlier section we showed that the sample proportion of successes $\\hat{p}$ is the MLE of the fixed but unknown $p$. Instead, suppose we think of $p$ as the result of a random draw from a distribution on the unit interval. That is, we assume the probability that the coin lands heads is a random variable $X$ with some density $f_X$ on $(0, 1)$. This is called the prior density of $X$. The prior density reflects...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_20/02_Prior_and_Posterior.html",
        "teaser":null},{
        "title": "Independence, Revisited",
        
        "excerpt":
            "Independence, Revisited In this section we will remind ourselves about what can happen to independence when parameters are randomized. First, let's go over some basics. Average Conditional Probabilities Let $X$ have density $f_X$ and let $A$ be an event. Then $$ P(A, X \\in dx) ~ = ~ P(X \\in dx)P(A \\mid X = x) ~ \\sim ~ f_X(x)dxP(A \\mid X = x) $$So $$ P(A) ~ = ~ \\int_{\\text{all x}} P(A, X \\in dx) ~ = ~ \\int_{\\text{all x}} P(A \\mid X = x)f_X(x)dx $$In more compact notation, $P(A) = E(P(A \\mid X))$. This is an example of finding...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_20/03_Independence_Revisited.html",
        "teaser":null},{
        "title": "Chapter 21: The Beta and the Binomial",
        
        "excerpt":
            "The Beta and the Binomial Connections between the beta and binomial families have acquired fundamental importance in machine learning. In the previous chapter, you began to see some of these connections. In this chapter we will generalize those observations. The experiment that we will study has two stages. Pick a value of $p$ according to a beta distribution Toss a coin that lands heads with the chosen probability $p$ We will see how the posterior distribution of the chance of heads is affected by the prior and by the data. After observing the results of $n$ tosses, we will make...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_21/00_The_Beta_and_the_Binomial.html",
        "teaser":null},{
        "title": "Updating and Prediction",
        
        "excerpt":
            "Updating and Prediction Let $X$ be a random variable with a beta density. Given $X=p$, toss a $p$-coin $n$ times and observe the number of heads. Based on the number of heads, we are going to: Identify the posterior distribution of $X$ Predict the chance of heads on the $(n+1)$st toss Beta Prior For positive integers $r$ and $s$, we derived the beta $(r, s)$ density $$ f(x) = \\frac{(r+s-1)!}{(r-1)!(s-1)!} x^{r-1}(1-x)^{s-1}, ~~~ 0 &lt; x &lt; 1 $$by studying order statistics of i.i.d. uniform $(0, 1)$ random variables. The beta family can be extended to include parameters $r$ and $s$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_21/01_Updating_and_Prediction.html",
        "teaser":null},{
        "title": "The Beta-Binomial Distribution",
        
        "excerpt":
            "The Beta-Binomial Distribution As in the previous section, let $X$ have the beta $(r, s)$ prior, and given $X = p$ let the $S_n$ be the number of heads in the first $n$ tosses of a $p$-coin. All the calculations we carried out in the previous section were under the condition that $S_n = k$, but we never needed to find the probability of this event. It was part of the constant that made the posterior density of $X$ integrate to 1. We can now find $P(S_n = k)$ by writing the posterior density in two ways: By recalling that...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_21/02_Beta_Binomial_Distribution.html",
        "teaser":null},{
        "title": "Long Run Proportion of Heads",
        
        "excerpt":
            "Long Run Proportion of Heads Fix $p$ for now, and consider a sequence of i.i.d. Bernoulli $(p)$ trials $I_1, I_2, \\ldots$. By the Weak Law of Large Numbers, the proportion of successes is likely to be close to $p$ in the number of trials is large. But something even stronger is true, and is known as the Strong Law: with probability 1, the sample proportion converges to $p$. Establishing the Strong Law is beyond the scope of this course, but we can easily visualize the result by simulation. We will start with the function binomial_proportion that takes $p$ and $n$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_21/03_Long_Run_Proportion_of_Heads.html",
        "teaser":null},{
        "title": "Chapter 22: Prediction",
        
        "excerpt":
            "    Prediction            This chapter is about predicting the value of one variable based on information about another. Our main predictor will be conditional expectation, which we defined earlier but will now examine as a guess for one variable given the value of another.  We will define a criterion for comparing predictions, and identify the best predictor using that criterion. In the process we will define a quantity called conditional variance and use it to find variance.           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_22/00_Prediction.html",
        "teaser":null},{
        "title": "Conditional Expectation As a Projection",
        
        "excerpt":
            "Conditional Expectation As a Projection Suppose we are trying to predict the value of a random variable $Y$ based on a related random variable $X$. As you saw in Data 8, a natural method of prediction is to use the \"center of the vertical strip\" at the given value of $X$. Formally, given $X=x$, we are proposing to predict $Y$ by $E(Y \\mid X=x)$. The conditional expectation $E(Y \\mid X)$ is the function of $X$ defined by $$ b(x) ~ = ~ E(Y \\mid X = x) $$We are using the letter $b$ to signifiy the \"best guess\" of $Y$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_22/01_Conditional_Expectation_Projection.html",
        "teaser":null},{
        "title": "Variance by Conditioning",
        
        "excerpt":
            "Variance by Conditioning Iteration allows us to find expectation by conditioning. We now have the tools to find variance by conditioning as well. Recall the notation of the previous section: $X$ and $Y$ are jointly distributed random variables $b(X) = E(Y \\mid X)$ $D_w = Y - b(X)$ Define $D_Y = Y - E(Y)$. Then $$ D_Y ~ = ~ D_w + (b(X) - E(Y)) ~ = ~ D_w + D_b $$where $D_b = b(X) - E(Y)$ is the deviation of the random variable $b(X)$ from its expectation $E(Y)$. In the graph below, the black line is at the level...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_22/02_Variance_by_Conditioning.html",
        "teaser":null},{
        "title": "Examples",
        
        "excerpt":
            "Examples This section is a workout in finding expectation and variance by conditioning. As before, if you are trying to find a probability, expectation, or variance, and you think, \"If only I knew the value of this other random variable, I'd have the answer,\" then that's a sign that you should consider conditioning on that other random variable. Mixture of Two Distributions Let $X$ have mean $\\mu_X$ and SD $\\sigma_X$. Let $Y$ have mean $\\mu_Y$ and SD $\\sigma_Y$. Now let $p$ be a number between 0 and 1, and define the random variable $M$ as follows. $$ M = \\begin{cases}...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_22/03_Examples.html",
        "teaser":null},{
        "title": "Least Squares Predictor",
        
        "excerpt":
            "Least Squares Predictor As the function that picks off the \"centers of vertical strips,\" the conditional expectation $b(X) = E(Y \\mid X)$ is a natural estimate or predictor of $Y$ given the value of $X$. We will now see how good $b(X)$ is if we use mean squared error as our criterion. Minimizing the MSE Let $h(X)$ be any function of $X$, and consider using $h(X)$ to predict $Y$. Define the mean squared error of the predictor $h(X)$ to be $$ MSE(h) ~ = ~ E\\Big{(}\\big{(}Y - h(X)\\big{)}^2\\Big{)} $$We will show that $b(X)$ is the best predictor of $Y$ based...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_22/04_Least_Squares_Predictor.html",
        "teaser":null},{
        "title": "Chapter 23: Jointly Normal Random Variables",
        
        "excerpt":
            "Jointly Normal Random Variables Galton's observations about oval scatter plots became the foundation of multiple regression, one of most commonly used methods in data analysis. Inference in multiple regression and its modern variants is often based on multivariate normal models. In this chapter we will study what it means for a collection of random variables to be jointly normally distributed. We will introduce matrix notation for linear combinations of random variables and then study the main properties of the multivariate normal distribution. This is the necessary groundwork for using multivariate normal models in prediction, which we will do in the...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_23/00_Multivariate_Normal_RVs.html",
        "teaser":null},{
        "title": "Random Vectors",
        
        "excerpt":
            "Random Vectors A vector valued random variable, or more simply, a random vector, is a list of random variables defined on the same space. We will think of it as a column. $$ \\mathbf{X} ~ = ~ \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix} $$ For ease of display, we will sometimes write $\\mathbf{X} = [X_1 X_2 \\ldots X_n]^T$ where $\\mathbf{M}^T$ is notation for the transpose of the matrix $\\mathbf{M}$. The mean vector of $\\mathbf{X}$ is $\\boldsymbol{\\mu} = [\\mu_1 ~ \\mu_2 ~ \\ldots ~ \\mu_n]^T$ where $\\mu_i = E(X_i)$. The covariance matrix of $\\mathbf{X}$ is the $n \\times...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_23/01_Random_Vectors.html",
        "teaser":null},{
        "title": "Multivariate Normal Distribution",
        
        "excerpt":
            "Multivariate Normal Distribution Let $\\boldsymbol{\\Sigma}$ be a positive definite matrix. An $n$-dimensional random vector $\\mathbf{X}$ has the multivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ if the joint density of the elements of $\\mathbf{X}$ is given by $$ f_\\mathbf{X}(\\mathbf{x}) ~ = ~ \\frac{1}{(\\sqrt{2\\pi})^n \\sqrt{\\det(\\boldsymbol{\\Sigma})} } \\exp\\big{(}-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\big{)} $$We will say that the elements of $\\mathbf{X}$ are jointly normal or jointly Gaussian. You should check that the formula is correct when $n = 1$. In this case $\\boldsymbol{\\Sigma} = [\\sigma^2]$ is just a scalar. It is a number, not a larger matrix; its determinant is...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_23/02_Multivariate_Normal_Distribution.html",
        "teaser":null},{
        "title": "Linear Combinations",
        
        "excerpt":
            "Linear Combinations Let $\\mathbf{X}$ be multivariate normal with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$. Definition 3 says that all linear combinations of elements of $\\mathbf{X}$ are normal too. This makes many calculations straightforward. Here is an example in two dimensions. Sum and Difference Let $\\mathbf{X} = [X_1 ~ X_2]^T$ have bivariate normal distribution mean vector $\\boldsymbol{\\mu} = [\\mu_1 ~ \\mu_2]^T$ and covariance matrix $\\boldsymbol{\\Sigma}$. Then the sum $S = X_1 + X_2$ has the normal distribution with mean $\\mu_1 + \\mu_2$ and variance $$ Var(S) ~ = ~ Var(X_1) + Var(X_2) + 2Cov(X_1, X_2) $$which you can calculate based...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_23/03_Linear_Combinations.html",
        "teaser":null},{
        "title": "Independence",
        
        "excerpt":
            "Independence If the elements of $\\mathbf{X}$ are mutually independent then $Cov(X_i, X_j) = 0$ for all $i \\ne j$ and hence the covariance matrix $\\boldsymbol{\\Sigma}$ is a diagonal matrix and the $i$th diagonal element is $Var(X_i)$. In the other direction, zero covariance doesn't imply independence, and pairwise independence doesn't imply mutual independence. But the multivariate normal is a wonderful distribution: If $\\mathbf{X}$ is multivariate normal and its elements are pairwise uncorrelated  that is, $Cov(X_i, X_j) = 0$ for all $i \\ne j$  then the elements of $\\mathbf{X}$ are mutually independent. That is, multivariate normal random variables are independent...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_23/04_Independence.html",
        "teaser":null},{
        "title": "Chapter 24: Simple Linear Regression",
        
        "excerpt":
            "Simple Linear Regression In data science, regression models are widely used for prediction. This chapter examines linear least squares from a probabilistic perspective. The focus is on simple regression, that is, prediction based on one numerical attribute. When the joint distribution of the attribute $X$ and the response $Y$ is bivariate normal, the empirical distribution of $(X, Y)$ has the football shape so familiar from Data 8. We will start with a geometric interpretation of correlation, as that is helpful for understanding both regression and the bivariate normal. The equation of the regression line, which we will derive, can be...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_24/00_Simple_Linear_Regression.html",
        "teaser":null},{
        "title": "Bivariate Normal Distribution",
        
        "excerpt":
            "Bivariate Normal Distribution The multivariate normal distribution is defined in terms of a mean vector and a covariance matrix. The units of covariance are often hard to understand, as they are the product of the units of the two variables. Normalizing the covariance so that it is easier to interpret is a good idea. As you have seen in exercises, for jointly distributed random variables $X$ and $Y$ the correlation between $X$ and $Y$ is defined as $$ r_{X,Y} ~ = ~ \\frac{Cov(X, Y)}{\\sigma_X\\sigma_Y} ~ = ~ E\\Big{(} \\frac{X-\\mu_X}{\\sigma_X} \\cdot \\frac{Y-\\mu_Y}{\\sigma_Y} \\Big{)} ~ = ~ E(X^*Y^*) $$where $X^\\*$ is $X$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_24/01_Bivariate_Normal_Distribution.html",
        "teaser":null},{
        "title": "Least Squares Linear Predictor",
        
        "excerpt":
            "Least Squares Linear Predictor In this section we are going to step away from the bivariate normal distribution and see if we can identify the best among all linear predictors of one numerical variable based on another, regardless of the joint distribution of the two variables. For jointly distributed random variables $X$ and $Y$, you know that $E(Y \\mid X)$ is the least squares predictor of $Y$ based on functions of $X$. We will now restrict the allowed functions to linear functions and see if we can find the best among those. In the next section we will see the...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_24/02_Linear_Least_Squares.html",
        "teaser":null},{
        "title": "Regression and the Bivariate Normal",
        
        "excerpt":
            "Regression and the Bivariate Normal Let $X$ and $Y$ be standard bivariate normal with correlatin $\\rho$. The relation $$ Y ~ = ~ \\rho X + \\sqrt{1 - \\rho^2}Z $$where $X$ and $Z$ are independent standard normal variables leads directly the best predictor of $Y$ based on all functions of $X$. You know that the best predictor is the conditional expectation $E(Y \\mid X)$, and clearly, $$ E(Y \\mid X) ~ = ~ \\rho X $$because $Z$ is independent of $X$ and $E(Z) = 0$. Because $E(Y \\mid X)$ is a linear function of $X$, we have shown: If $X$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_24/03_Regression_and_Bivariate_Normal.html",
        "teaser":null},{
        "title": "The Regression Equation",
        
        "excerpt":
            "The Regression Equation The equation of the regression line for predicting $Y$ based on $X$ can be written in several equivalent ways. The regression equation, and the error in the regression estimate, are best understood in standard units. All the other representations follow by straightforward algebra. Let $X$ and $Y$ be bivariate normal with parameters $(\\mu_X, \\mu_Y, \\sigma_X^2, \\sigma_Y^2, \\rho)$. Then, as we have seen, the best predictor $E(Y \\mid X)$ is a linear function of $X$ and hence the formula for $E(Y \\mid X)$ is also the equation of the regression line. In Standard Units Let $X^\\*$ be $X$...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_24/04_Regression_Equation.html",
        "teaser":null},{
        "title": "Chapter 25: Multiple Regression",
        
        "excerpt":
            "Multiple Regression The most common use of regression is to predict the value of a numerical variable based on the values of several other variables. In our probabilistic setting, we have a finite number of jointly distributed random variables and the goal is to predict one of them based on all the others. In the previous chapters we did this in the case where there was just one predcitor variable. In particular, we developed the theory of simple linear regression. In this chapter we will extend our calculations for simple regression to the case of multiple regression. Indeed, we will...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_25/00_Multiple_Regression.html",
        "teaser":null},{
        "title": "Bilinearity in Matrix Notation",
        
        "excerpt":
            "Bilinearity in Matrix Notation As a preliminary to regression, we will express bilinearity in a compact form using matrix notation. The results of this section are not new. They are simply restatements of familiar results about variances and covariances, using new notation and matrix representations. Let $\\mathbf{X}$ be a $p \\times 1$ vector of predictor variables. We know that for an $m \\times p$ matrix $\\mathbf{A}$ and an $m \\times 1$ vector $\\mathbf{b}$, $$ Var(\\mathbf{AX} + \\mathbf{b}) ~ = ~ \\mathbf{A}\\boldsymbol{\\Sigma}_\\mathbf{X} \\mathbf{A}^T $$The results below are special cases of this. Linear Combinations To define two generic linear combinations of elements...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_25/01_Bilinearity_in_Matrix_Notation.html",
        "teaser":null},{
        "title": "Best Linear Predictor",
        
        "excerpt":
            "Best Linear Predictor Let $Y$ and the $p \\times 1$ vector $\\mathbf{X}$ be jointly distributed, and suppose you are trying to predict $Y$ based on a linear function of $\\mathbf{X}$. For the predictor $$ \\hat{Y}_{\\mathbf{c}, d} ~ = \\mathbf{c}^T\\mathbf{X} + d $$the mean squared error of prediction is $$ MSE(\\hat{Y}_{\\mathbf{c}, d}) ~ = ~ E\\big{(} (Y - \\hat{Y}_{\\mathbf{c}, d})^2 \\big{)} $$In this section we will identify the linear predictor that minimizes the mean squared error. We will also find the variance of the error made by this best predictor. A Linear Predictor In the case of simple regression, we found...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_25/02_Best_Linear_Predictor.html",
        "teaser":null},{
        "title": "Conditioning and the Multivariate Normal",
        
        "excerpt":
            "Conditioning and the Multivariate Normal Whe $Y$ and $\\mathbf{X}$ have a multivariate normal distribution with positive definite covariance matrix, then best linear predictor derived in the previous section is the best among all predictors of $Y$ based on $\\mathbf{X}$. That is, $$ ~E(Y \\mid \\mathbf{X}) = \\boldsymbol{\\Sigma}_{Y, \\mathbf{X}}\\boldsymbol{\\Sigma}_\\mathbf{X}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu}_\\mathbf{X}) + \\mu_Y $$$$ Var(Y \\mid \\mathbf{X}) = \\sigma_Y^2 - \\boldsymbol{\\Sigma}_{Y, \\mathbf{X}}\\boldsymbol{\\Sigma}_\\mathbf{X}^{-1} \\boldsymbol{\\Sigma}_{\\mathbf{X}, Y} $$Also, the conditional distribution of $Y$ given $\\mathbf{X}$ is normal. These results are extensions of those in the case where $Y$ was predicted based on just one predictor $X$. To prove them, you need some linear...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_25/03_Multivariate_Normal_Conditioning.html",
        "teaser":null},{
        "title": "Multiple Regression",
        
        "excerpt":
            "Multiple Regression Regression provides one way of predicting a numerical variable, called a response, based on other variables called predictor variables. The multiple regression model says in essence that $$ \\text{response} ~ = ~ \\text{linear combination of predictor variables} + \\text{ random noise } $$You can think of the first term on the right hand side as a signal. The problem is that we don't get to observe the signal. The observed response is the sum of the signal and the noise. The data scientist's task is to use the observations to extract the signal as accurately as possible. It...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_25/04_Multiple_Regression.html",
        "teaser":null},{
        "title": "Further Review Exercises",
        
        "excerpt":
            "Further Review Exercises Many of these exercises require the material of Chapter 19 onwards, which of course relies on the material of the previous chapters. However, some of them can be solved using earlier material alone. According to students and alumni, some of the exercises have appeared as questions in \"quant\" interviews. 1. A coin lands heads with probability $p$. Let $X$ be the number of tosses till the first head appears and let $Y$ be the number of tails before the first head. (a) Find the moment generating function of $X$. (b) Use the answer to (a) to find...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/Chapter_25/05_Further_Review_Exercises.html",
        "teaser":null},{
        "title": "Authors and License",
        
        "excerpt":
            "    Prob 140  Probability for Data Science  By Ani Adhikari and Jim Pitman  This is the textbook for the Probability for Data Science class at UC Berkeley.  The contents of this book are licensed for free consumption under the following license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)           ",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/README.html",
        "teaser":null},{
        "title": "To the Student",
        
        "excerpt":
            "To the Student Data science is about making conclusions based on large amounts of data of diverse forms. But most data sets, no matter how large or complex, provide only incomplete information about the questions that interest us. When data scientists make conclusions, they often have to account for uncertainty. Probability theory studies the nature of randomness and gives us ways to quantify uncertainty. For example, it helps explain a phenomenon familiar to you from Data 8: that large random samples can hold rich stores of information about unknown quantities. Data scientists can use this to make valid conclusions about...",
        "categories": [],
        "tags": [],
        "url": "http://prob140.org/textbook/To_the_Student.html",
        "teaser":null},]
</script>
            </div>
            <nav class="c-page__nav">
  

  
</nav>

            <footer>
  <p class="footer"></p>
</footer>

        </div>
      </main>
    </div>
  </body>
</html>
