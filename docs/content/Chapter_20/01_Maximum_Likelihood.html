

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>20.1. Maximum Likelihood &#8212; Prob 140 Textbook</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/headers.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="20.2. Independence, Revisited" href="02_Independence_Revisited.html" />
    <link rel="prev" title="20. Approaches to Estimation" href="00_Approaches_to_Estimation.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Prob 140 Textbook</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="http://prob140.org">
   Course Home
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../To_the_Student.html">
   To the  Student
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_01/00_Fundamentals.html">
   1. Fundamentals
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">
     1.1. Outcome Space and Events
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">
     1.2. Equally Likely Outcomes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">
     1.3. Collisions in Hashing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">
     1.4. The Birthday Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">
     1.5. An Exponential Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/06_Exercises.html">
     1.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">
   2. Calculating Chances
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/01_Addition.html">
     2.1. Addition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/02_Examples.html">
     2.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/03_Multiplication.html">
     2.3. Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/04_More_Examples.html">
     2.4. More Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">
     2.5. Updating Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/06_Exercises.html">
     2.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_03/00_Random_Variables.html">
   3. Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">
     3.1. Functions on an Outcome Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/02_Distributions.html">
     3.2. Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/03_Equality.html">
     3.3. Equality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/04_Exercises.html">
     3.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">
   4. Relations Between Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">
     4.1. Joint Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/02_Examples.html">
     4.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">
     4.3. Marginal Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">
     4.4. Conditional Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">
     4.5. Dependence and Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/06_Exercises.html">
     4.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">
   5. Collections of Events
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">
     5.1. Bounding the Chance of a Union
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">
     5.2. Inclusion-Exclusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">
     5.3. The Matching Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">
     5.4. Sampling Without Replacement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/05_Exercises.html">
     5.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_06/00_Random_Counts.html">
   6. Random Counts
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">
     6.1. The Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/02_Examples.html">
     6.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/03_Multinomial_Distribution.html">
     6.3. Multinomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/04_The_Hypergeometric_Revisited.html">
     6.4. The Hypergeometric, Revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/05_Odds_Ratios.html">
     6.5. Odds Ratios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/06_Law_of_Small_Numbers.html">
     6.6. The Law of Small Numbers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/07_Exercises.html">
     6.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_07/00_Poissonization.html">
   7. Poissonization
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/01_Poisson_Distribution.html">
     7.1. Poisson Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Binomial.html">
     7.2. Poissonizing the Binomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/03_Poissonizing_the_Multinomial.html">
     7.3. Poissonizing the Multinomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/04_Exercises.html">
     7.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_08/00_Expectation.html">
   8. Expectation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/01_Definition.html">
     8.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/02_Applying_the_Definition.html">
     8.2. Applying the Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/03_Expectations_of_Functions.html">
     8.3. Expectations of Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/04_Additivity.html">
     8.4. Additivity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/05_Method_of_Indicators.html">
     8.5. Method of Indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/06_Exercises.html">
     8.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">
   9. Conditioning, Revisited
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">
     9.1. Probability by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">
     9.2. Expectation by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">
     9.3. Expected Waiting Times
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/04_Exercises.html">
     9.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">
   10. Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/01_Transitions.html">
     10.1. Transitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">
     10.2. Deconstructing Chains
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">
     10.3. Long Run Behavior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/04_Examples.html">
     10.4. Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_11/00_Markov_Chain_Monte_Carlo.html">
   11. Markov Chain Monte Carlo
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/01_Balance_and_Detailed_Balance.html">
     11.1. Balance and Detailed Balance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/02_Code_Breaking.html">
     11.2. Code Breaking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/03_Metropolis_Algorithm.html">
     11.3. Metropolis Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/04_Exercises.html">
     11.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">
   12. Standard Deviation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/01_Definition.html">
     12.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">
     12.2. Prediction and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/03_Bounds.html">
     12.3. Tail Bounds
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">
     12.4. Heavy Tails
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/05_Exercises.html">
     12.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">
   13. Variance Via Covariance
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/01_Covariance.html">
     13.1. Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/02_Properties_of_Covariance.html">
     13.2. Properties of Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/03_Sums_of_Independent_Variables.html">
     13.3. Sums of Independent Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/04_Symmetry_and_Indicators.html">
     13.4. Symmetry and Indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/05_Finite_Population_Correction.html">
     13.5. Finite Population Correction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/06_Exercises.html">
     13.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">
   14. The Central Limit Theorem
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/01_Exact_Distribution_of_a_Sum.html">
     14.1. Exact Distribution of a Sum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">
     14.2. PGFs in NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">
     14.3. Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/04_SciPy_and_Normal_Curves.html">
     14.4. SciPy and Normal Curves
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/05_The_Sample_Mean.html">
     14.5. The Sample Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/06_Confidence_Intervals.html">
     14.6. Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/07_Exercises.html">
     14.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">
   15. Continuous Distributions
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">
     15.1. Density and CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">
     15.2. The Meaning of Density
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/03_Expectation.html">
     15.3. Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">
     15.4. Exponential Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">
     15.5. Calculus in SymPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/06_Exercises.html">
     15.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_16/00_Transformations.html">
   16. Transformations
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">
     16.1. Linear Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">
     16.2. Monotone Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/03_Simulation_via_the_CDF.html">
     16.3. Simulation via the CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/04_Two_to_One_Functions.html">
     16.4. Two-to-One Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/05_Exercises.html">
     16.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">
   17. Joint Densities
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">
     17.1. Probabilities and Expectations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/02_Independence.html">
     17.2. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">
     17.3. Marginal and Conditional Densities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">
     17.4. Beta Densities with Integer Parameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/05_Exercises.html">
     17.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">
   18. The Normal and Gamma Families
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">
     18.1. Standard Normal: The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">
     18.2. Sums of Independent Normal Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">
     18.3. The Gamma Family
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">
     18.4. Chi-Squared Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/05_Exercises.html">
     18.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">
   19. Distributions of Sums
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">
     19.1. The Convolution Formula
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">
     19.2. Moment Generating Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">
     19.3. MGFs, the Normal, and the CLT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">
     19.4. Chernoff Bound
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/05_Exercises.html">
     19.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="00_Approaches_to_Estimation.html">
   20. Approaches to Estimation
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     20.1. Maximum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_Independence_Revisited.html">
     20.2. Independence, Revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_Prior_and_Posterior.html">
     20.3. Prior and Posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_Exercises.html">
     20.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">
   21. The Beta and the Binomial
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">
     21.1. Updating and Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">
     21.2. The Beta-Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">
     21.3. Long Run Proportion of Heads
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/04_Exercises.html">
     21.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_22/00_Prediction.html">
   22. Prediction
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">
     22.1. Conditional Expectation As a Projection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/02_Variance_by_Conditioning.html">
     22.2. Variance by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/03_Examples.html">
     22.3. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/04_Least_Squares_Predictor.html">
     22.4. Least Squares Predictor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">
   23. Jointly Normal Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">
     23.1. Random Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Distribution.html">
     23.2. Multivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/03_Linear_Combinations.html">
     23.3. Linear Combinations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/04_Independence.html">
     23.4. Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">
   24. Simple Linear Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/01_Bivariate_Normal_Distribution.html">
     24.1. Bivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/02_Linear_Least_Squares.html">
     24.2. Least Squares Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">
     24.3. Regression and the Bivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">
     24.4. The Regression Equation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_25/00_Multiple_Regression.html">
   25. Multiple Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/01_Bilinearity_in_Matrix_Notation.html">
     25.1. Bilinearity in Matrix Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/02_Best_Linear_Predictor.html">
     25.2. Best Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/03_Multivariate_Normal_Conditioning.html">
     25.3. Conditioning and the Multivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/04_Multiple_Regression.html">
     25.4. Multiple Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/05_Further_Review_Exercises.html">
     25.5. Further Review Exercises
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Chapter_20/01_Maximum_Likelihood.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_20/01_Maximum_Likelihood.ipynb&branch=gh-pages"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> On this page
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample">
   20.1.1. Maximum Likelihood Estimate of
   <span class="math notranslate nohighlight">
    \(p\)
   </span>
   Based on a Bernoulli
   <span class="math notranslate nohighlight">
    \((p)\)
   </span>
   Sample
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample">
   20.1.2. MLE of
   <span class="math notranslate nohighlight">
    \(\mu\)
   </span>
   Based on a Normal
   <span class="math notranslate nohighlight">
    \((\mu, \sigma^2)\)
   </span>
   Sample
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps-for-finding-the-mle">
   20.1.3. Steps for Finding the MLE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties-of-the-mle">
   20.1.4. Properties of the MLE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">
   20.1.5. MLEs of
   <span class="math notranslate nohighlight">
    \(\mu\)
   </span>
   and
   <span class="math notranslate nohighlight">
    \(\sigma\)
   </span>
   Based on a Normal
   <span class="math notranslate nohighlight">
    \((\mu, \sigma^2)\)
   </span>
   Sample
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="maximum-likelihood">
<h1><span class="section-number">20.1. </span>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h1>
<p>Suppose you have an i.i.d. sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> where the distribution of each <span class="math notranslate nohighlight">\(X_i\)</span> depends on a parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Assume that <span class="math notranslate nohighlight">\(\theta\)</span> is fixed but unknown. The method of <em>maximum likelihood</em> estimates <span class="math notranslate nohighlight">\(\theta\)</span> by answering the following question:</p>
<p><strong>Among all the possible values of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>, which one maximizes the likeihood of getting our sample?</strong></p>
<p>That maximizing value of the parameter is called the <em>maximum likelihood estimate</em> or MLE for short. In this section we will develop a method for finding MLEs.</p>
<p>Let’s look at an example to illustrate the main idea. Suppose you toss a coin that lands heads with a fixed but unknown probability <span class="math notranslate nohighlight">\(p\)</span>, and you observe the sequence HHHTHT.</p>
<p>Now suppose I propose two estimates of <span class="math notranslate nohighlight">\(p\)</span>: one estimate is <span class="math notranslate nohighlight">\(0.6\)</span>, and one estimate is <span class="math notranslate nohighlight">\(0.2\)</span>. Which would you say is better, and why?</p>
<p>Between these two, you would pick <span class="math notranslate nohighlight">\(0.6\)</span> as better, because a coin that lands heads with chance <span class="math notranslate nohighlight">\(0.6\)</span> <em>is more likely to generate the observed data</em> than a coin that lands heads with chance 0.2.</p>
<p>Your choice is based on the likelihood of the data under each of the two proposed values of <span class="math notranslate nohighlight">\(p\)</span>: the one that makes the data more likely wins.</p>
<p>Of course, <span class="math notranslate nohighlight">\(p\)</span> could be any number between in the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>. To find the best among all of these, using the criterion we have just developed, we have to find the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the function</p>
<div class="math notranslate nohighlight">
\[
p ~ \to ~ p \cdot p \cdot p \cdot (1-p) \cdot p \cdot (1-p) ~ = ~ p^4(1-p)^2
\]</div>
<p>Here is a graph of this function of <span class="math notranslate nohighlight">\(p\)</span>. Clearly, <span class="math notranslate nohighlight">\(0.6\)</span> is a better choice of estimate of <span class="math notranslate nohighlight">\(p\)</span> than <span class="math notranslate nohighlight">\(0.2\)</span>. But there’s one that’s even better.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/01_Maximum_Likelihood_4_0.png" src="../../_images/01_Maximum_Likelihood_4_0.png" />
</div>
</div>
<p>You can see that the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the likelihood looks suspiciously like <span class="math notranslate nohighlight">\(2/3\)</span>, the observed proportion of heads in our data HHHTHT. Let’s see why that is true.</p>
<div class="cell tag_remove-input tag_hide-output docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="400"
    height="300"
    src="https://www.youtube.com/embed/AN6y89dfNCM"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
<div class="section" id="maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample">
<h2><span class="section-number">20.1.1. </span>Maximum Likelihood Estimate of <span class="math notranslate nohighlight">\(p\)</span> Based on a Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> Sample<a class="headerlink" href="#maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> sample. Our goal is to find the MLE of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>The random variables are discrete, so the likelihood function is defined as the joint probability mass function evaluated at the sample, as a function of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>In our example,</p>
<div class="math notranslate nohighlight">
\[
Lik(p) ~ = ~ p \cdot p \cdot p \cdot (1-p) \cdot p \cdot (1-p) ~ = ~ p^4(1-p)^2
\]</div>
<p>The likelihood depends on the number of 1’s, just as in the binomial probability formula. The combinatorial term is missing because we are observing each element of the sequence.</p>
<p>You’ll soon see the reason for using the strange notation <span class="math notranslate nohighlight">\(Lik\)</span>. Please just accept it for now.</p>
<p>Notice that the likelihood function depends on the data. Therefore, the value of the function is a random variable. For a general i.i.d. Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> sample, the likelihood function is calculated as follows.</p>
<p><strong>Likelihood function: Discrete Case</strong></p>
<p>Let <span class="math notranslate nohighlight">\(X = X_1 + X_2 + \ldots + X_n\)</span> be the number of 1’s in the sample. The likelihood function is</p>
<div class="math notranslate nohighlight">
\[
Lik(p) = p^X (1-p)^{n-X}
\]</div>
<p>For each <span class="math notranslate nohighlight">\(p\)</span>, the value of <span class="math notranslate nohighlight">\(Lik(p)\)</span> is the likelihood of the data if <span class="math notranslate nohighlight">\(p\)</span> is the probability of heads.</p>
<p>Our goal is to find the value <span class="math notranslate nohighlight">\(\hat{p}\)</span> that maximizes this likelihood over all the possible values of <span class="math notranslate nohighlight">\(p\)</span>, that is, over the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<p>One way to do this is by calculus. To make the calculus simpler, we recall a crucial observation we have made before:</p>
<p>Taking the <span class="math notranslate nohighlight">\(\log\)</span> turns the product into a sum, which simplifies calculation. Also, <span class="math notranslate nohighlight">\(\log\)</span> is an increasing function. Hence <strong>the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the likelihood function is the same as the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the log of the likelihood function.</strong></p>
<p><strong>Log-likelihood function</strong></p>
<p>Let <span class="math notranslate nohighlight">\(L\)</span> be the log of the likelihood function, also known as the <em>log likelihood function</em>. You can see the letter l appearing repeatedly in the terminology. Since we’ll be doing most of our work with the log likelihood function, we are calling it <span class="math notranslate nohighlight">\(L\)</span> and using <span class="math notranslate nohighlight">\(Lik\)</span> for the likelihood function.</p>
<div class="math notranslate nohighlight">
\[
L(p) = X\log(p) + (n-X)\log(1-p)
\]</div>
<p>The function <span class="math notranslate nohighlight">\(L\)</span> is easier to work with than <span class="math notranslate nohighlight">\(Lik\)</span>. We just have to carry out the calculus.</p>
<p><strong>Differentiate the log-likelihood function with respect to <span class="math notranslate nohighlight">\(p\)</span>:</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dp} L(p) = \frac{X}{p} - \frac{n-X}{1-p}
\]</div>
<p>The <em>maximum likelihood estimate</em> (MLE) of <span class="math notranslate nohighlight">\(p\)</span> is the value <span class="math notranslate nohighlight">\(\hat{p}\)</span> that maximizes the log-likelihood <span class="math notranslate nohighlight">\(L\)</span>. Statisticians have long used the “hat” symbol to denote estimates.</p>
<p><strong>Set the derivative equal to 0 and solve for the MLE</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{X}{\hat{p}} - \frac{n-X}{1-\hat{p}} = 0
\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
(1-\hat{p})X = (n-X)\hat{p} ~~~~~ \text{so} ~~~~~ X = n\hat{p}
\]</div>
<p>Therefore the MLE of <span class="math notranslate nohighlight">\(p\)</span> is</p>
<div class="math notranslate nohighlight">
\[ 
\hat{p} = \frac{X}{n} = \frac{1}{n}\sum_{i=1}^n X_i
\]</div>
<p>That is, the MLE of <span class="math notranslate nohighlight">\(p\)</span> is the sample proportion of 1’s. To compute this estimate, all you need is the number of 1’s in the sample. You don’t need to see the entire sample as a sequence of 0’s and 1’s.</p>
<p>Because the MLE <span class="math notranslate nohighlight">\(\hat{p}\)</span> is the sample proportion, it is unbiased, has SD <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n}\)</span>, and is asymptotically normal. When <span class="math notranslate nohighlight">\(n\)</span> is large you can estimate the SD based on the sample and therefore construct confidence intervals for <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>To be very careful, we should check that this calculation yields a maximum and not a minimum, but given the answer you will surely accept that it’s a max. You are welcome to take the second derivative of <span class="math notranslate nohighlight">\(L\)</span> and check that we do indeed have a maximum.</p>
<div class="cell tag_remove-input tag_hide-output docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="400"
    height="300"
    src="https://www.youtube.com/embed/KPVK4t58zPY"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
</div>
<div class="section" id="mle-of-mu-based-on-a-normal-mu-sigma-2-sample">
<h2><span class="section-number">20.1.2. </span>MLE of <span class="math notranslate nohighlight">\(\mu\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample<a class="headerlink" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample. The sample mean is a pretty good estimate of <span class="math notranslate nohighlight">\(\mu\)</span>, as you know. In this example we will show that it is the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>What if you want to estimate <span class="math notranslate nohighlight">\(\sigma\)</span> as well? We will tackle that problem at the end of this section. For now, let’s just estimate <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>Likelihood Function: Density Case</strong></p>
<p>In the density case, the likelihood function is defined as the joint density of the sample evaluated at the observed values, considered as a function of the parameter. That’s a bit of a mouthful but it becomes clear once you see the calculation. The joint density in this example is the product of <span class="math notranslate nohighlight">\(n\)</span> normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> density functions, and hence the likelihood function is</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu) ~ = ~ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \big{(} -\frac{1}{2} \big{(} \frac{X_i - \mu}{\sigma} \big{)}^2 \big{)}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(Lik(\mu)\)</span> is called the likelihood of the data <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> when the mean of the underlying normal distribution is <span class="math notranslate nohighlight">\(\mu\)</span>. For every fixed <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(Lik(\mu)\)</span> is a function of the sample and hence is a random variable.</p>
<p>The goal is to find the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes this likelihood function over all the possible values that <span class="math notranslate nohighlight">\(\mu\)</span> could be. We don’t yet know if such a maximizing value exists, but let’s try to find it anyway.</p>
<p>To do this we will simplify the likelihood function as much as possible.</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu) ~ = ~ \big{(} \frac{1}{\sqrt{2\pi}\sigma} \big{)}^n
\exp \big{(} -\frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2 \big{)}
~ = ~ C \exp \big{(} -\frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2 \big{)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\mu\)</span> and thus won’t affect the maximization.</p>
<p>Even in this simplified form, the likelihood function looks difficult to maximize. But as it is a product, we can simplify our calculations still further by taking its log as we did in the binomial example.</p>
<p>The log-likelihood function is</p>
<div class="math notranslate nohighlight">
\[
L(\mu) ~ = ~ \log(C) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\log(C)\)</span> doesn’t affect the maximization, and nor does <span class="math notranslate nohighlight">\(\sigma\)</span>, we have defined a function to calculate <span class="math notranslate nohighlight">\(L - \log(C)\)</span> for the sample 52.8, 51.1, 54.2, and 52.5 drawn from the normal <span class="math notranslate nohighlight">\((\mu, 1)\)</span> distribution. Remember that we began this section by comparing 32 and 52 as estimates of <span class="math notranslate nohighlight">\(\mu\)</span>, based on this sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">(</span><span class="mf">52.8</span><span class="p">,</span> <span class="mf">51.1</span><span class="p">,</span> <span class="mf">54.2</span><span class="p">,</span> <span class="mf">52.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shifted_log_lik</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">((</span><span class="n">sample</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a graph of the function for <span class="math notranslate nohighlight">\(\mu\)</span> in the interval <span class="math notranslate nohighlight">\((30, 70)\)</span>.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/01_Maximum_Likelihood_12_0.png" src="../../_images/01_Maximum_Likelihood_12_0.png" />
</div>
</div>
<p>The maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> is somewhere around <span class="math notranslate nohighlight">\(52.5\)</span>. To find exactly where it is, we have to complete the maximizatin.</p>
<p>Find the derivative of <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(\mu\)</span>. Use the Chain Rule and be careful about negative signs.</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\mu} L(\mu) ~ = ~ \frac{2}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)
\]</div>
<p>Now set this equal to <span class="math notranslate nohighlight">\(0\)</span> and solve. Let <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> be the MLE of <span class="math notranslate nohighlight">\(\mu\)</span>. Then <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> satisfies the following equation.</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n (X_i - \hat{\mu}) ~ = ~ 0 ~~~~~~ \Longleftrightarrow ~~~~~~ \sum_{i=1}^n X_i ~ = ~ n\hat{\mu} ~~~~~~ \Longleftrightarrow ~~~~~~ \hat{\mu} ~ = ~ \frac{1}{n} \sum_{i=1}^n X_i ~ = ~ \bar{X}
\]</div>
<p>Once again we should check that this is a max and not a min, but at this point you will surely be convinced that it is a max.</p>
<p>We have shown that the MLE of <span class="math notranslate nohighlight">\(\mu\)</span> is the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span>, regardless of the population SD <span class="math notranslate nohighlight">\(\sigma\)</span>. In the case of the sample we used for the plot above, <span class="math notranslate nohighlight">\(\bar{X} = 52.65\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>52.650000000000006
</pre></div>
</div>
</div>
</div>
<p>You know that the distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is normal with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. If you don’t know <span class="math notranslate nohighlight">\(\sigma\)</span>, then if the sample is large you can estimate <span class="math notranslate nohighlight">\(\sigma\)</span> by the SD of the sample and hence construct confidence intervals for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</div>
<div class="section" id="steps-for-finding-the-mle">
<h2><span class="section-number">20.1.3. </span>Steps for Finding the MLE<a class="headerlink" href="#steps-for-finding-the-mle" title="Permalink to this headline">¶</a></h2>
<p>Let’s capture our sequence of steps in an algorithm to find the MLE of a parameter given an i.i.d. sample. See the <strong>Computational Notes</strong> at the end of this section for other ways of finding the MLE.</p>
<ul class="simple">
<li><p>Write the likelihood of the sample. The goal is to find the value of the parameter that maximizes this likelihood.</p></li>
<li><p>To make the maximization easier, take the log of the likelihood function.</p></li>
<li><p>To maximize the log likelihood with respect to the parameter, take its derivative with respect to the parameter.</p></li>
<li><p>Set the derivative equal to 0 and solve; the solution is the MLE.</p></li>
</ul>
</div>
<div class="section" id="properties-of-the-mle">
<h2><span class="section-number">20.1.4. </span>Properties of the MLE<a class="headerlink" href="#properties-of-the-mle" title="Permalink to this headline">¶</a></h2>
<p>In the two examples above, the MLE is unbiased and either exactly normal or asymptotically normal. In general, MLEs need not be unbiased, as you will see in an example below. However, under some regularity conditions on the underlying probability distribution or mass function, when the sample is large the MLE is:</p>
<ul class="simple">
<li><p><em>consistent</em>, that is, likely to be close to the parameter</p></li>
<li><p>roughly normal and almost unbiased</p></li>
</ul>
<p>Establishing this is outside the scope of this class, but in exercises you will observe these properties through simulation.</p>
<p>Though there is beautiful theory about the asymptotic variance of the MLE, in practice it can be hard to estimate the variance analytically. This can make it hard to find formulas for confidence intervals. However, you can use the bootstrap to estimate the variance: each bootstrapped sample yields a value of the MLE, and you can construct confidence intervals based on the empirical distribution of the bootstrapped MLEs.</p>
</div>
<div class="section" id="mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">
<h2><span class="section-number">20.1.5. </span>MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample<a class="headerlink" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample. We will now find the MLEs of both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p><strong>Likelihood Function</strong></p>
<p>We have to think of this as a function of both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu, \sigma) ~ = ~ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \big{(} -\frac{1}{2} \big{(} \frac{X_i - \mu}{\sigma} \big{)}^2 \big{)} ~ = ~
C \cdot \frac{1}{\sigma^n} \prod_{i=1}^n \exp \big{(} -\frac{1}{2\sigma^2} (X_i - \mu)^2 \big{)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C = 1/(\sqrt{2\pi})^n\)</span> does not affect the maximization.</p>
<p><strong>Log-Likelihood Function</strong></p>
<div class="math notranslate nohighlight">
\[
L(\mu, \sigma) ~ = ~ \log(C) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
\]</div>
<p><strong>Maximizing the Log Likelihood Function</strong></p>
<p>We will maximize <span class="math notranslate nohighlight">\(L\)</span> in two stages:</p>
<ul class="simple">
<li><p>First fix <span class="math notranslate nohighlight">\(\sigma\)</span> and maximize with respect to <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Then plug in the maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> and maximize the resulting function with respect to <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
<p>We have already completed the first stage in the first example of this section. For each fixed <span class="math notranslate nohighlight">\(\sigma\)</span>, the maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> is <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span>.</p>
<p>So now our job is to find the value <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> that maximizes the new function</p>
<div class="math notranslate nohighlight">
\[
L^*(\sigma) ~ = ~ -n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \bar{X})^2 ~ = ~ -n\log(\sigma) - \frac{1}{2\sigma^2} V
\]</div>
<p>where <span class="math notranslate nohighlight">\(V = \sum_{i=1}^n (X_i - \bar{X})^2\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\sigma\)</span>. Differentiate with respect to <span class="math notranslate nohighlight">\(\sigma\)</span>; keep track of minus signs and factors of 2.</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\sigma} L^*(\sigma) ~ = ~ -\frac{n}{\sigma} + \frac{1}{\sigma^3}V
\]</div>
<p>Set this equal to 0 and solve for the maximizing value <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
-\frac{n}{\hat{\sigma}} + \frac{1}{\hat{\sigma}^3}V ~ = ~ 0 
~~~~~~~ \Longleftrightarrow ~~~~~~~ \hat{\sigma}^2 ~ = ~ \frac{V}{n} ~ = ~ 
\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\]</div>
<p>Again you should check that this yields a maximum and not a minimum, but again given the answer you will surely accept that it’s a max.</p>
<p>You have shown in exercises that <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is <em>not</em> an unbiased estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>. You have also shown that it is close to unbiased when <span class="math notranslate nohighlight">\(n\)</span> is large.</p>
<p>To summarize our result, if <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> is an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample, then the MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma} = \sqrt{\hat{\sigma}^2}\)</span> where <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2\)</span></p></li>
</ul>
<p>It is a remarkable fact about i.i.d. normal samples that <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> are independent of each other even though they are statistics calculated from the same sample. Later in this course you will see why.</p>
<p><strong>Computational Notes</strong></p>
<ul class="simple">
<li><p>The goal is to find the value of the parameter that maximizes the likelihood. Sometimes, you can do that without any calculus, just by observing properties of the likelihood function. See the Exercises.</p></li>
<li><p>MLEs can’t always be derived analytically as easily as in our examples. It’s important to keep in mind that maximizing log likelihood functions can often be intractable without a numerical optimization method.</p></li>
<li><p>Not all likelihood functions have unique maxima.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Chapter_20"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00_Approaches_to_Estimation.html" title="previous page"><span class="section-number">20. </span>Approaches to Estimation</a>
    <a class='right-next' id="next-link" href="02_Independence_Revisited.html" title="next page"><span class="section-number">20.2. </span>Independence, Revisited</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ani Adhikari<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>