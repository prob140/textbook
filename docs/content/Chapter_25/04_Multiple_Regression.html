

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>25.4. Multiple Regression &#8212; Prob 140 Textbook</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/headers.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="25.3. Conditioning and the Multivariate Normal" href="03_Multivariate_Normal_Conditioning.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Prob 140 Textbook</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="http://prob140.org">
   Course Home
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../To_the_Student.html">
   To the  Student
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_01/00_Fundamentals.html">
   1. Fundamentals
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">
     1.1. Outcome Space and Events
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">
     1.2. Equally Likely Outcomes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">
     1.3. Collisions in Hashing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">
     1.4. The Birthday Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">
     1.5. An Exponential Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/06_Exercises.html">
     1.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">
   2. Calculating Chances
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/01_Addition.html">
     2.1. Addition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/02_Examples.html">
     2.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/03_Multiplication.html">
     2.3. Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/04_More_Examples.html">
     2.4. More Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">
     2.5. Updating Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/06_Exercises.html">
     2.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_03/00_Random_Variables.html">
   3. Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">
     3.1. Functions on an Outcome Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/02_Distributions.html">
     3.2. Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/03_Equality.html">
     3.3. Equality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/04_Exercises.html">
     3.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">
   4. Relations Between Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">
     4.1. Joint Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/02_Examples.html">
     4.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">
     4.3. Marginal Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">
     4.4. Conditional Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">
     4.5. Dependence and Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/06_Exercises.html">
     4.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">
   5. Collections of Events
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">
     5.1. Bounding the Chance of a Union
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">
     5.2. Inclusion-Exclusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">
     5.3. The Matching Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">
     5.4. Sampling Without Replacement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/05_Exercises.html">
     5.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_06/00_Random_Counts.html">
   6. Random Counts
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">
     6.1. The Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/02_Examples.html">
     6.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/03_Multinomial_Distribution.html">
     6.3. Multinomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/04_The_Hypergeometric_Revisited.html">
     6.4. The Hypergeometric, Revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/05_Odds_Ratios.html">
     6.5. Odds Ratios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/06_Law_of_Small_Numbers.html">
     6.6. The Law of Small Numbers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/07_Exercises.html">
     6.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_07/00_Poissonization.html">
   7. Poissonization
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/01_Poisson_Distribution.html">
     7.1. Poisson Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Binomial.html">
     7.2. Poissonizing the Binomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/03_Poissonizing_the_Multinomial.html">
     7.3. Poissonizing the Multinomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/04_Exercises.html">
     7.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_08/00_Expectation.html">
   8. Expectation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/01_Definition.html">
     8.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/02_Applying_the_Definition.html">
     8.2. Applying the Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/03_Expectations_of_Functions.html">
     8.3. Expectations of Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/04_Additivity.html">
     8.4. Additivity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/05_Method_of_Indicators.html">
     8.5. Method of Indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/06_Exercises.html">
     8.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">
   9. Conditioning, Revisited
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">
     9.1. Probability by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">
     9.2. Expectation by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">
     9.3. Expected Waiting Times
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/04_Exercises.html">
     9.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">
   10. Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/01_Transitions.html">
     10.1. Transitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">
     10.2. Deconstructing Chains
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">
     10.3. Long Run Behavior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/04_Examples.html">
     10.4. Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_11/00_Markov_Chain_Monte_Carlo.html">
   11. Markov Chain Monte Carlo
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/01_Balance_and_Detailed_Balance.html">
     11.1. Balance and Detailed Balance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/02_Code_Breaking.html">
     11.2. Code Breaking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/03_Metropolis_Algorithm.html">
     11.3. Metropolis Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/04_Exercises.html">
     11.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">
   12. Standard Deviation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/01_Definition.html">
     12.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">
     12.2. Prediction and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/03_Bounds.html">
     12.3. Tail Bounds
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">
     12.4. Heavy Tails
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/05_Exercises.html">
     12.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">
   13. Variance Via Covariance
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/01_Covariance.html">
     13.1. Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/02_Properties_of_Covariance.html">
     13.2. Properties of Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/03_Sums_of_Independent_Variables.html">
     13.3. Sums of Independent Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/04_Symmetry_and_Indicators.html">
     13.4. Symmetry and Indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/05_Finite_Population_Correction.html">
     13.5. Finite Population Correction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/06_Exercises.html">
     13.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">
   14. The Central Limit Theorem
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/01_Exact_Distribution_of_a_Sum.html">
     14.1. Exact Distribution of a Sum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">
     14.2. PGFs in NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">
     14.3. Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/04_SciPy_and_Normal_Curves.html">
     14.4. SciPy and Normal Curves
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/05_The_Sample_Mean.html">
     14.5. The Sample Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/06_Confidence_Intervals.html">
     14.6. Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/07_Exercises.html">
     14.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">
   15. Continuous Distributions
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">
     15.1. Density and CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">
     15.2. The Meaning of Density
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/03_Expectation.html">
     15.3. Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">
     15.4. Exponential Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">
     15.5. Calculus in SymPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/06_Exercises.html">
     15.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_16/00_Transformations.html">
   16. Transformations
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">
     16.1. Linear Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">
     16.2. Monotone Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/03_Simulation_via_the_CDF.html">
     16.3. Simulation via the CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/04_Two_to_One_Functions.html">
     16.4. Two-to-One Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/05_Exercises.html">
     16.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">
   17. Joint Densities
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">
     17.1. Probabilities and Expectations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/02_Independence.html">
     17.2. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">
     17.3. Marginal and Conditional Densities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">
     17.4. Beta Densities with Integer Parameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/05_Exercises.html">
     17.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">
   18. The Normal and Gamma Families
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">
     18.1. Standard Normal: The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">
     18.2. Sums of Independent Normal Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">
     18.3. The Gamma Family
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">
     18.4. Chi-Squared Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/05_Exercises.html">
     18.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">
   19. Distributions of Sums
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">
     19.1. The Convolution Formula
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">
     19.2. Moment Generating Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">
     19.3. MGFs, the Normal, and the CLT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">
     19.4. Chernoff Bound
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/05_Exercises.html">
     19.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_20/00_Approaches_to_Estimation.html">
   20. Approaches to Estimation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/01_Maximum_Likelihood.html">
     20.1. Maximum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/02_Independence_Revisited.html">
     20.2. Independence, Revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/03_Prior_and_Posterior.html">
     20.3. Prior and Posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/04_Exercises.html">
     20.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">
   21. The Beta and the Binomial
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">
     21.1. Updating and Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">
     21.2. The Beta-Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">
     21.3. Long Run Proportion of Heads
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/04_Exercises.html">
     21.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_22/00_Prediction.html">
   22. Prediction
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">
     22.1. Conditional Expectation As a Projection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/02_Least_Squares_Predictor.html">
     22.2. Least Squares Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/03_Variance_by_Conditioning.html">
     22.3. Variance by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/04_Examples.html">
     22.4. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/05_Exercises.html">
     22.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">
   23. Jointly Normal Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">
     23.1. Random Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Distribution.html">
     23.2. Multivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/03_Linear_Combinations.html">
     23.3. Linear Combinations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/04_Independence.html">
     23.4. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/05_Exercises.html">
     23.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">
   24. Simple Linear Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/01_Linear_Least_Squares.html">
     24.1. Least Squares Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/02_Bivariate_Normal_Distribution.html">
     24.2. Bivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">
     24.3. Regression and the Bivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">
     24.4. The Regression Equation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/05_Exercises.html">
     24.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="00_Multiple_Regression.html">
   25. Multiple Regression
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_Bilinearity_in_Matrix_Notation.html">
     25.1. Bilinearity in Matrix Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_Best_Linear_Predictor.html">
     25.2. Best Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_Multivariate_Normal_Conditioning.html">
     25.3. Conditioning and the Multivariate Normal
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     25.4. Multiple Regression
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Chapter_25/04_Multiple_Regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_25/04_Multiple_Regression.ipynb&branch=gh-pages"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> On this page
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-model">
   25.4.1. The Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#signal-and-noise-matrix-representation">
   25.4.2. Signal and Noise: Matrix Representation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ordinary-least-squares">
   25.4.3. Ordinary Least Squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#guessing-the-best-estimate-of-boldsymbol-beta">
   25.4.4. Guessing the Best Estimate of
   <span class="math notranslate nohighlight">
    \(\boldsymbol{\beta}\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projection">
   25.4.5. Projection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#least-squares">
   25.4.6. Least Squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#signal-and-noise-revisited">
   25.4.7. Signal and Noise, Revisited
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimate-of-sigma-2">
   25.4.8. Estimate of
   <span class="math notranslate nohighlight">
    \(\sigma^2\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#confidence-intervals">
   25.4.9. Confidence Intervals
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="multiple-regression">
<h1><span class="section-number">25.4. </span>Multiple Regression<a class="headerlink" href="#multiple-regression" title="Permalink to this headline">¶</a></h1>
<p>Regression provides one way of predicting a numerical variable, called a <em>response</em>, based on other variables called <em>predictor variables</em>. The multiple regression model says in essence that</p>
<div class="math notranslate nohighlight">
\[
\text{response} ~ = ~ \text{linear combination of predictor variables} + \text{ random noise }
\]</div>
<p>You can think of the first term on the right hand side as a <em>signal</em>. The problem is that we don’t get to observe the signal. The observed response is the sum of the signal and the noise. The data scientist’s task is to use the observations to extract the signal as accurately as possible.</p>
<p>It is worth looking more closely at exactly what is linear in linear regression, now that we are allowing more than one predictor variable. For example, notice that you can fit a quadratic function of <span class="math notranslate nohighlight">\(x\)</span> by using the two predictor variables <span class="math notranslate nohighlight">\(x_1 = x\)</span> and <span class="math notranslate nohighlight">\(x_2 = x^2\)</span>. Then the signal</p>
<div class="math notranslate nohighlight">
\[
\beta_0 + \beta_1x_1 + \beta_2x_2 ~ = ~ \beta_0 + \beta_1x + \beta_2x^2
\]</div>
<p>is a quadratic function of <span class="math notranslate nohighlight">\(x\)</span>. But it is linear in the coefficients, and it is a linear combination of the two predictor variables <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
<div class="section" id="the-model">
<h2><span class="section-number">25.4.1. </span>The Model<a class="headerlink" href="#the-model" title="Permalink to this headline">¶</a></h2>
<p>As in all of statistical inference, properties of estimates depend on the assumptions under which they are calculated. The <em>multiple regression model</em> is a commonly used set of assumptions that describes a particular kind of linear relation between a numerical response variable and a set of predictor variables. You should use it only if you believe that it makes sense for your data.</p>
<p>The model assumes that there are <span class="math notranslate nohighlight">\(n\)</span> individuals, on each of whom you have measured the response and the predictor variables. For <span class="math notranslate nohighlight">\(1 \le i \le n\)</span>, the relation between the variables is assumed to be</p>
<div class="math notranslate nohighlight">
\[
Y_i = \beta_0 + \beta_1x_{i,1} + \beta_2x_{i,2} + \cdots + \beta_{p-1}x_{i,p-1} + \epsilon_i
\]</div>
<p>in the notation described below.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_{i,1}, x_{i,2}, \ldots, x_{i,p-1}\)</span> are the observed constant values of <span class="math notranslate nohighlight">\(p-1\)</span> predictor variables for individual <span class="math notranslate nohighlight">\(i\)</span>. They are not random variables. If you prefer to think of the predictor variables as random, this model assumes that you have conditioned on them.</p></li>
<li><p>The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> and slopes <span class="math notranslate nohighlight">\(\beta_1, \beta_2, \ldots, \beta_{p-1}\)</span> are unobservable constants and are parameters of the model. There are <span class="math notranslate nohighlight">\(p\)</span> of them, hence the notation <span class="math notranslate nohighlight">\(p\)</span> for “parameters”.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i\)</span> is an unobservable random error that has the normal <span class="math notranslate nohighlight">\((0, \sigma^2)\)</span> distribution for some unobservable <span class="math notranslate nohighlight">\(\sigma^2\)</span>, and <span class="math notranslate nohighlight">\(\epsilon_1, \epsilon_2, \ldots, \epsilon_n\)</span> are i.i.d.</p></li>
<li><p><span class="math notranslate nohighlight">\(Y_i\)</span> is the observable response of individual <span class="math notranslate nohighlight">\(i\)</span>. It is random because <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is one of its components.</p></li>
</ul>
<p>We will assume that <span class="math notranslate nohighlight">\(n &gt; p\)</span>, that is, we will assume we have more individuals than parameters. Indeed in this course it is fine for you to think of <span class="math notranslate nohighlight">\(n\)</span> as much larger than <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Two special cases are already familiar.</p>
<p><strong><span class="math notranslate nohighlight">\(p = 1\)</span>: Prediction by a Constant</strong></p>
<p>When <span class="math notranslate nohighlight">\(p = 1\)</span> there is just one parameter: the intercept. There are no predictor variables at all. The model says that for each individual <span class="math notranslate nohighlight">\(i\)</span>, the response is <span class="math notranslate nohighlight">\(Y_i = \beta_0 + \epsilon_i\)</span>. This is a case of trying to estimate the response by a constant.</p>
<p><strong><span class="math notranslate nohighlight">\(p = 2\)</span>: Simple Linear Regression</strong></p>
<p>The two parameters are the intercept and a slope. The model says that for each individual <span class="math notranslate nohighlight">\(i\)</span>, the response is <span class="math notranslate nohighlight">\(Y_i = \beta_0 + \beta_1x_{i,1} + \epsilon_i\)</span>. That is, the response is the value on a hidden straight line, plus some normal noise. This is the simple regression model you used in Data 8.</p>
</div>
<div class="section" id="signal-and-noise-matrix-representation">
<h2><span class="section-number">25.4.2. </span>Signal and Noise: Matrix Representation<a class="headerlink" href="#signal-and-noise-matrix-representation" title="Permalink to this headline">¶</a></h2>
<p>For any <span class="math notranslate nohighlight">\(p\)</span>, the model can be written compactly as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y} ~ = ~ \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\]</div>
<p>in the matrix notation described below.</p>
<ul class="simple">
<li><p>The <em>design matrix</em> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n \times p\)</span> matrix of real numbers, not random variables. Column 0 of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a vector of 1’s and Column <span class="math notranslate nohighlight">\(j\)</span> for <span class="math notranslate nohighlight">\(1 \le j \le p-1\)</span> consists of the <span class="math notranslate nohighlight">\(n\)</span> observations on the <span class="math notranslate nohighlight">\(j\)</span>th predictor variable. For each <span class="math notranslate nohighlight">\(i\)</span> in the range 1 through <span class="math notranslate nohighlight">\(n\)</span>, Row <span class="math notranslate nohighlight">\(i\)</span> contains the values of all the predictor variables for individual <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>The <em>parameter vector</em> <span class="math notranslate nohighlight">\(\boldsymbol{\beta} = [\beta_0 ~~ \beta_1 ~~ \ldots ~~ \beta_{p-1}]^T\)</span> is a <span class="math notranslate nohighlight">\(p \times 1\)</span> vector of the coefficients.</p></li>
<li><p>The <em>error vector</em> <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is an <span class="math notranslate nohighlight">\(n \times 1\)</span> multivariate normal <span class="math notranslate nohighlight">\((\mathbf{0}, \sigma^2\mathbf{I}_n)\)</span> random vector. Its mean vector is an <span class="math notranslate nohighlight">\(n \times 1\)</span> vector of 0’s and <span class="math notranslate nohighlight">\(\mathbf{I}_n\)</span> is the <span class="math notranslate nohighlight">\(n \times n\)</span> identity matrix.</p></li>
<li><p>The <em>response vector</em> <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is a random vector that is the sum of the linear signal <span class="math notranslate nohighlight">\(\mathbf{X}\boldsymbol{\beta}\)</span> and the normal noise <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span>.</p></li>
</ul>
</div>
<div class="section" id="ordinary-least-squares">
<h2><span class="section-number">25.4.3. </span>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h2>
<p>Based on the observations of the predictor variables and the response, the goal is to find the best estimates of the intercept and slopes in the model.</p>
<p>These estimates can then be used to predict the response of a new individuals, assuming that the model holds for the new individual as well.</p>
<p>We must select a criterion by which we will decide whether one estimate is better than another. To develop one such criterion, start by noting that any linear function of the predictor variables can be written as <span class="math notranslate nohighlight">\(\mathbf{X}\boldsymbol{\gamma}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> is some <span class="math notranslate nohighlight">\(p \times 1\)</span> vector of coefficients. Think of <span class="math notranslate nohighlight">\(\mathbf{X}\boldsymbol{\gamma}\)</span> as an estimate of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>. Then the error in the estimate is <span class="math notranslate nohighlight">\(\mathbf{Y} - \mathbf{X}\boldsymbol{\gamma}\)</span>.</p>
<p>The goal of <em>ordinary least squares</em> (OLS) is to find the vector <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> that minimises the mean squared error</p>
<div class="math notranslate nohighlight">
\[
MSE(\boldsymbol{\gamma}) ~ = ~ \frac{1}{n} \sum_{i=1}^n (Y_i - (\mathbf{X}\boldsymbol{\gamma})_i)^2
\]</div>
<p>This is the same as the <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> that minimizes the sum of squared errors</p>
<div class="math notranslate nohighlight">
\[
SSE(\boldsymbol{\gamma}) ~ = ~ \sum_{i=1}^n (Y_i - (\mathbf{X}\boldsymbol{\gamma})_i)^2
\]</div>
<p>Again for compactness it will help to use matrix notation. For an <span class="math notranslate nohighlight">\(n \times 1\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n w_i^2 ~ = ~ \mathbf{w}^T\mathbf{w} ~ = ~ \mathbf{w} \cdot \mathbf{w} ~ = ~ \| \mathbf{w} \|^2
\]</div>
<p>which is sometimes called the <em>squared norm</em> of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p>
<p>In this notation, the goal of OLS is to find the <span class="math notranslate nohighlight">\(p \times 1\)</span> vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes <span class="math notranslate nohighlight">\(\| \mathbf{Y} - \mathbf{X}\boldsymbol{\gamma}\|^2\)</span> over all vectors <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span>.</p>
<p>Typically you will also have to estimate the unknown error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. But we will not cover that in this class except in the case <span class="math notranslate nohighlight">\(p = 1\)</span>.</p>
<div class="cell tag_remove-input tag_hide-output docutils container">
</div>
</div>
<div class="section" id="guessing-the-best-estimate-of-boldsymbol-beta">
<h2><span class="section-number">25.4.4. </span>Guessing the Best Estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span><a class="headerlink" href="#guessing-the-best-estimate-of-boldsymbol-beta" title="Permalink to this headline">¶</a></h2>
<p>Remember that we have assumed <span class="math notranslate nohighlight">\(n &gt; p\)</span>. Assume also that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is of full column rank <span class="math notranslate nohighlight">\(p\)</span>, that is, none of the predictor variables is a linear combination of the others. By a theorem in linear algebra, it follows that the <span class="math notranslate nohighlight">\(p \times p\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> has full rank <span class="math notranslate nohighlight">\(p\)</span> and is therefore invertible.</p>
<p>The claim is that OLS estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is the vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} ~ = ~ (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\]</div>
<p>The claim is motivated by our earlier formula</p>
<div class="math notranslate nohighlight">
\[
\mathbf{b} ~ = ~ \boldsymbol{\Sigma}_\mathbf{X}^{-1}\boldsymbol{\Sigma}_\mathbf{XY}
\]</div>
<p>for the coefficients of the least squares linear predictor a random variable <span class="math notranslate nohighlight">\(Y\)</span> based on a random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact the new formula is an application of the old one. But we will derive it afresh in our new setting.</p>
<p>The key idea is that of projection: the best <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> should be such that the error in the estimate is orthogonal to the linear space spanned by <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>The error in the best estimate is <span class="math notranslate nohighlight">\(\mathbf{Y} - \hat{\mathbf{Y}} = \mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}\)</span>. For this error to be orthogonal to linear transformations of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> we must have</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}^T (Y - \mathbf{X}\hat{\boldsymbol{\beta}}) = 0 ~ \implies \mathbf{X}^T Y ~ = ~ \mathbf{X}^T \mathbf{X}\hat{\boldsymbol{\beta}}
\]</div>
<p>We have assumed that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has full column rank, so <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> is invertible. So the natural guess for the best estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} ~ = ~ (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\]</div>
<p>Before we go further, notice that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a linear function of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>. This makes it straightforward to identify its distribution, which you will do in exercises.</p>
<p>Also note that the estimate of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{Y}} ~ = ~ \mathbf{X}\hat{\boldsymbol{\beta}} ~ = ~ \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\]</div>
<p>which is also a linear function of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>.</p>
</div>
<div class="section" id="projection">
<h2><span class="section-number">25.4.5. </span>Projection<a class="headerlink" href="#projection" title="Permalink to this headline">¶</a></h2>
<p>Define the <span class="math notranslate nohighlight">\(i\)</span>th <em>residual</em> as the prediction error <span class="math notranslate nohighlight">\(e_i = Y_i - \hat{Y}_i\)</span>. Then the <span class="math notranslate nohighlight">\(n \times 1\)</span> vector of residuals is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{e} ~ = ~ \mathbf{Y} - \hat{\mathbf{Y}} ~ = ~ \mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}
\]</div>
<p>As we have seen repeatedly, the key to least squares is that the prediction error is orthogonal to the space of allowed functions. Our space of allowed functions is all linear functions of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. So we will show:</p>
<p><strong>The residual vector is orthogonal to each column of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</strong></p>
<p>This is essentially true by construction. Formally, calculate the <span class="math notranslate nohighlight">\(p \times 1\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{e}\)</span>. Each of its elements is the dot product of <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> and one column of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. We will show that each of the elements is 0.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}^T\mathbf{e} ~ = ~ \mathbf{X}^T(\mathbf{Y} - \hat{\mathbf{Y}}) ~ = ~ \mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\hat{\mathbf{Y}} ~ = ~ \mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} ~ = ~ \mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\mathbf{Y} ~ = ~ 0
\]</div>
</div>
<div class="section" id="least-squares">
<h2><span class="section-number">25.4.6. </span>Least Squares<a class="headerlink" href="#least-squares" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> be any <span class="math notranslate nohighlight">\(p \times 1\)</span> vector. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
SSE(\boldsymbol{\gamma}) ~  &amp;= ~ \| \mathbf{Y} - \mathbf{X}\boldsymbol{\gamma} \|^2 \\
&amp;= ~ \| (\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) + (\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\gamma}) \|^2 \\
&amp;= ~ \|\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}\|^2 ~+~ \| \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\gamma} \|^2 + 2(\mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\gamma})^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
&amp;= ~ SSE(\hat{\boldsymbol{\beta}}) ~+~ \| \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\gamma} \|^2 + 2((\mathbf{X}(\hat{\boldsymbol{\beta}} - \boldsymbol{\gamma}))^T\mathbf{e} \\
&amp;= ~ SSE(\hat{\boldsymbol{\beta}}) ~+~ \| \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\gamma} \|^2 + 2(\hat{\boldsymbol{\beta}} - \boldsymbol{\gamma})^T\mathbf{X}^T \mathbf{e} \\
&amp;= ~ SSE(\hat{\boldsymbol{\beta}}) ~+~ \| \mathbf{X}\hat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\gamma} \|^2  ~~~~ \text{ by orthogonality} \\
&amp;\ge ~ SSE(\hat{\boldsymbol{\beta}})
\end{align*}
\end{split}\]</div>
<div class="cell tag_remove-input tag_hide-output docutils container">
</div>
</div>
<div class="section" id="signal-and-noise-revisited">
<h2><span class="section-number">25.4.7. </span>Signal and Noise, Revisited<a class="headerlink" href="#signal-and-noise-revisited" title="Permalink to this headline">¶</a></h2>
<p>Our regression model is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y} ~ = ~ \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\]</div>
<p>Here</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\boldsymbol{\beta}\)</span> is the unobservable but non-random true signal</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is an unobservable random vector consisting of the deviations of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> from the true plane <span class="math notranslate nohighlight">\(\mathbf{X}\boldsymbol{\beta}\)</span>. Elements of <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> are mutually independent.</p></li>
</ul>
<p>Once we have carried out the regression, our estimate of the response vector <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is the vector <span class="math notranslate nohighlight">\(\hat{\mathbf{Y}} = \mathbf{X}\boldsymbol{\hat{\boldsymbol{\beta}}}\)</span>.</p>
<p>The residual vector is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{e} ~ = ~ \mathbf{Y} - \hat{\mathbf{Y}} ~ = ~ \mathbf{Y} - \mathbf{X}\boldsymbol{\hat{\boldsymbol{\beta}}}
\]</div>
<p>Therefore we have another expression for the response vector <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>. This expression is our best attempt at separating the signal from the noise.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y} ~ = ~ \mathbf{X}\boldsymbol{\hat{\boldsymbol{\beta}}} + \mathbf{e}
\]</div>
<p>It is important to note the distinction between this identity and the model.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\boldsymbol{\hat{\boldsymbol{\beta}}}\)</span> is the <em>observable random estimated</em> signal.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{e}\)</span> is the <em>observable</em> random vector consisting of the deviations of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> from the estimated plane <span class="math notranslate nohighlight">\(\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>. Elements of <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> are <em>not independent</em> of each other, because they add up to <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
</ul>
<p>In exercises you will show that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and that both <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> of <span class="math notranslate nohighlight">\(\hat{\mathbf{Y}}\)</span> have normal (or multivariate normal) distributions. Both distributions have variance and covariance parameters that depend on the unknown error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
</div>
<div class="section" id="estimate-of-sigma-2">
<h2><span class="section-number">25.4.8. </span>Estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span><a class="headerlink" href="#estimate-of-sigma-2" title="Permalink to this headline">¶</a></h2>
<p>It should come as no surprise that under the multiple regression model, there is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span> that has a chi-squared distribution. There is some work involved in establishing that this estimate is</p>
<div class="math notranslate nohighlight">
\[
S^2 ~ = ~ \frac{1}{n-p} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 ~ = ~ \frac{1}{n-p} \| \mathbf{e} \|^2
\]</div>
<p>Some more work establishes that <span class="math notranslate nohighlight">\(\frac{n-p}{\sigma^2}S^2\)</span> has the chi-squared <span class="math notranslate nohighlight">\((n-p)\)</span> distribution.</p>
<p>We’ll leave that work for another course. For now, just notice that if the number of data points <span class="math notranslate nohighlight">\(n\)</span> is large compared to the number of parameters <span class="math notranslate nohighlight">\(p\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
S^2 ~ = ~ \frac{1}{n-p} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 ~ \approx ~ \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 ~ = ~ \hat{\sigma}^2
\]</div>
<p>which is the natural mean squared error. If you have a lot of data, you don’t have to worry about fine points like dividing by <span class="math notranslate nohighlight">\(n-p\)</span> instead of <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p><strong>Special Case</strong></p>
<p>As noted earlier, in the case <span class="math notranslate nohighlight">\(p = 1\)</span> you are trying to find the best constant by which to estimate <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>You know that the least squares constant is <span class="math notranslate nohighlight">\(\bar{Y}\)</span>, and you showed in exercises that</p>
<div class="math notranslate nohighlight">
\[
S^2 ~ = ~ \frac{1}{n-1} \sum_{i=1}^n (Y_i - \bar{Y})^2
\]</div>
<p>is an unbiased estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>. You have shown in exercises that <span class="math notranslate nohighlight">\(\frac{n-1}{\sigma^2}S^2\)</span> has the chi-squared <span class="math notranslate nohighlight">\(n-1\)</span> distribution under the assumption that the data are i.i.d. normal variables. This is the special case of the result stated above for general <span class="math notranslate nohighlight">\(p\)</span>.</p>
</div>
<div class="section" id="confidence-intervals">
<h2><span class="section-number">25.4.9. </span>Confidence Intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">¶</a></h2>
<p>The upshot of this discussion is that if <span class="math notranslate nohighlight">\(n\)</span> is large compared to <span class="math notranslate nohighlight">\(p\)</span> – that is, if you have a lot of observations compared to the number of predictors – then you can use ordinary normal theory to construct confidence intervals for the parameters <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>For example, a 95% confidence interval for the parameter <span class="math notranslate nohighlight">\(\beta_i\)</span> is <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} [i] \pm 2SD(\hat{\boldsymbol{\beta}} [i])\)</span>.</p>
<p>Here <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}[i]\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th element of the estimate vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>The variance of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}[i]\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th diagonal element of the covariance matrix of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. You will see in exercises that this involves the error variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Typically, <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown. But you can estimate it by the mean squared error <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> to get an approximate 95% confidence interval for <span class="math notranslate nohighlight">\(\beta_i\)</span>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Chapter_25"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="03_Multivariate_Normal_Conditioning.html" title="previous page"><span class="section-number">25.3. </span>Conditioning and the Multivariate Normal</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ani Adhikari<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>