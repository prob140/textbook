

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>25.5. Further Review Exercises &#8212; Prob 140 Textbook</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/headers.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="25.4. Multiple Regression" href="04_Multiple_Regression.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Prob 140 Textbook</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="http://prob140.org">
   Course Home
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../To_the_Student.html">
   To the  Student
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_01/00_Fundamentals.html">
   1. Fundamentals
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">
     1.1. Outcome Space and Events
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">
     1.2. Equally Likely Outcomes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">
     1.3. Collisions in Hashing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">
     1.4. The Birthday Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">
     1.5. An Exponential Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/06_Exercises.html">
     1.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">
   2. Calculating Chances
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/01_Addition.html">
     2.1. Addition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/02_Examples.html">
     2.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/03_Multiplication.html">
     2.3. Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/04_More_Examples.html">
     2.4. More Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">
     2.5. Updating Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/06_Exercises.html">
     2.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_03/00_Random_Variables.html">
   3. Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">
     3.1. Functions on an Outcome Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/02_Distributions.html">
     3.2. Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/03_Equality.html">
     3.3. Equality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/04_Exercises.html">
     3.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">
   4. Relations Between Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">
     4.1. Joint Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/02_Examples.html">
     4.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">
     4.3. Marginal Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">
     4.4. Conditional Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">
     4.5. Dependence and Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/06_Exercises.html">
     4.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">
   5. Collections of Events
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">
     5.1. Bounding the Chance of a Union
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">
     5.2. Inclusion-Exclusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">
     5.3. The Matching Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">
     5.4. Sampling Without Replacement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/05_Exercises.html">
     5.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_06/00_Random_Counts.html">
   6. Random Counts
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">
     6.1. The Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/02_Examples.html">
     6.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/03_Hypergeometric_Distribution.html">
     6.3. The Hypergeometric Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/04_Odds_Ratios.html">
     6.4. Odds Ratios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/05_Law_of_Small_Numbers.html">
     6.5. The Law of Small Numbers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_07/00_Poissonization.html">
   7. Poissonization
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/01_Poissonizing_the_Binomial.html">
     7.3.1. Poissonizing the Binomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Multinomial.html">
     7.3.2. Poissonizing the Multinomial
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_08/00_Expectation.html">
   8. Expectation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/01_Definition.html">
     8.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/02_Additivity.html">
     8.2. Additivity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/03_Expectations_of_Functions.html">
     8.3. Expectations of Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/04_Review_Problems_Set_2.html">
     8.4. Review Problems: Set 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">
   9. Conditioning, Revisited
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">
     9.1. Probability by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">
     9.2. Expectation by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">
     9.3. Expected Waiting Times
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">
   10. Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/01_Transitions.html">
     10.1. Transitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">
     10.2. Deconstructing Chains
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">
     10.3. Long Run Behavior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/04_Examples.html">
     10.4. Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_11/00_Reversing_Markov_Chains.html">
   11. Reversing Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/01_Detailed_Balance.html">
     11.1. Detailed Balance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/02_Reversibility.html">
     11.2. Reversibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/03_Code_Breaking.html">
     11.3. Code Breaking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/04_Markov_Chain_Monte_Carlo.html">
     11.4. Markov Chain Monte Carlo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/05_Review_Conditioning_and_MC.html">
     11.5. Review Set on Conditioning and Markov Chains
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">
   12. Standard Deviation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/01_Definition.html">
     12.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">
     12.2. Prediction and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/03_Bounds.html">
     12.3. Tail Bounds
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">
     12.4. Heavy Tails
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">
   13. Variance Via Covariance
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/01_Properties_of_Covariance.html">
     13.1. Properties of Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/02_Sums_of_IID_Samples.html">
     13.2. Sums of IID Samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/03_Sums_of_Simple_Random_Samples.html">
     13.3. Sums of Simple Random Samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/04_Finite_Population_Correction.html">
     13.4. Finite Population Correction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">
   14. The Central Limit Theorem
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/01_Exact_Distribution.html">
     14.1. Exact Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">
     14.2. PGFs in NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">
     14.3. Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/04_The_Sample_Mean.html">
     14.4. The Sample Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/05_Confidence_Intervals.html">
     14.5. Confidence Intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">
   15. Continuous Distributions
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">
     15.1. Density and CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">
     15.2. The Meaning of Density
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/03_Expectation.html">
     15.3. Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">
     15.4. Exponential Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">
     15.5. Calculus in SymPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/06_Review_Problems_Set_3.html">
     15.6. Review Problems: Set 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_16/00_Transformations.html">
   16. Transformations
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">
     16.1. Linear Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">
     16.2. Monotone Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/03_Two_to_One_Functions.html">
     16.3. Two-to-One Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">
   17. Joint Densities
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">
     17.1. Probabilities and Expectations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/02_Independence.html">
     17.2. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">
     17.3. Marginal and Conditional Densities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">
     17.4. Beta Densities with Integer Parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">
   18. The Normal and Gamma Families
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">
     18.1. Standard Normal: The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">
     18.2. Sums of Independent Normal Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">
     18.3. The Gamma Family
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">
     18.4. Chi-Squared Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/05_Review_Problems_Set_4.html">
     18.5. Review Problems: Set 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">
   19. Distributions of Sums
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">
     19.1. The Convolution Formula
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">
     19.2. Moment Generating Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">
     19.3. MGFs, the Normal, and the CLT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">
     19.4. Chernoff Bound
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_20/00_Approaches_to_Estimation.html">
   20. Approaches to Estimation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/01_Maximum_Likelihood.html">
     20.1. Maximum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/02_Prior_and_Posterior.html">
     20.2. Prior and Posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/03_Independence_Revisited.html">
     20.3. Independence, Revisited
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">
   21. The Beta and the Binomial
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">
     21.1. Updating and Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">
     21.2. The Beta-Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">
     21.3. Long Run Proportion of Heads
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_22/00_Prediction.html">
   22. Prediction
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">
     22.1. Conditional Expectation As a Projection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/02_Variance_by_Conditioning.html">
     22.2. Variance by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/03_Examples.html">
     22.3. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/04_Least_Squares_Predictor.html">
     22.4. Least Squares Predictor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">
   23. Jointly Normal Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">
     23.1. Random Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Distribution.html">
     23.2. Multivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/03_Linear_Combinations.html">
     23.3. Linear Combinations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/04_Independence.html">
     23.4. Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">
   24. Simple Linear Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/01_Bivariate_Normal_Distribution.html">
     24.1. Bivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/02_Linear_Least_Squares.html">
     24.2. Least Squares Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">
     24.3. Regression and the Bivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">
     24.4. The Regression Equation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="00_Multiple_Regression.html">
   25. Multiple Regression
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_Bilinearity_in_Matrix_Notation.html">
     25.1. Bilinearity in Matrix Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_Best_Linear_Predictor.html">
     25.2. Best Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_Multivariate_Normal_Conditioning.html">
     25.3. Conditioning and the Multivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_Multiple_Regression.html">
     25.4. Multiple Regression
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     25.5. Further Review Exercises
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Chapter_25/05_Further_Review_Exercises.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_25/05_Further_Review_Exercises.ipynb&branch=gh-pages"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> On this page
        </div>
        <nav id="bd-toc-nav">
            
        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="further-review-exercises">
<h1><span class="section-number">25.5. </span>Further Review Exercises<a class="headerlink" href="#further-review-exercises" title="Permalink to this headline">¶</a></h1>
<p>Many of these exercises require the material of Chapter 19 onwards, which of course relies on the material of the previous chapters. However, some of them can be solved using earlier material alone.</p>
<p>According to students and alumni, some of the exercises have appeared as questions in “quant” interviews.</p>
<p><strong>1.</strong>
A coin lands heads with probability <span class="math notranslate nohighlight">\(p\)</span>. Let <span class="math notranslate nohighlight">\(X\)</span> be the number of tosses till the first head appears and let <span class="math notranslate nohighlight">\(Y\)</span> be the number of tails before the first head.</p>
<p>(a) Find the moment generating function of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>(b) Use the answer to (a) to find <span class="math notranslate nohighlight">\(E(X)\)</span>. Note that by now you have found <span class="math notranslate nohighlight">\(E(X)\)</span> in several ways: by the tail sum formula, by conditioning on the first toss, by the pgf, and now by the mgf.</p>
<p>(c) Use the answer to (a) to find the moment generating function of <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p><strong>2.</strong>
Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be i.i.d. Poisson <span class="math notranslate nohighlight">\((\mu)\)</span> random variables. Find the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>3.</strong>
Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be i.i.d. uniform on <span class="math notranslate nohighlight">\((0, \theta)\)</span>.</p>
<p>(a) Find the MLE of <span class="math notranslate nohighlight">\(\theta\)</span>. [Don’t leap into a calculation. Sketch a graph of the function you are trying to maximize, and be careful about its domain.]</p>
<p>(b) Is the MLE unbiased? If not, use the MLE to construct an unbiased estimate of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><strong>4.</strong> <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are i.i.d. with moment generating function <span class="math notranslate nohighlight">\(M(t) = e^{t + t^2}\)</span>, <span class="math notranslate nohighlight">\(-\infty &lt; t &lt; \infty\)</span>. What is the distribution of <span class="math notranslate nohighlight">\((X-Y)^2\)</span>?</p>
<p><strong>5.</strong>
<em>Capture-recapture</em> methods are sometimes used to estimate population sizes. A standard image is that a pond contains <span class="math notranslate nohighlight">\(N\)</span> fish for some fixed but unknown <span class="math notranslate nohighlight">\(N\)</span>, and that <span class="math notranslate nohighlight">\(G\)</span> of the <span class="math notranslate nohighlight">\(N\)</span> fish have been captured, tagged, and returned alive to the pond. You can assume that <span class="math notranslate nohighlight">\(G/N\)</span> isn’t close to 0.</p>
<p>In the recapture phase, assume that a simple random sample of <span class="math notranslate nohighlight">\(n\)</span> fish is drawn from the <span class="math notranslate nohighlight">\(N\)</span> fish in the pond (you might have to use some imagination to believe this assumption). We can observe <span class="math notranslate nohighlight">\(X\)</span>, the random number of tagged fish in the sample.</p>
<p>The goal is to use the observation to estimate <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>(a) For large <span class="math notranslate nohighlight">\(n\)</span>, the sample proportion <span class="math notranslate nohighlight">\(X/n\)</span> is likely to be close to a constant. Identify the constant and hence construct an estimate of <span class="math notranslate nohighlight">\(N\)</span> based on <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Later in this exercise you will see how your estimate is related to the MLE of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>(b) For <span class="math notranslate nohighlight">\(N \ge n\)</span>, find the likelihood <span class="math notranslate nohighlight">\(lik(N)\)</span>. You can assume <span class="math notranslate nohighlight">\(n &gt; G\)</span>.</p>
<p>(c) Find the likelihood ratio <span class="math notranslate nohighlight">\(R(N) = \frac{lik(N)}{lik(N-1)}\)</span> for <span class="math notranslate nohighlight">\(N &gt; n\)</span>. Simplify the answer as much as you can.</p>
<p>(d) Find the maximum likelihood estimate of <span class="math notranslate nohighlight">\(N\)</span> by comparing the likelihood ratios and 1. How does the MLE compare with your estimate in (a)?</p>
<p><strong>6.</strong>
Show that if <span class="math notranslate nohighlight">\(r &gt; 1\)</span> and <span class="math notranslate nohighlight">\(s &gt; 1\)</span> then the mode of the beta <span class="math notranslate nohighlight">\((r, s)\)</span> distribution is <span class="math notranslate nohighlight">\((r-1)/(r+s-2)\)</span>. Remember to ignore multiplicative constants and take the log before maximizing.</p>
<p><strong>7.</strong>
Suppose that <span class="math notranslate nohighlight">\(X\)</span> has the beta <span class="math notranslate nohighlight">\((r, s)\)</span> distribution, and that given <span class="math notranslate nohighlight">\(X=p\)</span>, the conditional distribution of <span class="math notranslate nohighlight">\(H\)</span> is binomial <span class="math notranslate nohighlight">\((10, p)\)</span>. Find</p>
<p>(a) the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(H = 7\)</span></p>
<p>(b) <span class="math notranslate nohighlight">\(E(X \mid H = 7)\)</span></p>
<p>(c) the MAP estimate of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(H = 7\)</span></p>
<p>(d) <span class="math notranslate nohighlight">\(P(H = 7)\)</span></p>
<p>(e) <span class="math notranslate nohighlight">\(E(H)\)</span></p>
<p><strong>8.</strong>
The chance of heads of a random coin is picked according to the beta <span class="math notranslate nohighlight">\((r, s)\)</span> distribution. The coin is tossed repeatedly.</p>
<p>(a) What is the chance that the first three tosses are heads and the next three tosses are tails?</p>
<p>(b) Given that the first three tosses are heads and the next three tosses are tails, what is the chance that the seventh toss is a head?</p>
<p>(c) Given that three out of the first six tosses are heads, what is the chance that the seventh toss is a head? Compare with the answer to (b).</p>
<p><strong>9.</strong>
Person A creates a coin by picking its chance of heads uniformly on <span class="math notranslate nohighlight">\((0, 1)\)</span>. In three tosses of that coin, Person A gets two heads.</p>
<p>Independently of Person A, Person B creates a coin by picking its chance of heads uniformly on <span class="math notranslate nohighlight">\((0, 1)\)</span>. In three tosses of that coin, Person B gets one head.</p>
<p>(a) Given the data, what is the distribution of the chance of heads of Person A’s coin?</p>
<p>(b) Given the data, what is the distribution of the chance of heads of Person B’s coin?</p>
<p>(c) Given the data, what is the probability that Person A’s coin has a higher chance of heads than Person B’s coin?</p>
<p><strong>10: Markov and Chebyshev Bounds on the Poisson-Binomial Upper Tail.</strong>
For <span class="math notranslate nohighlight">\(j \ge 1\)</span> let <span class="math notranslate nohighlight">\(I_j\)</span> be independent indicators such that <span class="math notranslate nohighlight">\(P(I_j = 1) = p_j\)</span>. Let <span class="math notranslate nohighlight">\(X = I_1 + I_2 + \ldots + I_n\)</span>. Then <span class="math notranslate nohighlight">\(X\)</span> is the number of successes in <span class="math notranslate nohighlight">\(n\)</span> independent trials that are not necessarily identically distributed.</p>
<p>We say that <span class="math notranslate nohighlight">\(X\)</span> has the Poisson-binomial distribution with parameters <span class="math notranslate nohighlight">\(p_1, p_2, \ldots, p_n\)</span>. The binomial is the special case when all the <span class="math notranslate nohighlight">\(p_j\)</span>’s are equal.</p>
<p>You saw in lab that the number of occupied tables in a Chinese Restaurant process had a Poisson-Binomial distribution. These distributions arise in statistical learning theory, the theory of randomized algorithms, and other areas.</p>
<p>Let <span class="math notranslate nohighlight">\(E(X) = \mu\)</span>. For <span class="math notranslate nohighlight">\(c &gt; 0\)</span>, you are going to find an upper bound on <span class="math notranslate nohighlight">\(P(X \ge (1+c)\mu)\)</span>. That’s the chance that <span class="math notranslate nohighlight">\(X\)</span> exceeds its mean by some percent.</p>
<p>In the special case of the binomial, <span class="math notranslate nohighlight">\(\mu = np\)</span> and so <span class="math notranslate nohighlight">\(P(X \ge (1+c)\mu)\)</span> can be rewritten as <span class="math notranslate nohighlight">\(P(\frac{X}{n} - p \ge cp)\)</span>. That’s the chance that the sample proportion exceeds <span class="math notranslate nohighlight">\(p\)</span> by some percent.</p>
<p>(a) Find <span class="math notranslate nohighlight">\(\mu = E(X)\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 = Var(X)\)</span> in terms of <span class="math notranslate nohighlight">\(p_1, p_2, \ldots, p_n\)</span>.</p>
<p>(b) Find Markov’s bound on <span class="math notranslate nohighlight">\(P(X \ge (1+c)\mu)\)</span>.</p>
<p>(c) Find Chebyshev’s bound on <span class="math notranslate nohighlight">\(P(X \ge (1+c)\mu)\)</span> in terms of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>(d) If all the <span class="math notranslate nohighlight">\(p_j\)</span>’s are equal to <span class="math notranslate nohighlight">\(p\)</span>, what is the value of the bound in (c)?</p>
<p><strong>11: Chernoff Bound on Poisson-Binomial Upper Tail.</strong>
This exercise continues the previous one and uses the same notation.</p>
<p>(a) Show that the mgf of <span class="math notranslate nohighlight">\(I_j\)</span> is given by <span class="math notranslate nohighlight">\(M_{I_j}(t) = 1 + p_j(e^t - 1)\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>(b) Use (a) to derive an expression for <span class="math notranslate nohighlight">\(M_X(t)\)</span>, the mgf of <span class="math notranslate nohighlight">\(X\)</span> evaluated at <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>(c) An useful exponential bound is that <span class="math notranslate nohighlight">\(e^x \ge 1 + x\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>. You don’t have to show it but please look at the <a class="reference external" href="http://prob140.org/resources/exponential_approximations/">graphs</a>. Use the fact to show that <span class="math notranslate nohighlight">\(M_X(t) \le \exp\big{(}\mu(e^t -1)\big{)}\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>. Notice that the right hand side is the mgf of a Poisson random variable that has the same mean as <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>(d) Use Chernoff’s method and the bound in (c) to show that</p>
<div class="math notranslate nohighlight">
\[
P\big{(}X \ge (1+c)\mu\big{)} 
~ \le ~ 
\Big{(} \frac{\exp(c)}{ (1+c)^{1+c}} \Big{)}^\mu
\]</div>
<p>Remember that <span class="math notranslate nohighlight">\(\mu = np\)</span> when all the <span class="math notranslate nohighlight">\(p_j\)</span>’s are equal. If <span class="math notranslate nohighlight">\(g(c) = \exp(c)/(1+c)^{1+c}\)</span> is small then the bound above will decrease exponentially as <span class="math notranslate nohighlight">\(n\)</span> gets large. That is the focus of the next exercise.</p>
<p><strong>12: Simplified Chernoff Bounds on Poisson-Binomial Upper Tail.</strong> This exercise continues the previous one and uses the same notation.</p>
<p>The bound in the previous exercise is a bit complicated. Often, simpler versions are used because they are good enough even though they are weaker.</p>
<p>(a) It is not hard to show that <span class="math notranslate nohighlight">\(\log(1+c) \ge \frac{2c}{2+c}\)</span> for <span class="math notranslate nohighlight">\(c &gt; 0\)</span>. You don’t have to show it but please look at the <a class="reference external" href="http://prob140.org/resources/exponential_approximations/">graphs</a>.
Use the fact to show that <span class="math notranslate nohighlight">\(c - (1+c)\log(1+c) \le -\frac{c^2}{2+c}\)</span> for <span class="math notranslate nohighlight">\(c &gt; 0\)</span>.</p>
<p>(b) Show that if <span class="math notranslate nohighlight">\(X\)</span> has a Poisson-binomial distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> then</p>
<div class="math notranslate nohighlight" id="equation-c">
<span class="eqno">(25.1)<a class="headerlink" href="#equation-c" title="Permalink to this equation">¶</a></span>\[
P\big{(} X \ge (1+c)\mu\big{)} ~ \le ~ \exp\big{(}-\frac{c^2}{2+c}\mu\big{)}, ~~~~ c &gt; 0
\]</div>
<div class="math notranslate nohighlight">
\[
P\big{(} X \ge (1+c)\mu\big{)} ~ \le ~ \exp\big{(}-\frac{c^2}{3}\mu\big{)}, ~~~~ c \in (0, 1)
\]</div>
<p><strong>13.</strong>
A positive random variable <span class="math notranslate nohighlight">\(V\)</span> has expectation <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>(a) For each <span class="math notranslate nohighlight">\(v &gt; 0\)</span>, the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(V=v\)</span> is Poisson <span class="math notranslate nohighlight">\((v)\)</span>. Find <span class="math notranslate nohighlight">\(E(X)\)</span> and <span class="math notranslate nohighlight">\(Var(X)\)</span> in terms of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>(b) For each <span class="math notranslate nohighlight">\(v &gt; 0\)</span>, the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(V=v\)</span> is gamma <span class="math notranslate nohighlight">\((v, \lambda)\)</span> for some fixed <span class="math notranslate nohighlight">\(\lambda\)</span>. Find <span class="math notranslate nohighlight">\(E(X)\)</span> and <span class="math notranslate nohighlight">\(Var(X)\)</span> in terms of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p><strong>14.</strong>
Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be i.i.d. with expectation <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Let <span class="math notranslate nohighlight">\(S = \sum_{i=1}^n X_i\)</span>.</p>
<p>(a) Find the least squares predictor of <span class="math notranslate nohighlight">\(S\)</span> based on <span class="math notranslate nohighlight">\(X_1\)</span>, and find the mean squared error (MSE) of the predictor.</p>
<p>(b) Find the least squares predictor of <span class="math notranslate nohighlight">\(X_1\)</span> based on <span class="math notranslate nohighlight">\(S\)</span>, and find the MSE of the predictor. Is the predictor a linear function of <span class="math notranslate nohighlight">\(S\)</span>? If so, it must also be the best among all linear predictors based on <span class="math notranslate nohighlight">\(S\)</span>, which is commonly known as the regression predictor.</p>
<p><strong>15.</strong>
A <span class="math notranslate nohighlight">\(p\)</span>-coin is tossed repeatedly. Let <span class="math notranslate nohighlight">\(W_{H}\)</span> be the number of tosses till the first head appears, and <span class="math notranslate nohighlight">\(W_{HH}\)</span> the number of tosses till two consecutive heads appear.</p>
<p>(a) Describe a random variable <span class="math notranslate nohighlight">\(X\)</span> that  depends only on the tosses after <span class="math notranslate nohighlight">\(W_H\)</span> and satisfies <span class="math notranslate nohighlight">\(W_{HH} = W_H + X\)</span>.</p>
<p>(b) Use Part (a) to find <span class="math notranslate nohighlight">\(E(W_{HH})\)</span> and <span class="math notranslate nohighlight">\(Var(W_{HH})\)</span>.</p>
<p><strong>16.</strong>
Let <span class="math notranslate nohighlight">\(N\)</span> be a non-negative integer valued random variable,
and let $X, X_1, X_2, \ldots $ be i.i.d. and independent of <span class="math notranslate nohighlight">\(N\)</span>. As before, define
the <em>random sum</em> <span class="math notranslate nohighlight">\(S\)</span> by</p>
<div class="math notranslate nohighlight" id="equation-a">
<span class="eqno">(25.2)<a class="headerlink" href="#equation-a" title="Permalink to this equation">¶</a></span>\[\begin{split}
\begin{align*}
S ~~&amp;=~~0~~ \mbox{if}~N=0 \\
&amp;=~~ X_1 + X_2 + \cdots + X_n ~~ \mbox{if}~N = n &gt; 0 
\end{align*}
\end{split}\]</div>
<p>By conditioning on <span class="math notranslate nohighlight">\(N\)</span>, show that</p>
<div class="math notranslate nohighlight">
\[
M_S(t) ~~=~~ M_N\big{(}\log M_X(t) \big{)}
\]</div>
<p>assuming that all the quantities above are well defined.
[The identity <span class="math notranslate nohighlight">\((e^a)^n = e^{an}\)</span> might be handy.]</p>
<p>(b) Let <span class="math notranslate nohighlight">\(N\)</span> have the geometric <span class="math notranslate nohighlight">\((p)\)</span> distribution on <span class="math notranslate nohighlight">\(\{1, 2, 3, \ldots \}\)</span>. Find the mgf of <span class="math notranslate nohighlight">\(N\)</span>. This doesn’t use Part (a).</p>
<p>(c) Let $X_1, X_2, \ldots $ be i.i.d. exponential <span class="math notranslate nohighlight">\((\lambda)\)</span> variables and let <span class="math notranslate nohighlight">\(N\)</span> be geometric as in Part (b). Use the results of Parts (a) and (b) to identify the distribution of <span class="math notranslate nohighlight">\(S\)</span>.</p>
<p><strong>17.</strong>
A random vector <span class="math notranslate nohighlight">\(\mathbf{Y} = [Y_1 ~~ Y_2 ~~ \cdots ~~ Y_n]^T\)</span> has mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\sigma^2 \mathbf{I}_n\)</span> where <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span> is a number and <span class="math notranslate nohighlight">\(\mathbf{I}_n\)</span> is the <span class="math notranslate nohighlight">\(n \times n\)</span> identity matrix.</p>
<p>(a) Pick one option and explain: <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> are</p>
<p><span class="math notranslate nohighlight">\(~~~~~ (i) ~ \text{independent.} ~~~~~~~~ (ii) ~ \text{uncorrelated but might not be independent.} ~~~~~~~~ (iii) ~ \text{not uncorrelated.}\)</span></p>
<p>(b) Pick one option and explain: <span class="math notranslate nohighlight">\(Var(Y_1)\)</span> and <span class="math notranslate nohighlight">\(Var(Y_2)\)</span> are</p>
<p>$~~~~~ (i) ~ \text{equal.} ~~~~~~~~ (ii) ~ \text{possibly equal, but might not be.} ~~~~~~~~ (iii) ~ \text{not equal.} $</p>
<p>(c) For <span class="math notranslate nohighlight">\(m \le n\)</span> let <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> be an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of real numbers, and let <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> be an <span class="math notranslate nohighlight">\(m \times 1\)</span> vector of real numbers. Let <span class="math notranslate nohighlight">\(\mathbf{V} = \mathbf{AY} + \mathbf{b}\)</span>. Find the mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\_\mathbf{V}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\_\mathbf{V}\)</span> of <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>.</p>
<p>(d) Let <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> be an <span class="math notranslate nohighlight">\(m \times 1\)</span> vector of real numbers and let <span class="math notranslate nohighlight">\(W = \mathbf{c}^T\mathbf{V}\)</span> for <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> defined in Part (c). In terms of <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\_\mathbf{V}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\_\mathbf{V}\)</span>, find <span class="math notranslate nohighlight">\(E(W)\)</span> and <span class="math notranslate nohighlight">\(Var(W)\)</span>.</p>
<p><strong>18.</strong>
Let <span class="math notranslate nohighlight">\([X_1 ~~ X_2 ~~ X_3]^T\)</span> be multivariate normal with mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\mu} ~ = ~
\begin{bmatrix}
\mu \\
\mu \\
\mu
\end{bmatrix}
~~~~~~~~~~~
\boldsymbol{\Sigma} ~ = ~ 
\begin{bmatrix}
\sigma_1^2 &amp; \sigma_{12} &amp; \sigma_{13} \\
\sigma_{21} &amp; \sigma_2^2 &amp; \sigma_{23} \\
\sigma_{31} &amp; \sigma_{32} &amp; \sigma_3^2
\end{bmatrix}
\end{split}\]</div>
<p>Find <span class="math notranslate nohighlight">\(P\big{(} (X_1 + X_2)/2 &lt; X_3 + 1 \big{)}\)</span>.</p>
<p><strong>19.</strong>
Let <span class="math notranslate nohighlight">\(X\)</span> be standard normal. Construct a random variable <span class="math notranslate nohighlight">\(Y\)</span> as follows:</p>
<ul class="simple">
<li><p>Toss a fair coin.</p></li>
<li><p>If the coin lands heads, let <span class="math notranslate nohighlight">\(Y = X\)</span>.</p></li>
<li><p>If the coin lands tails, let <span class="math notranslate nohighlight">\(Y = -X\)</span>.</p></li>
</ul>
<p>(a) Find the cdf of <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>(b) Find <span class="math notranslate nohighlight">\(E(XY)\)</span> by conditioning on the result of the toss.</p>
<p>(c) Are <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> uncorrelated?</p>
<p>(d) Are <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> independent?</p>
<p>(e) Is the joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> bivariate normal?</p>
<p><strong>20.</strong>
Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be standard bivariate normal with correlation <span class="math notranslate nohighlight">\(\rho\)</span>. Find <span class="math notranslate nohighlight">\(E(\max(X, Y))\)</span>. The easiest way is to use the fact that for any two numbers <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(\max(a, b) = (a + b + \vert a - b \vert)/2\)</span>. Check the fact first, and then use it.</p>
<p><strong>21.</strong>
Suppose that <span class="math notranslate nohighlight">\(X\)</span> is normal <span class="math notranslate nohighlight">\((\mu_X, \sigma_X^2\)</span>), <span class="math notranslate nohighlight">\(Y\)</span> is normal <span class="math notranslate nohighlight">\((\mu_Y, \sigma_Y^2)\)</span>, and the two random variables are independent. Let <span class="math notranslate nohighlight">\(S = X+Y\)</span>.</p>
<p>(a) Find the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(S=s\)</span>.</p>
<p>(b) Find the least squares predictor of <span class="math notranslate nohighlight">\(X\)</span> based on <span class="math notranslate nohighlight">\(S\)</span> and provide its mean squared error.</p>
<p>(c) Find the least squares linear predictor of <span class="math notranslate nohighlight">\(X\)</span> based on <span class="math notranslate nohighlight">\(S\)</span> and provide its mean squared error.</p>
<p><strong>22.</strong>
Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a <span class="math notranslate nohighlight">\(p \times 1\)</span> random vector and suppose we are trying to predict a random variable <span class="math notranslate nohighlight">\(Y\)</span> by a linear function of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. A <a class="reference external" href="http://prob140.org/textbook/chapters/Chapter_25/02_Best_Linear_Predictor">section</a> of the textbook identifies the least squares linear predictor by restricting the search to linear functions <span class="math notranslate nohighlight">\(h(\mathbf{X})\)</span> for which <span class="math notranslate nohighlight">\(E(h(\mathbf{X})) = \mu_Y\)</span>. Show that this is a legitimate move.</p>
<p>Specifically, let <span class="math notranslate nohighlight">\(\hat{Y}_1 = \mathbf{c}^T \mathbf{X} + d\)</span> be a linear predictor such that <span class="math notranslate nohighlight">\(E(\hat{Y}_1) \ne \mu_Y\)</span>. Find a non-zero constant <span class="math notranslate nohighlight">\(k\)</span> such that <span class="math notranslate nohighlight">\(\hat{Y}_2 = \hat{Y}_1 + k\)</span> satisfies <span class="math notranslate nohighlight">\(E(\hat{Y}_2) = \mu_Y\)</span>. Then show that <span class="math notranslate nohighlight">\(MSE(\hat{Y}_1) \ge MSE(\hat{Y}_2)\)</span>. This will show that the least squares linear predictor has to have the same mean as <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p><strong>23.</strong>
Let $U_1, U_2, U_3, \ldots $ be i.i.d. uniform on <span class="math notranslate nohighlight">\((0, 1)\)</span>. For <span class="math notranslate nohighlight">\(n \ge 1\)</span>, let <span class="math notranslate nohighlight">\(S_n = \sum_{i=1}^n U_i\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(f_{S_n}\)</span> be the density of <span class="math notranslate nohighlight">\(S_n\)</span>. The formula for <span class="math notranslate nohighlight">\(f_{S_n}\)</span> is piecewise polynomial on the possible values <span class="math notranslate nohighlight">\((0, n)\)</span>. In this problem we will just focus on the density on the interval <span class="math notranslate nohighlight">\((0, 1)\)</span> and discover a nice consequence.</p>
<p>(a) For <span class="math notranslate nohighlight">\(0 &lt; x &lt; 1\)</span>, find <span class="math notranslate nohighlight">\(f_{S_2}(x)\)</span>.</p>
<p>(b) Use Part (a) and the convolution formula to find <span class="math notranslate nohighlight">\(f_{S_3}(x)\)</span> for <span class="math notranslate nohighlight">\(0 &lt; x &lt; 1\)</span>.</p>
<p>(c) Guess a formula for <span class="math notranslate nohighlight">\(f_{S_n}(x)\)</span> for <span class="math notranslate nohighlight">\(0 &lt; x &lt; 1\)</span> and prove it by induction.</p>
<p>(d) Find <span class="math notranslate nohighlight">\(P(S_n &lt; 1)\)</span>.</p>
<p>(e) Let <span class="math notranslate nohighlight">\(N = \min\{n: S_n &gt; 1\}\)</span>. Use Part (d) to find <span class="math notranslate nohighlight">\(E(N)\)</span>.</p>
<p><strong>24: Normal Sample Mean and Sample Variance, Part 1.</strong>
Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be i.i.d. with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[
\bar{X} ~ = ~ \frac{1}{n} \sum_{i=1}^n X_i
\]</div>
<p>denote the sample mean and</p>
<div class="math notranslate nohighlight">
\[
S^2 ~=~ \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2
\]</div>
<p>denote the sample variance as defined earlier in the course.</p>
<p>(a) For <span class="math notranslate nohighlight">\(1 \le i \le n\)</span> let <span class="math notranslate nohighlight">\(D_i = X_i - \bar{X}\)</span>. Find <span class="math notranslate nohighlight">\(Cov(D_i, \bar{X})\)</span>.</p>
<p>(b) Now assume in addition that <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span>. What is the joint distribution of <span class="math notranslate nohighlight">\(\bar{X}, D_1, D_2, \ldots, D_{n-1}\)</span>? Explain why <span class="math notranslate nohighlight">\(D_n\)</span> isn’t on the list.</p>
<p>(c) True or false (justify your answer): The sample mean and sample variance of an i.i.d. normal sample are independent of each other.</p>
<p><strong>25: Normal Sample Mean and Sample Variance, Part 2</strong></p>
<p>(a) Let <span class="math notranslate nohighlight">\(R\)</span> have the chi-squared distribution with <span class="math notranslate nohighlight">\(n\)</span> degrees of freedom. What is the mgf of <span class="math notranslate nohighlight">\(R\)</span>?</p>
<p>(b)
For <span class="math notranslate nohighlight">\(R\)</span> as in Part (a), suppose
<span class="math notranslate nohighlight">\(R = V + W\)</span> where <span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(W\)</span> are independent and <span class="math notranslate nohighlight">\(V\)</span> has the chi-squared
distribution with <span class="math notranslate nohighlight">\(m &lt; n\)</span> degrees of freedom. Can you identify the distribution of <span class="math notranslate nohighlight">\(W\)</span>? Justify your answer.</p>
<p>(c) Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be any sequence of random variables and let <span class="math notranslate nohighlight">\(\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\)</span>. Let <span class="math notranslate nohighlight">\(\alpha\)</span> be
any constant. Prove the <em>sum of squares decomposition</em></p>
<div class="math notranslate nohighlight" id="equation-d">
<span class="eqno">(25.3)<a class="headerlink" href="#equation-d" title="Permalink to this equation">¶</a></span>\[
\sum_{i=1}^n (X_i - \alpha)^2 ~=~ \sum_{i=1}^n (X_i - \bar{X})^2 ~+~ n(\bar{X} - \alpha)^2
\]</div>
<div class="math notranslate nohighlight">
\[
S^2 ~=~ \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2
\]</div>
<p>Find a constant <span class="math notranslate nohighlight">\(c\)</span> such that <span class="math notranslate nohighlight">\(cS^2\)</span> has a chi-squared distribution. Provide the degrees of freedom.</p>
<p>[Use Parts (b) and (c) as well as the result of the previous exercise.]</p>
<p><strong>26.</strong>
The heights of a population of mother-daughter pairs have the bivariate normal distribution with equal means of 67 inches, equal SDs of 2 inches, and correlation 0.5.</p>
<p>(a) Of the mothers on the 90th percentile of heights, what proportion have daughters who are taller than the 90th percentile?</p>
<p>(b) In what proportion of mother-daughter pairs are both women taller than average?</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Chapter_25"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="04_Multiple_Regression.html" title="previous page"><span class="section-number">25.4. </span>Multiple Regression</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ani Adhikari<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>