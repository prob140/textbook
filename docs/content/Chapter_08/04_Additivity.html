

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>8.4. Additivity &#8212; Prob 140 Textbook</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/headers.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8.5. Method of Indicators" href="05_Method_of_Indicators.html" />
    <link rel="prev" title="8.3. Expectations of Functions" href="03_Expectations_of_Functions.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Prob 140 Textbook</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="http://prob140.org">
   Course Home
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../To_the_Student.html">
   To the  Student
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_01/00_Fundamentals.html">
   1. Fundamentals
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">
     1.1. Outcome Space and Events
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">
     1.2. Equally Likely Outcomes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">
     1.3. Collisions in Hashing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">
     1.4. The Birthday Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">
     1.5. An Exponential Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/06_Exercises.html">
     1.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">
   2. Calculating Chances
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/01_Addition.html">
     2.1. Addition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/02_Examples.html">
     2.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/03_Multiplication.html">
     2.3. Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/04_More_Examples.html">
     2.4. More Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">
     2.5. Updating Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/06_Exercises.html">
     2.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_03/00_Random_Variables.html">
   3. Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">
     3.1. Functions on an Outcome Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/02_Distributions.html">
     3.2. Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/03_Equality.html">
     3.3. Equality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/04_Exercises.html">
     3.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">
   4. Relations Between Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">
     4.1. Joint Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/02_Examples.html">
     4.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">
     4.3. Marginal Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">
     4.4. Conditional Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">
     4.5. Dependence and Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/06_Exercises.html">
     4.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">
   5. Collections of Events
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">
     5.1. Bounding the Chance of a Union
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">
     5.2. Inclusion-Exclusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">
     5.3. The Matching Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">
     5.4. Sampling Without Replacement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/05_Exercises.html">
     5.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_06/00_Random_Counts.html">
   6. Random Counts
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">
     6.1. The Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/02_Examples.html">
     6.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/03_Multinomial_Distribution.html">
     6.3. Multinomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/04_The_Hypergeometric_Revisited.html">
     6.4. The Hypergeometric, Revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/05_Odds_Ratios.html">
     6.5. Odds Ratios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/06_Law_of_Small_Numbers.html">
     6.6. The Law of Small Numbers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/07_Exercises.html">
     6.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_07/00_Poissonization.html">
   7. Poissonization
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/01_Poisson_Distribution.html">
     7.1. Poisson Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Binomial.html">
     7.2. Poissonizing the Binomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/03_Poissonizing_the_Multinomial.html">
     7.3. Poissonizing the Multinomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/04_Exercises.html">
     7.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="00_Expectation.html">
   8. Expectation
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_Definition.html">
     8.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_Applying_the_Definition.html">
     8.2. Applying the Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_Expectations_of_Functions.html">
     8.3. Expectations of Functions
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.4. Additivity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05_Method_of_Indicators.html">
     8.5. Method of Indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06_Exercises.html">
     8.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">
   9. Conditioning, Revisited
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">
     9.1. Probability by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">
     9.2. Expectation by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">
     9.3. Expected Waiting Times
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/04_Exercises.html">
     9.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">
   10. Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/01_Transitions.html">
     10.1. Transitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">
     10.2. Deconstructing Chains
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">
     10.3. Long Run Behavior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/04_Examples.html">
     10.4. Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_11/00_Markov_Chain_Monte_Carlo.html">
   11. Markov Chain Monte Carlo
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/01_Balance_and_Detailed_Balance.html">
     11.1. Balance and Detailed Balance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/02_Code_Breaking.html">
     11.2. Code Breaking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/03_Metropolis_Algorithm.html">
     11.3. Metropolis Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/04_Exercises.html">
     11.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">
   12. Standard Deviation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/01_Definition.html">
     12.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">
     12.2. Prediction and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/03_Bounds.html">
     12.3. Tail Bounds
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">
     12.4. Heavy Tails
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/05_Exercises.html">
     12.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">
   13. Variance Via Covariance
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/01_Covariance.html">
     13.1. Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/02_Properties_of_Covariance.html">
     13.2. Properties of Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/03_Sums_of_Independent_Variables.html">
     13.3. Sums of Independent Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/04_Symmetry_and_Indicators.html">
     13.4. Symmetry and Indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/05_Finite_Population_Correction.html">
     13.5. Finite Population Correction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/06_Exercises.html">
     13.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">
   14. The Central Limit Theorem
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/01_Exact_Distribution_of_a_Sum.html">
     14.1. Exact Distribution of a Sum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">
     14.2. PGFs in NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">
     14.3. Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/04_SciPy_and_Normal_Curves.html">
     14.4. SciPy and Normal Curves
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/05_The_Sample_Mean.html">
     14.5. The Sample Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/06_Confidence_Intervals.html">
     14.6. Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/07_Exercises.html">
     14.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">
   15. Continuous Distributions
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">
     15.1. Density and CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">
     15.2. The Meaning of Density
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/03_Expectation.html">
     15.3. Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">
     15.4. Exponential Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">
     15.5. Calculus in SymPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/06_Exercises.html">
     15.6. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_16/00_Transformations.html">
   16. Transformations
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">
     16.1. Linear Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">
     16.2. Monotone Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/03_Simulation_via_the_CDF.html">
     16.3. Simulation via the CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/04_Two_to_One_Functions.html">
     16.4. Two-to-One Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/05_Exercises.html">
     16.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">
   17. Joint Densities
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">
     17.1. Probabilities and Expectations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/02_Independence.html">
     17.2. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">
     17.3. Marginal and Conditional Densities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">
     17.4. Beta Densities with Integer Parameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/05_Exercises.html">
     17.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">
   18. The Normal and Gamma Families
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">
     18.1. Standard Normal: The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">
     18.2. Sums of Independent Normal Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">
     18.3. The Gamma Family
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">
     18.4. Chi-Squared Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/05_Exercises.html">
     18.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">
   19. Distributions of Sums
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">
     19.1. The Convolution Formula
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">
     19.2. Moment Generating Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">
     19.3. MGFs, the Normal, and the CLT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">
     19.4. Chernoff Bound
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/05_Exercises.html">
     19.5. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_20/00_Approaches_to_Estimation.html">
   20. Approaches to Estimation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/01_Maximum_Likelihood.html">
     20.1. Maximum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/02_Independence_Revisited.html">
     20.2. Independence, Revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/03_Prior_and_Posterior.html">
     20.3. Prior and Posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_20/04_Exercises.html">
     20.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">
   21. The Beta and the Binomial
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">
     21.1. Updating and Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">
     21.2. The Beta-Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">
     21.3. Long Run Proportion of Heads
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/04_Exercises.html">
     21.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_22/00_Prediction.html">
   22. Prediction
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">
     22.1. Conditional Expectation As a Projection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/02_Variance_by_Conditioning.html">
     22.2. Variance by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/03_Examples.html">
     22.3. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/04_Least_Squares_Predictor.html">
     22.4. Least Squares Predictor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">
   23. Jointly Normal Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">
     23.1. Random Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Distribution.html">
     23.2. Multivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/03_Linear_Combinations.html">
     23.3. Linear Combinations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/04_Independence.html">
     23.4. Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">
   24. Simple Linear Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/01_Bivariate_Normal_Distribution.html">
     24.1. Bivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/02_Linear_Least_Squares.html">
     24.2. Least Squares Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">
     24.3. Regression and the Bivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">
     24.4. The Regression Equation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_25/00_Multiple_Regression.html">
   25. Multiple Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/01_Bilinearity_in_Matrix_Notation.html">
     25.1. Bilinearity in Matrix Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/02_Best_Linear_Predictor.html">
     25.2. Best Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/03_Multivariate_Normal_Conditioning.html">
     25.3. Conditioning and the Multivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/04_Multiple_Regression.html">
     25.4. Multiple Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/05_Further_Review_Exercises.html">
     25.5. Further Review Exercises
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Chapter_08/04_Additivity.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_08/04_Additivity.ipynb&branch=gh-pages"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> On this page
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additivity-of-expectation">
   8.4.1. Additivity of Expectation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#e-x-2-for-a-poisson-variable-x">
   8.4.2.
   <span class="math notranslate nohighlight">
    \(E(X^2)\)
   </span>
   for a Poisson Variable
   <span class="math notranslate nohighlight">
    \(X\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sample-sum">
   8.4.3. Sample Sum
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unbiased-estimator">
   8.4.4. Unbiased Estimator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unbiased-estimators-of-a-population-mean">
   8.4.5. Unbiased Estimators of a Population Mean
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#first-unbiased-estimator-of-a-maximum-possible-value">
   8.4.6. First Unbiased Estimator of a Maximum Possible Value
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#second-unbiased-estimator-of-the-maximum-possible-value">
   8.4.7. Second Unbiased Estimator of the Maximum Possible Value
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#which-estimator-to-use">
   8.4.8. Which Estimator to Use?
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="additivity">
<h1><span class="section-number">8.4. </span>Additivity<a class="headerlink" href="#additivity" title="Permalink to this headline">¶</a></h1>
<p>Calculating expectation by plugging into the definition works in simple cases, but often it can be cumbersome or lack insight. The most powerful result for calculating expectation turns out not to be the definition. It looks rather innocuous:</p>
<div class="section" id="additivity-of-expectation">
<h2><span class="section-number">8.4.1. </span>Additivity of Expectation<a class="headerlink" href="#additivity-of-expectation" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables defined on the same probability space. Then</p>
<div class="math notranslate nohighlight">
\[
E(X+Y) = E(X) + E(Y)
\]</div>
<p>Before we look more closely at this result, note that we are assuming that all the expectations exist; we will do this throughout in this course.</p>
<p>And now note that <strong>there are no assumptions about the relation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></strong>. They could be dependent or independent. Regardless, the expectation of the sum is the sum of the expectations. This makes the result powerful.</p>
<div class="cell tag_remove_input tag_hide-output docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="400"
    height="300"
    src="https://www.youtube.com/embed/HzZEhM4NHUQ"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
<p>Additivity follows easily from the definition of <span class="math notranslate nohighlight">\(X+Y\)</span> and the definition of expectation on the domain space. First note that the random variable <span class="math notranslate nohighlight">\(X+Y\)</span> is the function defined by</p>
<div class="math notranslate nohighlight">
\[
(X+Y)(\omega) = X(\omega) + Y(\omega) ~~~~ \text{for all }
\omega \in \Omega
\]</div>
<p>Thus a “value of <span class="math notranslate nohighlight">\(X+Y\)</span> weighted by the probability” can be written as</p>
<div class="math notranslate nohighlight">
\[
(X+Y)(\omega) \cdot P(\omega) = X(\omega)P(\omega) + 
Y(\omega)P(\omega )
\]</div>
<p>Sum the two sides over all <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> to prove additivty of expecation.</p>
<p>By induction, additivity extends to any finite number of random variables. If <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> are random variables defined on the same probability space, then</p>
<div class="math notranslate nohighlight">
\[
E(X_1 + X_2 + \cdots + X_n) = E(X_1) + E(X_2) + \cdots + E(X_n)
\]</div>
<p>regardless of the dependence structure of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>.</p>
<p>If you are trying to find an expectation, then the way to use additivity is to write your random variable as a sum of simpler variables whose expectations you know or can calculate easily.</p>
</div>
<div class="section" id="e-x-2-for-a-poisson-variable-x">
<h2><span class="section-number">8.4.2. </span><span class="math notranslate nohighlight">\(E(X^2)\)</span> for a Poisson Variable <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#e-x-2-for-a-poisson-variable-x" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> have the Poisson <span class="math notranslate nohighlight">\(\mu\)</span> distribution. In earlier sections we showed that <span class="math notranslate nohighlight">\(E(X) = \mu\)</span> and <span class="math notranslate nohighlight">\(E(X(X-1)) = \mu^2\)</span>.</p>
<p>Now <span class="math notranslate nohighlight">\(X^2 = X(X-1) + X\)</span>. The random variables <span class="math notranslate nohighlight">\(X(X-1)\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are both functions of <span class="math notranslate nohighlight">\(X\)</span>, so they are not independent of each other. But additivity of expectation doesn’t require independence, so we can use it to see that</p>
<div class="math notranslate nohighlight">
\[
E(X^2) ~ = ~ E(X(X-1)) + E(X) ~ = ~ \mu^2 + \mu
\]</div>
<p>We will use this fact later when we study the variability of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>It is worth noting that it is not easy to calculate <span class="math notranslate nohighlight">\(E(X^2)\)</span> directly, since</p>
<div class="math notranslate nohighlight">
\[
E(X^2) ~ = ~ \sum_{k=0}^\infty k^2 e^{-\mu}\frac{\mu^k}{k!}
\]</div>
<p>is not an easy sum to simplify.</p>
</div>
<div class="section" id="sample-sum">
<h2><span class="section-number">8.4.3. </span>Sample Sum<a class="headerlink" href="#sample-sum" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be a sample drawn at random from a numerical population that has mean <span class="math notranslate nohighlight">\(\mu\)</span>, and let the sample sum be</p>
<div class="math notranslate nohighlight">
\[
S_n = X_1 + X_2 + \cdots + X_n
\]</div>
<p>Then, regardless of whether the sample was drawn with or without replacement, each <span class="math notranslate nohighlight">\(X_i\)</span> has the same distribution as the population. This is clearly true if the sampling is with replacement, and it is true by symmetry if the sampling is without replacement as we saw in an earlier chapter.</p>
<p>So, regardless of whether the sample is drawn with or without replacement, <span class="math notranslate nohighlight">\(E(X_i) = \mu\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>, and hence</p>
<div class="math notranslate nohighlight">
\[
E(S_n) = E(X_1) + E(X_2) + \cdots + E(X_n) = n\mu
\]</div>
<p>We can use this to estimate a population mean based on a sample mean.</p>
</div>
<div class="section" id="unbiased-estimator">
<h2><span class="section-number">8.4.4. </span>Unbiased Estimator<a class="headerlink" href="#unbiased-estimator" title="Permalink to this headline">¶</a></h2>
<p>Suppose a random variable <span class="math notranslate nohighlight">\(X\)</span> is being used to estimate a fixed numerical parameter <span class="math notranslate nohighlight">\(\theta\)</span>. Then <span class="math notranslate nohighlight">\(X\)</span> is called an <em>estimator</em> of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>The <em>bias</em> of <span class="math notranslate nohighlight">\(X\)</span> is the difference <span class="math notranslate nohighlight">\(E(X) - \theta\)</span>. The bias measures the amount by which the estimator exceeds the parameter, on average. The bias can be negative if the estimator tends to underestimate the parameter.</p>
<p>If the bias of an estimator is <span class="math notranslate nohighlight">\(0\)</span> then the estimator is called <em>unbiased</em>. So <span class="math notranslate nohighlight">\(X\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\theta\)</span> if <span class="math notranslate nohighlight">\(E(X) = \theta\)</span>.</p>
<p>If an estimator is unbiased, and you use it to generate estimates repeatedly and independently, then in the long run the average of all the estimates is equal to the parameter being estimated. On average, the unbiased estimator is neither higher nor lower than the parameter. That’s usually considered a good quality in an estimator.</p>
<p>In practical terms, if a data scientist wants to estimate an unknown parameter based on a random sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>, the data scientist has to come up with a <em>statistic</em> to use as the estimator.</p>
<p>Recall from Data 8 that a statistic is a number computed from the sample. In other words, a statistic is a numerical function of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>.</p>
<p>Constructing an unbiased estimator of a parameter <span class="math notranslate nohighlight">\(\theta\)</span> therefore amounts to finding a statistic <span class="math notranslate nohighlight">\(T = g(X_1, X_2, \ldots, X_n)\)</span> for a function <span class="math notranslate nohighlight">\(g\)</span> such that <span class="math notranslate nohighlight">\(E(T) = \theta\)</span>.</p>
</div>
<div class="section" id="unbiased-estimators-of-a-population-mean">
<h2><span class="section-number">8.4.5. </span>Unbiased Estimators of a Population Mean<a class="headerlink" href="#unbiased-estimators-of-a-population-mean" title="Permalink to this headline">¶</a></h2>
<p>As in the sample sum example above, let <span class="math notranslate nohighlight">\(S_n\)</span> be the sum of a sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> drawn at random from a population that has mean <span class="math notranslate nohighlight">\(\mu\)</span>. The standard statistical notation for the average of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> is <span class="math notranslate nohighlight">\(\bar{X}_n\)</span>. So</p>
<div class="math notranslate nohighlight">
\[
\bar{X}_n = \frac{S_n}{n}
\]</div>
<p>Then, regardless of whether the draws were made with replacement or without,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E(\bar{X}_n) &amp;= \frac{E(S_n)}{n} ~~~~ \text{(linear function rule)} \\
&amp;= \frac{n \mu}{n} ~~~~~~~~~ \text{(} E(S_n) = n\mu \text{)} \\
&amp;= \mu
\end{align*}
\end{split}\]</div>
<p>Thus the sample mean is an unbiased estimator of the population mean.</p>
<p>It is worth noting that <span class="math notranslate nohighlight">\(X_1\)</span> is also an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>, since <span class="math notranslate nohighlight">\(E(X_1) = \mu\)</span>. So is <span class="math notranslate nohighlight">\(X_j\)</span> for any <span class="math notranslate nohighlight">\(j\)</span>, also <span class="math notranslate nohighlight">\((X_1 + X_9)/2\)</span>, or any linear combination of the sample if the coefficients add up to 1.</p>
<p>But it seems clear that using the sample mean as the estimator is better than using just one sampled element, even though both are unbiased. This is true, and is related to how variable the estimators are. We will address this later in the course.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span> be i.i.d. Poisson <span class="math notranslate nohighlight">\((\mu)\)</span> random variables, and suppose the value of <span class="math notranslate nohighlight">\(\mu\)</span> is unknown. Is <span class="math notranslate nohighlight">\(0.4X_1 + 0.2X_2 + 0.4X_3\)</span> an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Yes</p>
</div>
<div class="cell tag_remove-input tag_hide-output docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="400"
    height="300"
    src="https://www.youtube.com/embed/ruEpGZJwHmw"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
</div>
<div class="section" id="first-unbiased-estimator-of-a-maximum-possible-value">
<h2><span class="section-number">8.4.6. </span>First Unbiased Estimator of a Maximum Possible Value<a class="headerlink" href="#first-unbiased-estimator-of-a-maximum-possible-value" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have a sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> drawn at random from <span class="math notranslate nohighlight">\(1, 2, \ldots , N\)</span> for some fixed <span class="math notranslate nohighlight">\(N\)</span>, and we are trying to estimate <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>How can we use the sample to construct an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>? By definition, such an estimator must be a function of the sample and its expectation must be <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>In other words, we have to construct a statistic that has expectation <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Each <span class="math notranslate nohighlight">\(X_i\)</span> has the uniform distribution on <span class="math notranslate nohighlight">\(1, 2, \ldots , N\)</span>. This is true for sampling with replacement as well as for simple random sampling, by symmetry.</p>
<p>The expectation of each of the uniform variables is <span class="math notranslate nohighlight">\((N+1)/2\)</span>, as we have seen earlier. So if <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> is the sample mean, then</p>
<div class="math notranslate nohighlight">
\[
E(\bar{X}_n) = \frac{N+1}{2}
\]</div>
<p>Clearly, <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> is not an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>. That’s not surprising because <span class="math notranslate nohighlight">\(N\)</span> is the maximum possible value of each observation and <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> should be somewhere in the middle of all the possible values.</p>
<p>But because <span class="math notranslate nohighlight">\(E(\bar{X}_n)\)</span> is a linear function of <span class="math notranslate nohighlight">\(N\)</span>, we can figure out how to create an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Remember that our job is to create a function of the sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> in such a way that the expectation of that function is <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Start by inverting the linear function, that is, by isolating <span class="math notranslate nohighlight">\(N\)</span> in the equation above.</p>
<div class="math notranslate nohighlight">
\[
2E(\bar{X}_n) - 1 =  N
\]</div>
<p>This tells us what we have to do to the sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> to get an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>We should just use the statistic <span class="math notranslate nohighlight">\(T_1 = 2\bar{X}_n - 1\)</span> as the estimator. It is unbiased because <span class="math notranslate nohighlight">\(E(T_1) = N\)</span> by the calculation above.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>In the setting above, what is the bias of <span class="math notranslate nohighlight">\(2\bar{X}_n\)</span> as an estimator of <span class="math notranslate nohighlight">\(N\)</span>? Does it tend to overestimate on average, or underestimate?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(1\)</span>; overestimate</p>
</div>
</div>
<div class="section" id="second-unbiased-estimator-of-the-maximum-possible-value">
<h2><span class="section-number">8.4.7. </span>Second Unbiased Estimator of the Maximum Possible Value<a class="headerlink" href="#second-unbiased-estimator-of-the-maximum-possible-value" title="Permalink to this headline">¶</a></h2>
<p>The calculation above stems from a problem the Allied forces faced in World War II. Germany had a seemingly never-ending fleet of Panzer tanks, and the Allies needed to estimate how many they had. They decided to base their estimates on the serial numbers of the tanks that they saw.</p>
<p>Here is a picture of one from <a class="reference external" href="https://en.wikipedia.org/wiki/Panzer_IV">Wikipedia</a>.</p>
<p><img alt="Panzer Tank" src="../../_images/panzer.png" /></p>
<p>Notice the serial number on the top left. When tanks were disabled or destroyed, it was discovered that their parts had serial numbers too. The ones from the gear boxes proved very useful.</p>
<p>The idea was to model the observed serial numbers as random draws from <span class="math notranslate nohighlight">\(1, 2, \ldots, N\)</span> and then estimate <span class="math notranslate nohighlight">\(N\)</span>. This is of course a very simplified model of reality. But estimates based on even such simple probabilistic models proved to be quite a bit <a class="reference external" href="https://en.wikipedia.org/wiki/German_tank_problem#Specific_data">more accurate</a> than those based on the intelligence gathered by the Allies. For example, in August 1942, intelligence estimates were that Germany was producing 1,550 tanks per month. The prediction based on the probability model was 327 per month. After the war, German records showed that the actual production rate was 342 per month.</p>
<p>The model was that the draws were made at random without replacement from the integers 1 through <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>In the example above, we constructed the random variable <span class="math notranslate nohighlight">\(T\)</span> to be an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span> under this model.</p>
<p>The Allied statisticians instead started with <span class="math notranslate nohighlight">\(M\)</span>, the sample maximum:</p>
<div class="math notranslate nohighlight">
\[
M ~ = ~ \max\{X_1, X_2, \ldots, X_n\}
\]</div>
<p>The sample maximum <span class="math notranslate nohighlight">\(M\)</span> is a biased estimator of <span class="math notranslate nohighlight">\(N\)</span>, because we know that its value is always less than or equal to <span class="math notranslate nohighlight">\(N\)</span>. Its average value therefore will be somewhat less than <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>To correct for this, the Allied statisticians imagined a row of <span class="math notranslate nohighlight">\(N\)</span> spots for the serial numbers <span class="math notranslate nohighlight">\(1\)</span> through <span class="math notranslate nohighlight">\(N\)</span>, with marks at the spots corresponding to the observed serial numbers. The visualization below shows an outcome in the case <span class="math notranslate nohighlight">\(N= 20\)</span> and <span class="math notranslate nohighlight">\(n = 3\)</span>.</p>
<p><img alt="gaps" src="../../_images/all_gaps.png" /></p>
<ul class="simple">
<li><p>There are <span class="math notranslate nohighlight">\(N = 20\)</span> spots in all.</p></li>
<li><p>From these, we take a simple random sample of size <span class="math notranslate nohighlight">\(n = 3\)</span>. Those are the gold spots.</p></li>
<li><p>The remaining <span class="math notranslate nohighlight">\(N - n = 17\)</span> spots are colored blue.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(n = 3\)</span> sampled spots create <span class="math notranslate nohighlight">\(n+1 = 4\)</span> blue “gaps” between sampled values: one before the leftmost gold spot, two between successive gold spots, and one after the rightmost gold spot that is at position <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>A key observation is that because of the symmetry of simple random sampling, <strong>the lengths of all four gaps have the same distribution.</strong></p>
<p>But of course we don’t get to see all the gaps. In the sample, we can see all but the last gap, as in the figure below. The red question mark reminds you that the gap to the right of <span class="math notranslate nohighlight">\(M\)</span> is invisible to us.</p>
<p><img alt="mystery gap" src="../../_images/mystery_gap.png" /></p>
<p>If we could see the gap to the right of <span class="math notranslate nohighlight">\(M\)</span>, we would see <span class="math notranslate nohighlight">\(N\)</span>. But we can’t. So we can try to do the next best thing, which is to augment <span class="math notranslate nohighlight">\(M\)</span> by the estimated size of that gap.</p>
<p>Since we can see all of the spots and their colors up to and including <span class="math notranslate nohighlight">\(M\)</span>, we can see <span class="math notranslate nohighlight">\(n\)</span> out of the <span class="math notranslate nohighlight">\(n+1\)</span> gaps. The lengths of the gaps all have the same distribution by symmetry, so we can estimate the length of a single gap by the average length of all the gaps that we can see.</p>
<p>We can see <span class="math notranslate nohighlight">\(M\)</span> spots, of which <span class="math notranslate nohighlight">\(n\)</span> are the sampled values. So the total length of all <span class="math notranslate nohighlight">\(n\)</span> visible gaps is <span class="math notranslate nohighlight">\(M-n\)</span>. Therefore</p>
<div class="math notranslate nohighlight">
\[
\text{estimated length of one gap} ~ = ~ \frac{M-n}{n}
\]</div>
<p>So the Allied statisticians decided to improve upon <span class="math notranslate nohighlight">\(M\)</span> by using the <em>augmented maximum</em> as their estimator:</p>
<div class="math notranslate nohighlight">
\[
T_2 ~ = ~  M + \frac{M-n}{n}
\]</div>
<p>By algebra, this estimator can be rewritten as</p>
<div class="math notranslate nohighlight">
\[
T_2 ~ = ~ M\cdot\frac{n+1}{n} ~ - ~ 1
\]</div>
<p>Is <span class="math notranslate nohighlight">\(T_2\)</span> an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>? To answer this, we have to find its expectation. Since <span class="math notranslate nohighlight">\(T_2\)</span> is a linear function of <span class="math notranslate nohighlight">\(M\)</span>, we’ll find the expectation of <span class="math notranslate nohighlight">\(M\)</span> first.</p>
<p>Here once again is the visualization of what’s going on.</p>
<p><img alt="gaps" src="../../_images/all_gaps.png" /></p>
<p>Let <span class="math notranslate nohighlight">\(G\)</span> be the length of the last gap. Then <span class="math notranslate nohighlight">\(M = N - G\)</span>.</p>
<p>There are <span class="math notranslate nohighlight">\(n+1\)</span> gaps, made up of the <span class="math notranslate nohighlight">\(N-n\)</span> unsampled values. Since they all have the same expected length,</p>
<div class="math notranslate nohighlight">
\[
E(G) ~ = ~ \frac{N-n}{n+1}
\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
E(M) ~ = ~ N - \frac{N-n}{n+1} ~ = ~ (N+1)\frac{n}{n+1}
\]</div>
<p>Recall that the Allied statisticians’ estimate of <span class="math notranslate nohighlight">\(N\)</span> is</p>
<div class="math notranslate nohighlight">
\[
T_2 ~ = ~ M\cdot\frac{n+1}{n}  -  1
\]</div>
<p>Now</p>
<div class="math notranslate nohighlight">
\[
E(T_2) ~ = ~ E(M)\cdot\frac{n+1}{n}  -  1 ~ = ~ (N+1)\frac{n}{n+1}\cdot\frac{n+1}{n} - 1 ~ = ~ N
\]</div>
<p>Thus the augmented maximum <span class="math notranslate nohighlight">\(T_2\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>A gardener in Berkeley has 23 blue flower pots in a row. She picks a simple random sample of 5 of them and colors the selected pots gold. What is the expected number of blue flower pots at the end of the row?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(3\)</span></p>
</div>
</div>
<div class="section" id="which-estimator-to-use">
<h2><span class="section-number">8.4.8. </span>Which Estimator to Use?<a class="headerlink" href="#which-estimator-to-use" title="Permalink to this headline">¶</a></h2>
<p>The Allied statisticians thus had two unbiased estimators of <span class="math notranslate nohighlight">\(N\)</span> from which to choose. They went with <span class="math notranslate nohighlight">\(T_2\)</span> instead of <span class="math notranslate nohighlight">\(T_1\)</span> because <span class="math notranslate nohighlight">\(T_2\)</span> has less variability.</p>
<p>We will quantify this later in the course. For now, here is a simulation of distributions of the two estimators in the case <span class="math notranslate nohighlight">\(N = 300\)</span> and <span class="math notranslate nohighlight">\(n=30\)</span>. The simulation is based on <span class="math notranslate nohighlight">\(5000\)</span> repetitions of drawing a simple random sample of size <span class="math notranslate nohighlight">\(30\)</span> from the integers <span class="math notranslate nohighlight">\(1\)</span> through <span class="math notranslate nohighlight">\(300\)</span>.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/04_Additivity_20_0.png" src="../../_images/04_Additivity_20_0.png" />
</div>
</div>
<p>You can see why <span class="math notranslate nohighlight">\(T_2\)</span> is a better estimator than <span class="math notranslate nohighlight">\(T_1\)</span>.</p>
<ul class="simple">
<li><p>Both are unbiased. So both the empirical histograms are balanced at around <span class="math notranslate nohighlight">\(300\)</span>, the true value of <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p>The emipirical distribution of <span class="math notranslate nohighlight">\(T_2\)</span> is clustered much closer to the true value <span class="math notranslate nohighlight">\(300\)</span> than the empirical distribution of <span class="math notranslate nohighlight">\(T_1\)</span>.</p></li>
</ul>
<p>For a recap, take another look at the <a class="reference external" href="https://en.wikipedia.org/wiki/German_tank_problem#Specific_data">accuracy table</a> of the Allied statisticians’ estimator <span class="math notranslate nohighlight">\(T_2\)</span>. Not bad for an estimator based on a model that assumes nothing more complicated than simple random sampling!</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Chapter_08"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="03_Expectations_of_Functions.html" title="previous page"><span class="section-number">8.3. </span>Expectations of Functions</a>
    <a class='right-next' id="next-link" href="05_Method_of_Indicators.html" title="next page"><span class="section-number">8.5. </span>Method of Indicators</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ani Adhikari<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>