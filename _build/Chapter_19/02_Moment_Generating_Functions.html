---
redirect_from:
  - "/chapter-19/02-moment-generating-functions"
interact_link: content/Chapter_19/02_Moment_Generating_Functions.ipynb
kernel_name: python3
kernel_path: content/Chapter_19
has_widgets: false
title: |-
  Moment Generating Functions
pagenum: 99
prev_page:
  url: /Chapter_19/01_Convolution_Formula.html
next_page:
  url: /Chapter_19/03_MGFs_Normal_and_the_CLT.html
suffix: .ipynb
search: t x e mx lambda frac mgf distribution random r variable finite gamma big moments s probability function negative tx positive independent p variables interval mgfs align n moment generating ways pgf values let expectation not sum m xy cdot defined distributions non gx hence section well around heavy cant large functions et both properties useful proofs begin cdots end sums property same must y q text poisson infty lambdar rs mass also finitely integer real fact necessarily because case near three class results should even examples called course mean center just infinite continue differentiate term d second very earlier

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Moment Generating Functions</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Moment-Generating-Functions">Moment Generating Functions<a class="anchor-link" href="#Moment-Generating-Functions"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The probability mass function and probability density, cdf, and survival functions are all ways of specifying the probability distribution of a random variable. They are all defined as probabilities or as probability per unit length, and thus have natural interpretations and visualizations.</p>
<p>But there are also more abstract ways of describing distributions. One that you have encountered is the probability generating function (pgf), which we defined for random variables with finitely many non-negative integer values.</p>
<p>We now define another such <em>transform</em> of a distribution. More general than the pgf, it is a powerful tool for studying distributions.</p>
<p>Let $X$ be a random variable. The <em>moment generating function</em> (mgf) of $X$ is a function defined on the real numbers by the formula</p>
$$
M_X(t) ~ = ~ E(e^{tX}) 
$$<p>for all $t$ for which the expectation is finite. It is a fact (which we will not prove) that the domain of the mgf has to be an interval, not necessarily finite but necessarily including 0 because $M_X(0) = 1$.</p>
<p>For $X$ with finitely many non-negative integer values, we had defined the pgf by $G_X(s) = E(s^X)$. Notice that this is a special case of the mgf with $s = e^t$ and hence positive. For a random variable $X$ that has both a pgf $G_X$ and an mgf $M_X$, the two functions are related by $M_X(\log(s)) = G_X(s)$. Therefore the properties of $M_X$ near 0 reflect the properties of $G_X$ near 1.</p>
<p>This section presents three ways in which the mgf is useful. Other ways are demonstrated in the subsequent sections of this chapter. Much of what we say about mgf's will not be accompanied by complete proofs as the math required is beyond the scope of this class. But the results should seem reasonable, even without formal proofs.</p>
<p>We will list the three ways first, and then use them all in examples.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generating-Moments">Generating Moments<a class="anchor-link" href="#Generating-Moments"> </a></h3><p>For non-negative integers $k$, the expectation $E(X^k)$ is called <em>$k$th moment</em> of $X$. You saw in Data 8 and again in this course that the mean $E(X)$ is the center of gravity of the probability histogram of $X$. In physics, the center of mass is called the <em>first moment</em>. The terminology of moments is used in probability theory as well.</p>
<p>In this course we are only going to work with mgf's that are finite in some interval around 0. The interval could be the entire real line. It is a fact that if the mgf is finite around 0 (not just to one side of 0), then all the moments exist.</p>
<p>Expand $e^{tX}$ to see that</p>
$$
\begin{align*}
M_X(t) ~ &amp;= ~ E \big{(} 1 + t \frac{X}{1!} + t^2 \frac{X^2}{2!} + t^3 \frac{X^3}{3!} + \cdots \big{)} \\ \\
&amp;= ~ 1 + t \frac{E(X)}{1!} + t^2 \frac{E(X^2)}{2!} + t^3 \frac{E(X^3)}{3!} + \cdots
\end{align*}
$$<p>by blithely switching the expectation and the infinite sum. This requires justification, which we won't go into.</p>
<p>Continue to set aside questions about whether we can switch infinite sums with other operations. Just go ahead and differentiate $M_X$ term by term. Let $M_X^{(n)}$ denote the $n$th derivative. Then</p>
$$
M_X^{(1)} (t) ~ = ~ \frac{d}{dt} M_X(t) ~ = \frac{E(X)}{1!} + 2t \frac{E(X^2)}{2!} + 3t^2 \frac{E(X^3)}{3!} + \cdots
$$<p>and hence</p>
$$
M^{(1)} (0) ~ = ~ E(X)
$$<p>Now differentiate $M_X^{(1)}$ to see that $M_X^{(2)}(0) = E(X^2)$, and, by induction,</p>
$$
M^{(n)} (0) ~ = ~ E(X^n), ~~~~ n = 1, 2, 3, \ldots
$$<p>Hence we can <em>generate the moments of $X$</em> by evaluating successive derivatives of $M_X$ at $t=0$. This is one way in which mgf's are helpful.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Identifying-the-Distribution">Identifying the Distribution<a class="anchor-link" href="#Identifying-the-Distribution"> </a></h3><p>In this class we have made heavy use of the first and second moments, and no use at all of the higher moments. That will continue to be the case. But mgf's do involve all the moments, and this results in a property that is very useful for proving facts about distributions.</p>
<p><strong>If two distributions have the same mgf, then they must be the same distribution.</strong> This property is valid if the mgf exists in an interval around 0, which we assumed earlier in this section.</p>
<p>For example, if you recognize the mgf of a random variable as the mgf of a normal distribution, then the random variable must be normal.</p>
<p>By contrast, if you know the expectation of a random variable you can't identify the distribution of the random variable; even if you know both the mean and the SD (equivalently, the first and second moments), you can't identify the distribution. But if you know the moment generating function, and hence all the moments, then you can.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Working-Well-with-Sums">Working Well with Sums<a class="anchor-link" href="#Working-Well-with-Sums"> </a></h3><p>The third reason mgf's are useful is that like the pgf, the mgf of the sum of independent random variables is easily computed as a product.</p>
<p>Let $X$ and $Y$ be independent. Then</p>
$$
M_{X+Y} (t) ~ = ~ E(e^{t(X+Y)}) ~ = ~ E(e^{tX} \cdot e^{tY})
$$<p>So if $X$ and $Y$ are independent,</p>
$$
M_{X+Y}(t) ~ = ~ M_X(t) M_Y(t)
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's time for some examples. Remember that the mgf of $X$ is the expectation of a function of $X$. In some cases we will calculate it using the non-linear function rule for expectations. In other cases we will use the multiplicative property of the mgf of the sum of independent random variables.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="MGFs-of-Some-Discrete-Random-Variables">MGFs of Some Discrete Random Variables<a class="anchor-link" href="#MGFs-of-Some-Discrete-Random-Variables"> </a></h3><h4 id="Bernoulli-$(p)$">Bernoulli $(p)$<a class="anchor-link" href="#Bernoulli-$(p)$"> </a></h4><p>$P(X = 1) = p$ and $P(X = 0) = 1 - p = q$. So</p>
$$
M_X(t) ~ = ~ qe^{t \cdot 0} + pe^{t \cdot 1} ~ = ~ q + pe^t ~ = ~ 1 + p(e^t - 1)  ~~~ \text{for all } t 
$$<h4 id="Binomial-$(n,-p)$">Binomial $(n, p)$<a class="anchor-link" href="#Binomial-$(n,-p)$"> </a></h4><p>A binomial random variable is the sum of $n$ i.i.d. indicators. So</p>
$$
M_X(t) ~ = ~ (q + pe^t)^n ~~~ \text{for all } t 
$$<h4 id="Poisson-$(\mu)$">Poisson $(\mu)$<a class="anchor-link" href="#Poisson-$(\mu)$"> </a></h4><p>This one is an exercise.</p>
$$
M_X(t) ~ = ~ e^{\mu(e^t - 1)} ~~~ \text{for all } t
$$<p>You can also use this to show that the sum of independent Poisson variables is Poisson.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="MGF-of-a-Gamma-$(r,-\lambda-)$-Random-Variable">MGF of a Gamma $(r, \lambda )$ Random Variable<a class="anchor-link" href="#MGF-of-a-Gamma-$(r,-\lambda-)$-Random-Variable"> </a></h3><p>Let $X$ have the gamma $(r, \lambda)$ distribution. Then</p>
$$
\begin{align*}
M_X(t) ~ &amp;= ~ \int_0^\infty e^{tx} \frac{\lambda^r}{\Gamma(r)} x^{r-1} e^{-\lambda x} dx \\ \\
&amp;= ~ \frac{\lambda^r}{\Gamma(r)} \int_0^\infty x^{r-1} e^{-(\lambda - t)x} dx \\ \\
&amp;= ~ \frac{\lambda^r}{\Gamma(r)} \cdot \frac{\Gamma(r)}{(\lambda - t)^r} ~~~~ t &lt; \lambda \\ \\
&amp;= \big{(} \frac{\lambda}{\lambda - t} \big{)}^r ~~~~ t &lt; \lambda
\end{align*} 
$$<h3 id="Sums-of-Independent-Gamma-Variables-with-the-Same-Rate">Sums of Independent Gamma Variables with the Same Rate<a class="anchor-link" href="#Sums-of-Independent-Gamma-Variables-with-the-Same-Rate"> </a></h3><p>If $X$ has gamma $(r, \lambda)$ distribution and $Y$ independent of $X$ has gamma $(s, \lambda)$ distribution, then</p>
$$
\begin{align*} 
M_{X+Y} (t) ~ &amp;= ~ \big{(} \frac{\lambda}{\lambda - t} \big{)}^r \cdot \big{(} \frac{\lambda}{\lambda - t} \big{)}^s ~~~~ t &lt; \lambda \\ \\
&amp;= ~ \big{(} \frac{\lambda}{\lambda - t} \big{)}^{r+s} ~~~~ t &lt; \lambda
\end{align*}
$$<p>That's the mgf of the gamma $(r+s, \lambda)$ distribution. Because the mgf identifies the distribution, $X+Y$ must have the gamma $(r+s, \lambda)$ distribution.</p>
<p>This is what we observed in an earlier section by simulation, using numerical values of $r$ and $\lambda$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Note-on-Existence">Note on Existence<a class="anchor-link" href="#Note-on-Existence"> </a></h3><p>Let $X$ be a random variable. For all $t$, the random variable $e^{tX}$ is positive, so $M_X(t)$ is either positive or $+\infty$.</p>
<p>The rough statements below should give you a sense of the connection between the tails of the distribution of $X$ and the existence of the mgf. We will not cover the proofs.</p>
<p>If $t &gt; 0$ then $e^{tX}$ is large for large positive values of $X$. So if $M_X(t)$ is finite for a positive $t$, then the right hand tail of the distribution of $X$ can't be heavy.</p>
<p>If $t &lt; 0$ then $e^{tX}$ is large for large negative values of $X$. So if $M_X(t)$ is finite for a negative $t$, then the left hand tail of the distribution of $X$ can't be heavy.</p>
<p>So if $M_X(t)$ is finite for a positive value of $t$ as well as for a negative value of $t$, then both of the tails aren't heavy.</p>
<p>It can be shown that if $M_X(t)$ is finite for some $t$, then $M_X(s)$ is finite for all $s$ between 0 and $t$. So $M_X(t)$ being finite for a positive $t$ as well as for a negative $t$ is equivalent to $M_X$ being finite on an interval around 0. The interval might be very small, but as long as it straddles 0 all the properties listed in this section hold.</p>

</div>
</div>
</div>
</div>

 


    </main>
    