---
redirect_from:
  - "/chapter-19/04-chernoff-bound"
interact_link: content/Chapter_19/04_Chernoff_Bound.ipynb
kernel_name: python3
kernel_path: content/Chapter_19
has_widgets: false
title: |-
  Chernoff Bound
pagenum: 101
prev_page:
  url: /Chapter_19/03_MGFs_Normal_and_the_CLT.html
next_page:
  url: /Chapter_20/00_Approaches_to_Estimation.html
suffix: .ipynb
search: x t c e bound p ge mu frac sigma le tc chernoff bounds function distribution value random tx exact tail g because min tails variable right negative any align mx normal sigmat ct finding independent form mean let markovs hand non chebychevs moment generating line below increasing fixed upper optimized above m minimizing sums sn probabilities good help us already finite interval calculations event same begin end step positive sharper sharpest minimizes fix phi standard analytically curve graph case flat point minimized log applied left variables ldots xn intractable difficult integration estimates become important quantify roughly close likely such

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Chernoff Bound</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Chernoff-Bound">Chernoff Bound<a class="anchor-link" href="#Chernoff-Bound"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the form of a distribution is intractable in that it is difficult to find exact probabilities by integration, then good estimates and bounds become important. Bounds on the tails of the distribution of a random variable help us quantify roughly how close to the mean the random variable is likely to be.</p>
<p>We already know two such bounds. Let $X$ be a random variable with expectation $\mu$ and SD $\sigma$.</p>
<h4 id="Markov's-Bound-on-the-Right-Hand-Tail">Markov's Bound on the Right Hand Tail<a class="anchor-link" href="#Markov's-Bound-on-the-Right-Hand-Tail"> </a></h4><p>If $X$ is non-negative,</p>
$$
P(X \ge c) ~  \le ~ \frac{\mu}{c}
$$<h4 id="Chebychev's-Bound-on-Two-Tails">Chebychev's Bound on Two Tails<a class="anchor-link" href="#Chebychev's-Bound-on-Two-Tails"> </a></h4>$$
P(\lvert X - \mu\rvert \ge c) ~ \le ~ \frac{\sigma^2}{c^2}
$$<p>Moment generating functions can help us improve upon these bounds in many cases. In what follows, we will assume that the moment generating function of $X$ is finite over the whole real line. If it is finite only over a smaller interval around 0, the calculations of the mgf below should be confined to that interval.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chernoff-Bound-on-the-Right-Tail">Chernoff Bound on the Right Tail<a class="anchor-link" href="#Chernoff-Bound-on-the-Right-Tail"> </a></h3><p>Observe that if $g$ is an increasing function, then the event $\{ X \ge c \}$ is the same as the event $\{ g(X) \ge g(c)\}$.</p>
<p>For any fixed $t &gt; 0$, the function defined by $g(x) = e^{tx}$ is increasing as well as non-negative. So for each $t &gt; 0$,</p>
$$
\begin{align*}
P(X \ge c) ~ &amp;= P(e^{tX} \ge e^{tc}) \\
&amp;\le ~ \frac{E(e^{tX})}{e^{tc}} ~~~~ \text{(Markov's bound)} \\
&amp;= ~ \frac{M_X(t)}{e^{tc}}
\end{align*}
$$<p>This is the first step in developing a <a href="https://en.wikipedia.org/wiki/Chernoff_bound">Chernoff bound</a> on the right hand tail.</p>
<p>For the next step, notice that you can choose $t$ to be any positive number. Some choices of $t$ will give sharper bounds than others. Because these are upper bounds, the sharpest among all of the bounds will correspond to the value of $t$ that minimizes the right hand side. So the Chernoff bound has an <em>optimized</em> form:</p>
$$
P(X \ge c) ~ \le ~ \min_{t &gt; 0} \frac{M_X(t)}{e^{tc}}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Application-to-the-Normal-Distribution">Application to the Normal Distribution<a class="anchor-link" href="#Application-to-the-Normal-Distribution"> </a></h3><p>Suppose $X$ has the normal $(\mu, \sigma^2)$ distribution and we want to get a sense of how far $X$ can be above the mean. Fix $c &gt; 0$. The exact chance that the value of $X$ is at least $c$ above the mean is</p>
$$
P(X - \mu \ge c) ~ = ~ 1 - \Phi(c/\sigma)
$$<p>because the distribution of $X - \mu$ is normal $(0, \sigma^2)$. This exact answer looks neat and tidy, but the standard normal cdf $\Phi$ is not easy to work with analytically. Sometimes we can gain more insight from a good bound.</p>
<p>The optimized Chernoff bound is</p>
$$
\begin{align*}
P(X- \mu \ge c) ~ &amp;\le ~ \min_{t &gt; 0} \frac{M_{X-\mu}(t)}{e^{tc}} \\ \\
&amp;= ~ \min_{t &gt; 0} \frac{e^{\sigma^2t^2/2}}{e^{tc}} \\ \\
&amp;= ~ \min_{t &gt; 0} e^{-ct + \sigma^2t^2/2}
\end{align*}
$$<p>The curve below is the graph of $\exp(-ct + \sigma^2t^2/2)$ as a function of $t$, in the case $\sigma = 2$ and $c = 5$. The flat line is the exact probability of $P(X - \mu \ge c)$. The curve is always above the flat line: no matter what $t$ is, the bound is an upper bound. The sharpest of all the upper bounds corresponds to the minimizing value $t^*$ which is somewhere in the 1.2 to 1.3 range.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_19/04_Chernoff_Bound_4_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To find the minimizing value of $t$ analytically, we will use the standard calculus method of minimization. But first we will simplify our calculations by observing that finding the point at which a positive function is minimized is the same as finding the point at which the log of the function is minimized. This is because $\log$ is an increasing function.</p>
<p>So the problem reduces to finding the value of $t$ that minimizes the function $h(t) = -ct + \sigma^2t^2/2$. By differentiation, the minimizing value of $t$ solves</p>
$$
c = \sigma^2 t^*
$$<p>and hence</p>
$$
t^* = \frac{c}{\sigma^2}
$$<p>So the Chernoff bound is</p>
$$
P(X - \mu \ge c) \le e^{-ct^* + \sigma^2{t^*}^2/2} = e^{-c^2/2\sigma^2}
$$<p>Compare this with the bounds we already have. Markov's bound can't be applied directly as $X - \mu$ can have negative values. Because the distribution of $X - \mu$ is symmetric about 0, Chebychev's bound becomes</p>
$$
P(X - \mu \ge c ) \le \frac{\sigma^2}{2c^2}
$$<p>When $c$ is large, the optimized Chernoff bound is quite a bit sharper than Chebychev's. In the case $\sigma = 2$, the graph below shows the exact value of $P(X - \mu \ge c)$ as a function of $c$, along with the Chernoff and Chebychev bounds.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_19/04_Chernoff_Bound_6_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chernoff-Bound-on-the-Left-Tail">Chernoff Bound on the Left Tail<a class="anchor-link" href="#Chernoff-Bound-on-the-Left-Tail"> </a></h3><p>By an analogous argument we can derive a Chernoff bound on the left tail of a distribution. For a fixed $t &gt; 0$, the function $g(x) = e^{-tx}$ is decreasing and non-negative. So for $t &gt; 0$ and any fixed $c$,</p>
$$
P(X \le c) = P(e^{-tX} \ge e^{-tc}) \le \frac{E(e^{-tX})}{e^{-tc}} = \frac{M_X(-t)}{e^{-tc}}
$$<p>and therefore</p>
$$
P(X \le c) \le \min_{t &gt; 0} \frac{M_X(-t)}{e^{-tc}}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sums-of-Independent-Random-Variables">Sums of Independent Random Variables<a class="anchor-link" href="#Sums-of-Independent-Random-Variables"> </a></h3><p>The Chernoff bound is often applied to sums of independent random variables. Let $X_1, X_2, \ldots, X_n$ be independent and let $S_n = X_1 + X_2 + \ldots + X_n$. Fix any number $c$. For every $t &gt; 0$,</p>
$$
P(S_n \ge c) \le \frac{M_{S_n}(t)}{e^{tc}} = \frac{\prod_{i=1}^n M_{X_i}(t)}{e^{tc}}
$$<p>This result is useful for finding bounds on binomial tails because the moment generating function of a Bernoulli random variable has a straightforward form. It is also used for bounding tails of sums of independent indicators with possibly different success probabilities. We will leave all this for a subsequent course.</p>

</div>
</div>
</div>
</div>

 


    </main>
    