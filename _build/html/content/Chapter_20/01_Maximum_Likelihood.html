
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>20.1. Maximum Likelihood &#8212; Data 140 Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/Chapter_20/01_Maximum_Likelihood';</script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="20.2. Independence, Revisited" href="02_Independence_Revisited.html" />
    <link rel="prev" title="20. Approaches to Estimation" href="00_Approaches_to_Estimation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/sp22_ugarte_logo.png" class="logo__image only-light" alt="Data 140 Textbook - Home"/>
    <script>document.write(`<img src="../../_static/sp22_ugarte_logo.png" class="logo__image only-dark" alt="Data 140 Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Data 140
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="http://prob140.org">Course Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../To_the_Student.html">To the  Student</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_01/00_Fundamentals.html">1. Fundamentals</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">1.1. Outcome Space and Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">1.2. Equally Likely Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">1.3. Collisions in Hashing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">1.4. The Birthday Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">1.5. An Exponential Approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/06_Exercises.html">1.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">2. Calculating Chances</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/01_Addition.html">2.1. Addition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/02_Examples.html">2.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/03_Multiplication.html">2.3. Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/04_More_Examples.html">2.4. More Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">2.5. Updating Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/06_Exercises.html">2.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_03/00_Random_Variables.html">3. Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">3.1. Functions on an Outcome Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/02_Distributions.html">3.2. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/03_Equality.html">3.3. Equality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/04_Exercises.html">3.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">4. Relations Between Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">4.1. Joint Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/02_Examples.html">4.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">4.3. Marginal Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">4.4. Conditional Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">4.5. Dependence and Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/06_Exercises.html">4.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">5. Collections of Events</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">5.1. Bounding the Chance of a Union</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">5.2. Inclusion-Exclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">5.3. The Matching Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">5.4. Sampling Without Replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/05_Exercises.html">5.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_06/00_Random_Counts.html">6. Random Counts</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">6.1. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/02_Examples.html">6.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/03_Multinomial_Distribution.html">6.3. Multinomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/04_The_Hypergeometric_Revisited.html">6.4. The Hypergeometric, Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/05_Odds_Ratios.html">6.5. Odds Ratios</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/06_Law_of_Small_Numbers.html">6.6. The Law of Small Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/07_Exercises.html">6.7. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_07/00_Poissonization.html">7. Poissonization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/01_Poisson_Distribution.html">7.1. Poisson Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Binomial.html">7.2. Poissonizing the Binomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/03_Poissonizing_the_Multinomial.html">7.3. Poissonizing the Multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/04_Exercises.html">7.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_08/00_Expectation.html">8. Expectation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/01_Definition.html">8.1. Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/02_Applying_the_Definition.html">8.2. Applying the Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/03_Expectations_of_Functions.html">8.3. Expectations of Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/04_Additivity.html">8.4. Additivity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/05_Method_of_Indicators.html">8.5. Method of Indicators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/06_Exercises.html">8.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">9. Conditioning, Revisited</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">9.1. Probability by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">9.2. Expectation by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">9.3. Expected Waiting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/04_Exercises.html">9.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">10. Markov Chains</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/01_Transitions.html">10.1. Transitions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">10.2. Deconstructing Chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">10.3. Long Run Behavior</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/04_Examples.html">10.4. Examples</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_11/00_Markov_Chain_Monte_Carlo.html">11. Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/01_Balance_and_Detailed_Balance.html">11.1. Balance and Detailed Balance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/02_Code_Breaking.html">11.2. Code Breaking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/03_Metropolis_Algorithm.html">11.3. Metropolis Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/04_Exercises.html">11.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">12. Standard Deviation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/01_Definition.html">12.1. Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">12.2. Prediction and Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/03_Bounds.html">12.3. Tail Bounds</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">12.4. Heavy Tails</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/05_Exercises.html">12.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">13. Variance Via Covariance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/01_Covariance.html">13.1. Covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/02_Properties_of_Covariance.html">13.2. Properties of Covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/03_Sums_of_Independent_Variables.html">13.3. Sums of Independent Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/04_Symmetry_and_Indicators.html">13.4. Symmetry and Indicators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/05_Finite_Population_Correction.html">13.5. Finite Population Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/06_Exercises.html">13.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">14. The Central Limit Theorem</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/01_Exact_Distribution_of_a_Sum.html">14.1. Exact Distribution of a Sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">14.2. PGFs in NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">14.3. Central Limit Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/04_SciPy_and_Normal_Curves.html">14.4. SciPy and Normal Curves</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/05_The_Sample_Mean.html">14.5. The Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/06_Confidence_Intervals.html">14.6. Confidence Intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/07_Exercises.html">14.7. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">15. Continuous Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">15.1. Density and CDF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">15.2. The Meaning of Density</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/03_Expectation.html">15.3. Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">15.4. Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">15.5. Calculus in SymPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/06_Exercises.html">15.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_16/00_Transformations.html">16. Transformations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">16.1. Linear Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">16.2. Monotone Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/03_Simulation_via_the_CDF.html">16.3. Simulation via the CDF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/04_Two_to_One_Functions.html">16.4. Two-to-One Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/05_Exercises.html">16.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">17. Joint Densities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">17.1. Probabilities and Expectations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/02_Independence.html">17.2. Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">17.3. Marginal and Conditional Densities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">17.4. Beta Densities with Integer Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/05_Exercises.html">17.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">18. The Normal and Gamma Families</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">18.1. Standard Normal: The Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">18.2. Sums of Independent Normal Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">18.3. The Gamma Family</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">18.4. Chi-Squared Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/05_Exercises.html">18.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">19. Distributions of Sums</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">19.1. The Convolution Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">19.2. Moment Generating Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">19.3. MGFs, the Normal, and the CLT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">19.4. Chernoff Bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/05_Exercises.html">19.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00_Approaches_to_Estimation.html">20. Approaches to Estimation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">20.1. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_Independence_Revisited.html">20.2. Independence, Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_Prior_and_Posterior.html">20.3. Prior and Posterior</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_Exercises.html">20.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">21. The Beta and the Binomial</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">21.1. Updating and Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">21.2. The Beta-Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">21.3. Long Run Proportion of Heads</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/04_Exercises.html">21.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_22/00_Prediction.html">22. Prediction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">22.1. Conditional Expectation As a Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/02_Least_Squares_Predictor.html">22.2. Least Squares Predictor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/03_Variance_by_Conditioning.html">22.3. Variance by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/04_Examples.html">22.4. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/05_Exercises.html">22.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">23. Jointly Normal Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">23.1. Random Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Vectors.html">23.2. Multivariate Normal Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/03_Multivariate_Normal_Density.html">23.3. Multivariate Normal Density</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/04_Independence.html">23.4. Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/05_Exercises.html">23.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">24. Simple Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/01_Linear_Least_Squares.html">24.1. Least Squares Linear Predictor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/02_Bivariate_Normal_Distribution.html">24.2. Bivariate Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">24.3. Regression and the Bivariate Normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">24.4. The Regression Equation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/05_Exercises.html">24.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_25/00_Multiple_Regression.html">25. Multiple Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/01_Bilinearity_in_Matrix_Notation.html">25.1. Bilinearity in Matrix Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/02_Best_Linear_Predictor.html">25.2. Best Linear Predictor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/03_Multivariate_Normal_Conditioning.html">25.3. Conditioning and the Multivariate Normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/04_Multiple_Regression.html">25.4. Multiple Regression</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A//github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_20/01_Maximum_Likelihood.ipynb&branch=gh-pages" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="../../_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/Chapter_20/01_Maximum_Likelihood.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Maximum Likelihood</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample">20.1.1. Maximum Likelihood Estimate of <span class="math notranslate nohighlight">\(p\)</span> Based on a Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> Sample</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample">20.1.2. MLE of <span class="math notranslate nohighlight">\(\mu\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-for-finding-the-mle">20.1.3. Steps for Finding the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-the-mle">20.1.4. Properties of the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">20.1.5. MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="maximum-likelihood">
<h1><span class="section-number">20.1. </span>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Link to this heading">#</a></h1>
<p>Suppose you have an i.i.d. sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> where the distribution of each <span class="math notranslate nohighlight">\(X_i\)</span> depends on a parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Assume that <span class="math notranslate nohighlight">\(\theta\)</span> is fixed but unknown. The method of <em>maximum likelihood</em> estimates <span class="math notranslate nohighlight">\(\theta\)</span> by answering the following question:</p>
<p><strong>Among all the possible values of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>, which one maximizes the likeihood of getting our sample?</strong></p>
<p>That maximizing value of the parameter is called the <em>maximum likelihood estimate</em> or MLE for short. In this section we will develop a method for finding MLEs.</p>
<p>Let’s look at an example to illustrate the main idea. Suppose you toss a coin that lands heads with a fixed but unknown probability <span class="math notranslate nohighlight">\(p\)</span>, and you observe the sequence HHHTHT.</p>
<p>Now suppose I propose two estimates of <span class="math notranslate nohighlight">\(p\)</span>: one estimate is <span class="math notranslate nohighlight">\(0.6\)</span>, and one estimate is <span class="math notranslate nohighlight">\(0.2\)</span>. Which would you say is better, and why?</p>
<p>Between these two, you would pick <span class="math notranslate nohighlight">\(0.6\)</span> as better, because a coin that lands heads with chance <span class="math notranslate nohighlight">\(0.6\)</span> <em>is more likely to generate the observed data</em> than a coin that lands heads with chance 0.2.</p>
<p>Your choice is based on the likelihood of the data under each of the two proposed values of <span class="math notranslate nohighlight">\(p\)</span>: the one that makes the data more likely wins.</p>
<p>Of course, <span class="math notranslate nohighlight">\(p\)</span> could be any number between in the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>. To find the best among all of these, using the criterion we have just developed, we have to find the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the function</p>
<div class="math notranslate nohighlight">
\[
p ~ \to ~ p \cdot p \cdot p \cdot (1-p) \cdot p \cdot (1-p) ~ = ~ p^4(1-p)^2
\]</div>
<p>Here is a graph of this function of <span class="math notranslate nohighlight">\(p\)</span>. Clearly, <span class="math notranslate nohighlight">\(0.6\)</span> is a better choice of estimate of <span class="math notranslate nohighlight">\(p\)</span> than <span class="math notranslate nohighlight">\(0.2\)</span>. But there’s one that’s even better.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/9fb05dfef86f2b55a7d324fe8b6a22fc5af44e1ba059d57ebd257e5ed0429e03.png" src="../../_images/9fb05dfef86f2b55a7d324fe8b6a22fc5af44e1ba059d57ebd257e5ed0429e03.png" />
</div>
</div>
<p>You can see that the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the likelihood looks suspiciously like <span class="math notranslate nohighlight">\(2/3\)</span>, the observed proportion of heads in our data HHHTHT. Let’s see why that is true.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-video" viewBox="0 0 16 16" aria-hidden="true"><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25Zm1.75-.25a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25Z"></path><path d="M6 10.559V5.442a.25.25 0 0 1 .379-.215l4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559Z"></path></svg></span><span class="sd-summary-text">See More</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><span class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/AN6y89dfNCM"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </span></p>
</div>
</details><section id="maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample">
<h2><span class="section-number">20.1.1. </span>Maximum Likelihood Estimate of <span class="math notranslate nohighlight">\(p\)</span> Based on a Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> Sample<a class="headerlink" href="#maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> sample. Our goal is to find the MLE of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>The random variables are discrete, so the likelihood function is defined as the joint probability mass function evaluated at the sample, as a function of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>In our example,</p>
<div class="math notranslate nohighlight">
\[
Lik(p) ~ = ~ p \cdot p \cdot p \cdot (1-p) \cdot p \cdot (1-p) ~ = ~ p^4(1-p)^2
\]</div>
<p>The likelihood depends on the number of 1’s, just as in the binomial probability formula. The combinatorial term is missing because we are observing each element of the sequence.</p>
<p>You’ll soon see the reason for using the strange notation <span class="math notranslate nohighlight">\(Lik\)</span>. Please just accept it for now.</p>
<p>Notice that the likelihood function depends on the data. Therefore, the value of the function is a random variable. For a general i.i.d. Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> sample, the likelihood function is calculated as follows.</p>
<p><strong>Likelihood function: Discrete Case</strong></p>
<p>Let <span class="math notranslate nohighlight">\(X = X_1 + X_2 + \ldots + X_n\)</span> be the number of 1’s in the sample. The likelihood function is</p>
<div class="math notranslate nohighlight">
\[
Lik(p) = p^X (1-p)^{n-X}
\]</div>
<p>For each <span class="math notranslate nohighlight">\(p\)</span>, the value of <span class="math notranslate nohighlight">\(Lik(p)\)</span> is the likelihood of the data if <span class="math notranslate nohighlight">\(p\)</span> is the probability of heads.</p>
<p>Our goal is to find the value <span class="math notranslate nohighlight">\(\hat{p}\)</span> that maximizes this likelihood over all the possible values of <span class="math notranslate nohighlight">\(p\)</span>, that is, over the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>.</p>
<p>One way to do this is by calculus. To make the calculus simpler, we recall a crucial observation we have made before:</p>
<p>Taking the <span class="math notranslate nohighlight">\(\log\)</span> turns the product into a sum, which simplifies calculation. Also, <span class="math notranslate nohighlight">\(\log\)</span> is an increasing function. Hence <strong>the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the likelihood function is the same as the value of <span class="math notranslate nohighlight">\(p\)</span> that maximizes the log of the likelihood function.</strong></p>
<p><strong>Log-likelihood function</strong></p>
<p>Let <span class="math notranslate nohighlight">\(L\)</span> be the log of the likelihood function, also known as the <em>log likelihood function</em>. You can see the letter l appearing repeatedly in the terminology. Since we’ll be doing most of our work with the log likelihood function, we are calling it <span class="math notranslate nohighlight">\(L\)</span> and using <span class="math notranslate nohighlight">\(Lik\)</span> for the likelihood function.</p>
<div class="math notranslate nohighlight">
\[
L(p) = X\log(p) + (n-X)\log(1-p)
\]</div>
<p>The function <span class="math notranslate nohighlight">\(L\)</span> is easier to work with than <span class="math notranslate nohighlight">\(Lik\)</span>. We just have to carry out the calculus.</p>
<p><strong>Differentiate the log-likelihood function with respect to <span class="math notranslate nohighlight">\(p\)</span>:</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dp} L(p) = \frac{X}{p} - \frac{n-X}{1-p}
\]</div>
<p>The <em>maximum likelihood estimate</em> (MLE) of <span class="math notranslate nohighlight">\(p\)</span> is the value <span class="math notranslate nohighlight">\(\hat{p}\)</span> that maximizes the log-likelihood <span class="math notranslate nohighlight">\(L\)</span>. Statisticians have long used the “hat” symbol to denote estimates.</p>
<p><strong>Set the derivative equal to 0 and solve for the MLE</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{X}{\hat{p}} - \frac{n-X}{1-\hat{p}} = 0
\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
(1-\hat{p})X = (n-X)\hat{p} ~~~~~ \text{so} ~~~~~ X = n\hat{p}
\]</div>
<p>Therefore the MLE of <span class="math notranslate nohighlight">\(p\)</span> is</p>
<div class="math notranslate nohighlight">
\[ 
\hat{p} = \frac{X}{n} = \frac{1}{n}\sum_{i=1}^n X_i
\]</div>
<p>That is, the MLE of <span class="math notranslate nohighlight">\(p\)</span> is the sample proportion of 1’s. To compute this estimate, all you need is the number of 1’s in the sample. You don’t need to see the entire sample as a sequence of 0’s and 1’s.</p>
<p>Because the MLE <span class="math notranslate nohighlight">\(\hat{p}\)</span> is the sample proportion, it is unbiased, has SD <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n}\)</span>, and is asymptotically normal. When <span class="math notranslate nohighlight">\(n\)</span> is large you can estimate the SD based on the sample and therefore construct confidence intervals for <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>To be very careful, we should check that this calculation yields a maximum and not a minimum, but given the answer you will surely accept that it’s a max. You are welcome to take the second derivative of <span class="math notranslate nohighlight">\(L\)</span> and check that we do indeed have a maximum.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-video" viewBox="0 0 16 16" aria-hidden="true"><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25Zm1.75-.25a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25Z"></path><path d="M6 10.559V5.442a.25.25 0 0 1 .379-.215l4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559Z"></path></svg></span><span class="sd-summary-text">See More</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><span class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/KPVK4t58zPY"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </span></p>
</div>
</details></section>
<section id="mle-of-mu-based-on-a-normal-mu-sigma-2-sample">
<h2><span class="section-number">20.1.2. </span>MLE of <span class="math notranslate nohighlight">\(\mu\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample<a class="headerlink" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample. The sample mean is a pretty good estimate of <span class="math notranslate nohighlight">\(\mu\)</span>, as you know. In this example we will show that it is the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>What if you want to estimate <span class="math notranslate nohighlight">\(\sigma\)</span> as well? We will tackle that problem at the end of this section. For now, let’s just estimate <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>Likelihood Function: Density Case</strong></p>
<p>In the density case, the likelihood function is defined as the joint density of the sample evaluated at the observed values, considered as a function of the parameter. That’s a bit of a mouthful but it becomes clear once you see the calculation. The joint density in this example is the product of <span class="math notranslate nohighlight">\(n\)</span> normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> density functions, and hence the likelihood function is</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu) ~ = ~ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \big{(} -\frac{1}{2} \big{(} \frac{X_i - \mu}{\sigma} \big{)}^2 \big{)}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(Lik(\mu)\)</span> is called the likelihood of the data <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> when the mean of the underlying normal distribution is <span class="math notranslate nohighlight">\(\mu\)</span>. For every fixed <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(Lik(\mu)\)</span> is a function of the sample and hence is a random variable.</p>
<p>The goal is to find the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes this likelihood function over all the possible values that <span class="math notranslate nohighlight">\(\mu\)</span> could be. We don’t yet know if such a maximizing value exists, but let’s try to find it anyway.</p>
<p>To do this we will simplify the likelihood function as much as possible.</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu) ~ = ~ \big{(} \frac{1}{\sqrt{2\pi}\sigma} \big{)}^n
\exp \big{(} -\frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2 \big{)}
~ = ~ C \exp \big{(} -\frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2 \big{)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\mu\)</span> and thus won’t affect the maximization.</p>
<p>Even in this simplified form, the likelihood function looks difficult to maximize. But as it is a product, we can simplify our calculations still further by taking its log as we did in the binomial example.</p>
<p>The log-likelihood function is</p>
<div class="math notranslate nohighlight">
\[
L(\mu) ~ = ~ \log(C) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\log(C)\)</span> doesn’t affect the maximization, and nor does <span class="math notranslate nohighlight">\(\sigma\)</span>, we have defined a function to calculate <span class="math notranslate nohighlight">\(L - \log(C)\)</span> for the sample 52.8, 51.1, 54.2, and 52.5 drawn from the normal <span class="math notranslate nohighlight">\((\mu, 1)\)</span> distribution. Remember that we began this section by comparing 32 and 52 as estimates of <span class="math notranslate nohighlight">\(\mu\)</span>, based on this sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">(</span><span class="mf">52.8</span><span class="p">,</span> <span class="mf">51.1</span><span class="p">,</span> <span class="mf">54.2</span><span class="p">,</span> <span class="mf">52.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shifted_log_lik</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">((</span><span class="n">sample</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a graph of the function for <span class="math notranslate nohighlight">\(\mu\)</span> in the interval <span class="math notranslate nohighlight">\((30, 70)\)</span>.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/f4b9a0856fd8d65594fa279f2a4a26858361401e8a1108a2b44ca032ed7fde58.png" src="../../_images/f4b9a0856fd8d65594fa279f2a4a26858361401e8a1108a2b44ca032ed7fde58.png" />
</div>
</div>
<p>The maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> is somewhere around <span class="math notranslate nohighlight">\(52.5\)</span>. To find exactly where it is, we have to complete the maximizatin.</p>
<p>Find the derivative of <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(\mu\)</span>. Use the Chain Rule and be careful about negative signs.</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\mu} L(\mu) ~ = ~ \frac{2}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)
\]</div>
<p>Now set this equal to <span class="math notranslate nohighlight">\(0\)</span> and solve. Let <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> be the MLE of <span class="math notranslate nohighlight">\(\mu\)</span>. Then <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> satisfies the following equation.</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n (X_i - \hat{\mu}) ~ = ~ 0 ~~~~~~ \Longleftrightarrow ~~~~~~ \sum_{i=1}^n X_i ~ = ~ n\hat{\mu} ~~~~~~ \Longleftrightarrow ~~~~~~ \hat{\mu} ~ = ~ \frac{1}{n} \sum_{i=1}^n X_i ~ = ~ \bar{X}
\]</div>
<p>Once again we should check that this is a max and not a min, but at this point you will surely be convinced that it is a max.</p>
<p>We have shown that the MLE of <span class="math notranslate nohighlight">\(\mu\)</span> is the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span>, regardless of the population SD <span class="math notranslate nohighlight">\(\sigma\)</span>. In the case of the sample we used for the plot above, <span class="math notranslate nohighlight">\(\bar{X} = 52.65\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>52.650000000000006
</pre></div>
</div>
</div>
</div>
<p>You know that the distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is normal with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. If you don’t know <span class="math notranslate nohighlight">\(\sigma\)</span>, then if the sample is large you can estimate <span class="math notranslate nohighlight">\(\sigma\)</span> by the SD of the sample and hence construct confidence intervals for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</section>
<section id="steps-for-finding-the-mle">
<h2><span class="section-number">20.1.3. </span>Steps for Finding the MLE<a class="headerlink" href="#steps-for-finding-the-mle" title="Link to this heading">#</a></h2>
<p>Let’s capture our sequence of steps in an algorithm to find the MLE of a parameter given an i.i.d. sample. See the <strong>Computational Notes</strong> at the end of this section for other ways of finding the MLE.</p>
<ul class="simple">
<li><p>Write the likelihood of the sample. The goal is to find the value of the parameter that maximizes this likelihood.</p></li>
<li><p>To make the maximization easier, take the log of the likelihood function.</p></li>
<li><p>To maximize the log likelihood with respect to the parameter, take its derivative with respect to the parameter.</p></li>
<li><p>Set the derivative equal to 0 and solve; the solution is the MLE.</p></li>
</ul>
</section>
<section id="properties-of-the-mle">
<h2><span class="section-number">20.1.4. </span>Properties of the MLE<a class="headerlink" href="#properties-of-the-mle" title="Link to this heading">#</a></h2>
<p>In the two examples above, the MLE is unbiased and either exactly normal or asymptotically normal. In general, MLEs need not be unbiased, as you will see in an example below. However, under some regularity conditions on the underlying probability distribution or mass function, when the sample is large the MLE is:</p>
<ul class="simple">
<li><p><em>consistent</em>, that is, likely to be close to the parameter</p></li>
<li><p>roughly normal and almost unbiased</p></li>
</ul>
<p>Establishing this is outside the scope of this class, but in exercises you will observe these properties through simulation.</p>
<p>Though there is beautiful theory about the asymptotic variance of the MLE, in practice it can be hard to estimate the variance analytically. This can make it hard to find formulas for confidence intervals. However, you can use the bootstrap to estimate the variance: each bootstrapped sample yields a value of the MLE, and you can construct confidence intervals based on the empirical distribution of the bootstrapped MLEs.</p>
</section>
<section id="mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">
<h2><span class="section-number">20.1.5. </span>MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample<a class="headerlink" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample. We will now find the MLEs of both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p><strong>Likelihood Function</strong></p>
<p>We have to think of this as a function of both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu, \sigma) ~ = ~ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \big{(} -\frac{1}{2} \big{(} \frac{X_i - \mu}{\sigma} \big{)}^2 \big{)} ~ = ~
C \cdot \frac{1}{\sigma^n} \prod_{i=1}^n \exp \big{(} -\frac{1}{2\sigma^2} (X_i - \mu)^2 \big{)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C = 1/(\sqrt{2\pi})^n\)</span> does not affect the maximization.</p>
<p><strong>Log-Likelihood Function</strong></p>
<div class="math notranslate nohighlight">
\[
L(\mu, \sigma) ~ = ~ \log(C) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
\]</div>
<p><strong>Maximizing the Log Likelihood Function</strong></p>
<p>We will maximize <span class="math notranslate nohighlight">\(L\)</span> in two stages:</p>
<ul class="simple">
<li><p>First fix <span class="math notranslate nohighlight">\(\sigma\)</span> and maximize with respect to <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Then plug in the maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> and maximize the resulting function with respect to <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
<p>We have already completed the first stage in the first example of this section. For each fixed <span class="math notranslate nohighlight">\(\sigma\)</span>, the maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> is <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span>.</p>
<p>So now our job is to find the value <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> that maximizes the new function</p>
<div class="math notranslate nohighlight">
\[
L^*(\sigma) ~ = ~ -n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \bar{X})^2 ~ = ~ -n\log(\sigma) - \frac{1}{2\sigma^2} V
\]</div>
<p>where <span class="math notranslate nohighlight">\(V = \sum_{i=1}^n (X_i - \bar{X})^2\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\sigma\)</span>. Differentiate with respect to <span class="math notranslate nohighlight">\(\sigma\)</span>; keep track of minus signs and factors of 2.</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\sigma} L^*(\sigma) ~ = ~ -\frac{n}{\sigma} + \frac{1}{\sigma^3}V
\]</div>
<p>Set this equal to 0 and solve for the maximizing value <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
-\frac{n}{\hat{\sigma}} + \frac{1}{\hat{\sigma}^3}V ~ = ~ 0 
~~~~~~~ \Longleftrightarrow ~~~~~~~ \hat{\sigma}^2 ~ = ~ \frac{V}{n} ~ = ~ 
\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\]</div>
<p>Again you should check that this yields a maximum and not a minimum, but again given the answer you will surely accept that it’s a max.</p>
<p>You have shown in exercises that <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is <em>not</em> an unbiased estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>. You have also shown that it is close to unbiased when <span class="math notranslate nohighlight">\(n\)</span> is large.</p>
<p>To summarize our result, if <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> is an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample, then the MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma} = \sqrt{\hat{\sigma}^2}\)</span> where <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2\)</span></p></li>
</ul>
<p>It is a remarkable fact about i.i.d. normal samples that <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> are independent of each other even though they are statistics calculated from the same sample. Later in this course you will see why.</p>
<p><strong>Computational Notes</strong></p>
<ul class="simple">
<li><p>The goal is to find the value of the parameter that maximizes the likelihood. Sometimes, you can do that without any calculus, just by observing properties of the likelihood function. See the Exercises.</p></li>
<li><p>MLEs can’t always be derived analytically as easily as in our examples. It’s important to keep in mind that maximizing log likelihood functions can often be intractable without a numerical optimization method.</p></li>
<li><p>Not all likelihood functions have unique maxima.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/Chapter_20"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="00_Approaches_to_Estimation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">20. </span>Approaches to Estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="02_Independence_Revisited.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">20.2. </span>Independence, Revisited</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample">20.1.1. Maximum Likelihood Estimate of <span class="math notranslate nohighlight">\(p\)</span> Based on a Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> Sample</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample">20.1.2. MLE of <span class="math notranslate nohighlight">\(\mu\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-for-finding-the-mle">20.1.3. Steps for Finding the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-the-mle">20.1.4. Properties of the MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">20.1.5. MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ani Adhikari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>