
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8.4. Additivity &#8212; Data 140 Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/Chapter_08/04_Additivity';</script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8.5. Method of Indicators" href="05_Method_of_Indicators.html" />
    <link rel="prev" title="8.3. Expectations of Functions" href="03_Expectations_of_Functions.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/sp22_ugarte_logo.png" class="logo__image only-light" alt="Data 140 Textbook - Home"/>
    <script>document.write(`<img src="../../_static/sp22_ugarte_logo.png" class="logo__image only-dark" alt="Data 140 Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Data 140
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="http://prob140.org">Course Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../To_the_Student.html">To the  Student</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_01/00_Fundamentals.html">1. Fundamentals</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">1.1. Outcome Space and Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">1.2. Equally Likely Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">1.3. Collisions in Hashing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">1.4. The Birthday Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">1.5. An Exponential Approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/06_Exercises.html">1.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">2. Calculating Chances</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/01_Addition.html">2.1. Addition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/02_Examples.html">2.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/03_Multiplication.html">2.3. Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/04_More_Examples.html">2.4. More Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">2.5. Updating Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/06_Exercises.html">2.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_03/00_Random_Variables.html">3. Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">3.1. Functions on an Outcome Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/02_Distributions.html">3.2. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/03_Equality.html">3.3. Equality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/04_Exercises.html">3.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">4. Relations Between Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">4.1. Joint Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/02_Examples.html">4.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">4.3. Marginal Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">4.4. Conditional Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">4.5. Dependence and Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/06_Exercises.html">4.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">5. Collections of Events</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">5.1. Bounding the Chance of a Union</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">5.2. Inclusion-Exclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">5.3. The Matching Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">5.4. Sampling Without Replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_05/05_Exercises.html">5.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_06/00_Random_Counts.html">6. Random Counts</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">6.1. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/02_Examples.html">6.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/03_Multinomial_Distribution.html">6.3. Multinomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/04_The_Hypergeometric_Revisited.html">6.4. The Hypergeometric, Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/05_Odds_Ratios.html">6.5. Odds Ratios</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/06_Law_of_Small_Numbers.html">6.6. The Law of Small Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/07_Exercises.html">6.7. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_07/00_Poissonization.html">7. Poissonization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/01_Poisson_Distribution.html">7.1. Poisson Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Binomial.html">7.2. Poissonizing the Binomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/03_Poissonizing_the_Multinomial.html">7.3. Poissonizing the Multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/04_Exercises.html">7.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00_Expectation.html">8. Expectation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_Definition.html">8.1. Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_Applying_the_Definition.html">8.2. Applying the Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_Expectations_of_Functions.html">8.3. Expectations of Functions</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">8.4. Additivity</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_Method_of_Indicators.html">8.5. Method of Indicators</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_Exercises.html">8.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">9. Conditioning, Revisited</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">9.1. Probability by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">9.2. Expectation by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">9.3. Expected Waiting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/04_Exercises.html">9.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">10. Markov Chains</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/01_Transitions.html">10.1. Transitions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">10.2. Deconstructing Chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">10.3. Long Run Behavior</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/04_Examples.html">10.4. Examples</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_11/00_Markov_Chain_Monte_Carlo.html">11. Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/01_Balance_and_Detailed_Balance.html">11.1. Balance and Detailed Balance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/02_Code_Breaking.html">11.2. Code Breaking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/03_Metropolis_Algorithm.html">11.3. Metropolis Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/04_Exercises.html">11.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">12. Standard Deviation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/01_Definition.html">12.1. Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">12.2. Prediction and Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/03_Bounds.html">12.3. Tail Bounds</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">12.4. Heavy Tails</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/05_Exercises.html">12.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">13. Variance Via Covariance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/01_Covariance.html">13.1. Covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/02_Properties_of_Covariance.html">13.2. Properties of Covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/03_Sums_of_Independent_Variables.html">13.3. Sums of Independent Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/04_Symmetry_and_Indicators.html">13.4. Symmetry and Indicators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/05_Finite_Population_Correction.html">13.5. Finite Population Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_13/06_Exercises.html">13.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">14. The Central Limit Theorem</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/01_Exact_Distribution_of_a_Sum.html">14.1. Exact Distribution of a Sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">14.2. PGFs in NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">14.3. Central Limit Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/04_SciPy_and_Normal_Curves.html">14.4. SciPy and Normal Curves</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/05_The_Sample_Mean.html">14.5. The Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/06_Confidence_Intervals.html">14.6. Confidence Intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_14/07_Exercises.html">14.7. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">15. Continuous Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">15.1. Density and CDF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">15.2. The Meaning of Density</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/03_Expectation.html">15.3. Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">15.4. Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">15.5. Calculus in SymPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_15/06_Exercises.html">15.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_16/00_Transformations.html">16. Transformations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">16.1. Linear Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">16.2. Monotone Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/03_Simulation_via_the_CDF.html">16.3. Simulation via the CDF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/04_Two_to_One_Functions.html">16.4. Two-to-One Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_16/05_Exercises.html">16.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">17. Joint Densities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">17.1. Probabilities and Expectations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/02_Independence.html">17.2. Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">17.3. Marginal and Conditional Densities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">17.4. Beta Densities with Integer Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_17/05_Exercises.html">17.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">18. The Normal and Gamma Families</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">18.1. Standard Normal: The Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">18.2. Sums of Independent Normal Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">18.3. The Gamma Family</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">18.4. Chi-Squared Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_18/05_Exercises.html">18.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">19. Distributions of Sums</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">19.1. The Convolution Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">19.2. Moment Generating Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">19.3. MGFs, the Normal, and the CLT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">19.4. Chernoff Bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_19/05_Exercises.html">19.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_20/00_Approaches_to_Estimation.html">20. Approaches to Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_20/01_Maximum_Likelihood.html">20.1. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_20/02_Independence_Revisited.html">20.2. Independence, Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_20/03_Prior_and_Posterior.html">20.3. Prior and Posterior</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_20/04_Exercises.html">20.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">21. The Beta and the Binomial</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">21.1. Updating and Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">21.2. The Beta-Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">21.3. Long Run Proportion of Heads</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_21/04_Exercises.html">21.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_22/00_Prediction.html">22. Prediction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">22.1. Conditional Expectation As a Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/02_Least_Squares_Predictor.html">22.2. Least Squares Predictor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/03_Variance_by_Conditioning.html">22.3. Variance by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/04_Examples.html">22.4. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_22/05_Exercises.html">22.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">23. Jointly Normal Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">23.1. Random Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Vectors.html">23.2. Multivariate Normal Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/03_Multivariate_Normal_Density.html">23.3. Multivariate Normal Density</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/04_Independence.html">23.4. Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_23/05_Exercises.html">23.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">24. Simple Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/01_Linear_Least_Squares.html">24.1. Least Squares Linear Predictor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/02_Bivariate_Normal_Distribution.html">24.2. Bivariate Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">24.3. Regression and the Bivariate Normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">24.4. The Regression Equation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_24/05_Exercises.html">24.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_25/00_Multiple_Regression.html">25. Multiple Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/01_Bilinearity_in_Matrix_Notation.html">25.1. Bilinearity in Matrix Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/02_Best_Linear_Predictor.html">25.2. Best Linear Predictor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/03_Multivariate_Normal_Conditioning.html">25.3. Conditioning and the Multivariate Normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_25/04_Multiple_Regression.html">25.4. Multiple Regression</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A//github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_08/04_Additivity.ipynb&branch=gh-pages" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="../../_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/Chapter_08/04_Additivity.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Additivity</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additivity-of-expectation">8.4.1. Additivity of Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-x-2-for-a-poisson-variable-x">8.4.2. <span class="math notranslate nohighlight">\(E(X^2)\)</span> for a Poisson Variable <span class="math notranslate nohighlight">\(X\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-sum">8.4.3. Sample Sum</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-estimator">8.4.4. Unbiased Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-estimators-of-a-population-mean">8.4.5. Unbiased Estimators of a Population Mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-unbiased-estimator-of-a-maximum-possible-value">8.4.6. First Unbiased Estimator of a Maximum Possible Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#second-unbiased-estimator-of-the-maximum-possible-value">8.4.7. Second Unbiased Estimator of the Maximum Possible Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-estimator-to-use">8.4.8. Which Estimator to Use?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="additivity">
<h1><span class="section-number">8.4. </span>Additivity<a class="headerlink" href="#additivity" title="Link to this heading">#</a></h1>
<p>Calculating expectation by plugging into the definition works in simple cases, but often it can be cumbersome or lack insight. The most powerful result for calculating expectation turns out not to be the definition. It looks rather innocuous:</p>
<section id="additivity-of-expectation">
<h2><span class="section-number">8.4.1. </span>Additivity of Expectation<a class="headerlink" href="#additivity-of-expectation" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables defined on the same probability space. Then</p>
<div class="math notranslate nohighlight">
\[
E(X+Y) = E(X) + E(Y)
\]</div>
<p>Before we look more closely at this result, note that we are assuming that all the expectations exist; we will do this throughout in this course.</p>
<p>And now note that <strong>there are no assumptions about the relation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></strong>. They could be dependent or independent. Regardless, the expectation of the sum is the sum of the expectations. This makes the result powerful.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-video" viewBox="0 0 16 16" aria-hidden="true"><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25Zm1.75-.25a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25Z"></path><path d="M6 10.559V5.442a.25.25 0 0 1 .379-.215l4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559Z"></path></svg></span><span class="sd-summary-text">See More</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><span class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/HzZEhM4NHUQ"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </span></p>
</div>
</details><p>Additivity follows easily from the definition of <span class="math notranslate nohighlight">\(X+Y\)</span> and the definition of expectation on the domain space. First note that the random variable <span class="math notranslate nohighlight">\(X+Y\)</span> is the function defined by</p>
<div class="math notranslate nohighlight">
\[
(X+Y)(\omega) = X(\omega) + Y(\omega) ~~~~ \text{for all }
\omega \in \Omega
\]</div>
<p>Thus a “value of <span class="math notranslate nohighlight">\(X+Y\)</span> weighted by the probability” can be written as</p>
<div class="math notranslate nohighlight">
\[
(X+Y)(\omega) \cdot P(\omega) = X(\omega)P(\omega) + 
Y(\omega)P(\omega )
\]</div>
<p>Sum the two sides over all <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> to prove additivty of expecation.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables on the same space, with <span class="math notranslate nohighlight">\(E(X) = 5\)</span> and <span class="math notranslate nohighlight">\(E(Y) = 3\)</span>.</p>
<p>(a) Find <span class="math notranslate nohighlight">\(E(X-Y)\)</span>.</p>
<p>(b) Find <span class="math notranslate nohighlight">\(E(2X-8Y+7)\)</span>.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>(a) <span class="math notranslate nohighlight">\(2\)</span> because <span class="math notranslate nohighlight">\(X-Y = X+(-Y)\)</span></p>
<p>(b) <span class="math notranslate nohighlight">\(-7\)</span></p>
</div>
<p>By induction, additivity extends to any finite number of random variables. If <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> are random variables defined on the same probability space, then</p>
<div class="math notranslate nohighlight">
\[
E(X_1 + X_2 + \cdots + X_n) = E(X_1) + E(X_2) + \cdots + E(X_n)
\]</div>
<p>regardless of the dependence structure of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>.</p>
<p>If you are trying to find an expectation, then the way to use additivity is to write your random variable as a sum of simpler variables whose expectations you know or can calculate easily.</p>
</section>
<section id="e-x-2-for-a-poisson-variable-x">
<h2><span class="section-number">8.4.2. </span><span class="math notranslate nohighlight">\(E(X^2)\)</span> for a Poisson Variable <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#e-x-2-for-a-poisson-variable-x" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> have the Poisson <span class="math notranslate nohighlight">\(\mu\)</span> distribution. In earlier sections we showed that <span class="math notranslate nohighlight">\(E(X) = \mu\)</span> and <span class="math notranslate nohighlight">\(E(X(X-1)) = \mu^2\)</span>.</p>
<p>Now <span class="math notranslate nohighlight">\(X^2 = X(X-1) + X\)</span>. The random variables <span class="math notranslate nohighlight">\(X(X-1)\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are both functions of <span class="math notranslate nohighlight">\(X\)</span>, so they are not independent of each other. But additivity of expectation doesn’t require independence, so we can use it to see that</p>
<div class="math notranslate nohighlight">
\[
E(X^2) ~ = ~ E(X(X-1)) + E(X) ~ = ~ \mu^2 + \mu
\]</div>
<p>We will use this fact later when we study the variability of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>It is worth noting that it is not easy to calculate <span class="math notranslate nohighlight">\(E(X^2)\)</span> directly, since</p>
<div class="math notranslate nohighlight">
\[
E(X^2) ~ = ~ \sum_{k=0}^\infty k^2 e^{-\mu}\frac{\mu^k}{k!}
\]</div>
<p>is not an easy sum to simplify.</p>
</section>
<section id="sample-sum">
<h2><span class="section-number">8.4.3. </span>Sample Sum<a class="headerlink" href="#sample-sum" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be a sample drawn at random from a numerical population that has mean <span class="math notranslate nohighlight">\(\mu\)</span>, and let the sample sum be</p>
<div class="math notranslate nohighlight">
\[
S_n = X_1 + X_2 + \cdots + X_n
\]</div>
<p>Then, regardless of whether the sample was drawn with or without replacement, each <span class="math notranslate nohighlight">\(X_i\)</span> has the same distribution as the population. This is clearly true if the sampling is with replacement, and it is true by symmetry if the sampling is without replacement as we saw in an earlier chapter.</p>
<p>So, regardless of whether the sample is drawn with or without replacement, <span class="math notranslate nohighlight">\(E(X_i) = \mu\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>, and hence</p>
<div class="math notranslate nohighlight">
\[
E(S_n) = E(X_1) + E(X_2) + \cdots + E(X_n) = n\mu
\]</div>
<p>We can use this to estimate a population mean based on a sample mean.</p>
</section>
<section id="unbiased-estimator">
<h2><span class="section-number">8.4.4. </span>Unbiased Estimator<a class="headerlink" href="#unbiased-estimator" title="Link to this heading">#</a></h2>
<p>Suppose a random variable <span class="math notranslate nohighlight">\(X\)</span> is being used to estimate a fixed numerical parameter <span class="math notranslate nohighlight">\(\theta\)</span>. Then <span class="math notranslate nohighlight">\(X\)</span> is called an <em>estimator</em> of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>The <em>bias</em> of <span class="math notranslate nohighlight">\(X\)</span> is the difference <span class="math notranslate nohighlight">\(E(X) - \theta\)</span>. The bias measures the amount by which the estimator exceeds the parameter, on average. The bias can be negative if the estimator tends to underestimate the parameter.</p>
<p>If the bias of an estimator is <span class="math notranslate nohighlight">\(0\)</span> then the estimator is called <em>unbiased</em>. So <span class="math notranslate nohighlight">\(X\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\theta\)</span> if <span class="math notranslate nohighlight">\(E(X) = \theta\)</span>.</p>
<p>If an estimator is unbiased, and you use it to generate estimates repeatedly and independently, then in the long run the average of all the estimates is equal to the parameter being estimated. On average, the unbiased estimator is neither higher nor lower than the parameter. That’s usually considered a good quality in an estimator.</p>
<p>In practical terms, if a data scientist wants to estimate an unknown parameter based on a random sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>, the data scientist has to come up with a <em>statistic</em> to use as the estimator.</p>
<p>Recall from Data 8 that a statistic is a number computed from the sample. In other words, a statistic is a numerical function of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span>.</p>
<p>Constructing an unbiased estimator of a parameter <span class="math notranslate nohighlight">\(\theta\)</span> therefore amounts to finding a statistic <span class="math notranslate nohighlight">\(T = g(X_1, X_2, \ldots, X_n)\)</span> for a function <span class="math notranslate nohighlight">\(g\)</span> such that <span class="math notranslate nohighlight">\(E(T) = \theta\)</span>.</p>
</section>
<section id="unbiased-estimators-of-a-population-mean">
<h2><span class="section-number">8.4.5. </span>Unbiased Estimators of a Population Mean<a class="headerlink" href="#unbiased-estimators-of-a-population-mean" title="Link to this heading">#</a></h2>
<p>As in the sample sum example above, let <span class="math notranslate nohighlight">\(S_n\)</span> be the sum of a sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> drawn at random from a population that has mean <span class="math notranslate nohighlight">\(\mu\)</span>. The standard statistical notation for the average of <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> is <span class="math notranslate nohighlight">\(\bar{X}_n\)</span>. So</p>
<div class="math notranslate nohighlight">
\[
\bar{X}_n = \frac{S_n}{n}
\]</div>
<p>Then, regardless of whether the draws were made with replacement or without,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E(\bar{X}_n) &amp;= \frac{E(S_n)}{n} ~~~~ \text{(linear function rule)} \\
&amp;= \frac{n \mu}{n} ~~~~~~~~~ \text{(} E(S_n) = n\mu \text{)} \\
&amp;= \mu
\end{align*}
\end{split}\]</div>
<p>Thus the sample mean is an unbiased estimator of the population mean.</p>
<p>It is worth noting that <span class="math notranslate nohighlight">\(X_1\)</span> is also an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>, since <span class="math notranslate nohighlight">\(E(X_1) = \mu\)</span>. So is <span class="math notranslate nohighlight">\(X_j\)</span> for any <span class="math notranslate nohighlight">\(j\)</span>, also <span class="math notranslate nohighlight">\((X_1 + X_9)/2\)</span>, or any linear combination of the sample if the coefficients add up to 1.</p>
<p>But it seems clear that using the sample mean as the estimator is better than using just one sampled element, even though both are unbiased. This is true, and is related to how variable the estimators are. We will address this later in the course.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span> be i.i.d. Poisson <span class="math notranslate nohighlight">\((\mu)\)</span> random variables, and suppose the value of <span class="math notranslate nohighlight">\(\mu\)</span> is unknown. Is <span class="math notranslate nohighlight">\(0.4X_1 + 0.2X_2 + 0.4X_3\)</span> an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Yes</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-video" viewBox="0 0 16 16" aria-hidden="true"><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25Zm1.75-.25a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25Z"></path><path d="M6 10.559V5.442a.25.25 0 0 1 .379-.215l4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559Z"></path></svg></span><span class="sd-summary-text">See More</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><span class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/ruEpGZJwHmw"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </span></p>
</div>
</details></section>
<section id="first-unbiased-estimator-of-a-maximum-possible-value">
<h2><span class="section-number">8.4.6. </span>First Unbiased Estimator of a Maximum Possible Value<a class="headerlink" href="#first-unbiased-estimator-of-a-maximum-possible-value" title="Link to this heading">#</a></h2>
<p>Suppose we have a sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> drawn at random from <span class="math notranslate nohighlight">\(1, 2, \ldots , N\)</span> for some fixed <span class="math notranslate nohighlight">\(N\)</span>, and we are trying to estimate <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>How can we use the sample to construct an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>? By definition, such an estimator must be a function of the sample and its expectation must be <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>In other words, we have to construct a statistic that has expectation <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Each <span class="math notranslate nohighlight">\(X_i\)</span> has the uniform distribution on <span class="math notranslate nohighlight">\(1, 2, \ldots , N\)</span>. This is true for sampling with replacement as well as for simple random sampling, by symmetry.</p>
<p>The expectation of each of the uniform variables is <span class="math notranslate nohighlight">\((N+1)/2\)</span>, as we have seen earlier. So if <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> is the sample mean, then</p>
<div class="math notranslate nohighlight">
\[
E(\bar{X}_n) = \frac{N+1}{2}
\]</div>
<p>Clearly, <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> is not an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>. That’s not surprising because <span class="math notranslate nohighlight">\(N\)</span> is the maximum possible value of each observation and <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> should be somewhere in the middle of all the possible values.</p>
<p>But because <span class="math notranslate nohighlight">\(E(\bar{X}_n)\)</span> is a linear function of <span class="math notranslate nohighlight">\(N\)</span>, we can figure out how to create an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Remember that our job is to create a function of the sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> in such a way that the expectation of that function is <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Start by inverting the linear function, that is, by isolating <span class="math notranslate nohighlight">\(N\)</span> in the equation above.</p>
<div class="math notranslate nohighlight">
\[
2E(\bar{X}_n) - 1 =  N
\]</div>
<p>This tells us what we have to do to the sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> to get an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>We should just use the statistic <span class="math notranslate nohighlight">\(T_1 = 2\bar{X}_n - 1\)</span> as the estimator. It is unbiased because <span class="math notranslate nohighlight">\(E(T_1) = N\)</span> by the calculation above.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>In the setting above, what is the bias of <span class="math notranslate nohighlight">\(2\bar{X}_n\)</span> as an estimator of <span class="math notranslate nohighlight">\(N\)</span>? Does it tend to overestimate on average, or underestimate?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(1\)</span>; overestimate</p>
</div>
</section>
<section id="second-unbiased-estimator-of-the-maximum-possible-value">
<h2><span class="section-number">8.4.7. </span>Second Unbiased Estimator of the Maximum Possible Value<a class="headerlink" href="#second-unbiased-estimator-of-the-maximum-possible-value" title="Link to this heading">#</a></h2>
<p>The calculation above stems from a problem the Allied forces faced in World War II. Germany had a seemingly never-ending fleet of Panzer tanks, and the Allies needed to estimate how many they had. They decided to base their estimates on the serial numbers of the tanks that they saw.</p>
<p>Here is a picture of one from <a class="reference external" href="https://en.wikipedia.org/wiki/Panzer_IV">Wikipedia</a>.</p>
<p><img alt="Panzer Tank" src="../../_images/panzer.png" /></p>
<p>Notice the serial number on the top left. When tanks were disabled or destroyed, it was discovered that their parts had serial numbers too. The ones from the gear boxes proved very useful.</p>
<p>The idea was to model the observed serial numbers as random draws from <span class="math notranslate nohighlight">\(1, 2, \ldots, N\)</span> and then estimate <span class="math notranslate nohighlight">\(N\)</span>. This is of course a very simplified model of reality. But estimates based on even such simple probabilistic models proved to be quite a bit <a class="reference external" href="https://en.wikipedia.org/wiki/German_tank_problem#Specific_data">more accurate</a> than those based on the intelligence gathered by the Allies. For example, in August 1942, intelligence estimates were that Germany was producing 1,550 tanks per month. The prediction based on the probability model was 327 per month. After the war, German records showed that the actual production rate was 342 per month.</p>
<p>The model was that the draws were made at random without replacement from the integers 1 through <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>In the example above, we constructed the random variable <span class="math notranslate nohighlight">\(T\)</span> to be an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span> under this model.</p>
<p>The Allied statisticians instead started with <span class="math notranslate nohighlight">\(M\)</span>, the sample maximum:</p>
<div class="math notranslate nohighlight">
\[
M ~ = ~ \max\{X_1, X_2, \ldots, X_n\}
\]</div>
<p>The sample maximum <span class="math notranslate nohighlight">\(M\)</span> is a biased estimator of <span class="math notranslate nohighlight">\(N\)</span>, because we know that its value is always less than or equal to <span class="math notranslate nohighlight">\(N\)</span>. Its average value therefore will be somewhat less than <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>To correct for this, the Allied statisticians imagined a row of <span class="math notranslate nohighlight">\(N\)</span> spots for the serial numbers <span class="math notranslate nohighlight">\(1\)</span> through <span class="math notranslate nohighlight">\(N\)</span>, with marks at the spots corresponding to the observed serial numbers. The visualization below shows an outcome in the case <span class="math notranslate nohighlight">\(N= 20\)</span> and <span class="math notranslate nohighlight">\(n = 3\)</span>.</p>
<p><img alt="gaps" src="../../_images/all_gaps.png" /></p>
<ul class="simple">
<li><p>There are <span class="math notranslate nohighlight">\(N = 20\)</span> spots in all.</p></li>
<li><p>From these, we take a simple random sample of size <span class="math notranslate nohighlight">\(n = 3\)</span>. Those are the gold spots.</p></li>
<li><p>The remaining <span class="math notranslate nohighlight">\(N - n = 17\)</span> spots are colored blue.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(n = 3\)</span> sampled spots create <span class="math notranslate nohighlight">\(n+1 = 4\)</span> blue “gaps” between sampled values: one before the leftmost gold spot, two between successive gold spots, and one after the rightmost gold spot that is at position <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>A key observation is that because of the symmetry of simple random sampling, <strong>the lengths of all four gaps have the same distribution.</strong></p>
<p>But of course we don’t get to see all the gaps. In the sample, we can see all but the last gap, as in the figure below. The red question mark reminds you that the gap to the right of <span class="math notranslate nohighlight">\(M\)</span> is invisible to us.</p>
<p><img alt="mystery gap" src="../../_images/mystery_gap.png" /></p>
<p>If we could see the gap to the right of <span class="math notranslate nohighlight">\(M\)</span>, we would see <span class="math notranslate nohighlight">\(N\)</span>. But we can’t. So we can try to do the next best thing, which is to augment <span class="math notranslate nohighlight">\(M\)</span> by the estimated size of that gap.</p>
<p>Since we can see all of the spots and their colors up to and including <span class="math notranslate nohighlight">\(M\)</span>, we can see <span class="math notranslate nohighlight">\(n\)</span> out of the <span class="math notranslate nohighlight">\(n+1\)</span> gaps. The lengths of the gaps all have the same distribution by symmetry, so we can estimate the length of a single gap by the average length of all the gaps that we can see.</p>
<p>We can see <span class="math notranslate nohighlight">\(M\)</span> spots, of which <span class="math notranslate nohighlight">\(n\)</span> are the sampled values. So the total length of all <span class="math notranslate nohighlight">\(n\)</span> visible gaps is <span class="math notranslate nohighlight">\(M-n\)</span>. Therefore</p>
<div class="math notranslate nohighlight">
\[
\text{estimated length of one gap} ~ = ~ \frac{M-n}{n}
\]</div>
<p>So the Allied statisticians decided to improve upon <span class="math notranslate nohighlight">\(M\)</span> by using the <em>augmented maximum</em> as their estimator:</p>
<div class="math notranslate nohighlight">
\[
T_2 ~ = ~  M + \frac{M-n}{n}
\]</div>
<p>By algebra, this estimator can be rewritten as</p>
<div class="math notranslate nohighlight">
\[
T_2 ~ = ~ M\cdot\frac{n+1}{n} ~ - ~ 1
\]</div>
<p>Is <span class="math notranslate nohighlight">\(T_2\)</span> an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>? To answer this, we have to find its expectation. Since <span class="math notranslate nohighlight">\(T_2\)</span> is a linear function of <span class="math notranslate nohighlight">\(M\)</span>, we’ll find the expectation of <span class="math notranslate nohighlight">\(M\)</span> first.</p>
<p>Here once again is the visualization of what’s going on.</p>
<p><img alt="gaps" src="../../_images/all_gaps.png" /></p>
<p>Let <span class="math notranslate nohighlight">\(G\)</span> be the length of the last gap. Then <span class="math notranslate nohighlight">\(M = N - G\)</span>.</p>
<p>There are <span class="math notranslate nohighlight">\(n+1\)</span> gaps, made up of the <span class="math notranslate nohighlight">\(N-n\)</span> unsampled values. Since they all have the same expected length,</p>
<div class="math notranslate nohighlight">
\[
E(G) ~ = ~ \frac{N-n}{n+1}
\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
E(M) ~ = ~ N - \frac{N-n}{n+1} ~ = ~ (N+1)\frac{n}{n+1}
\]</div>
<p>Recall that the Allied statisticians’ estimate of <span class="math notranslate nohighlight">\(N\)</span> is</p>
<div class="math notranslate nohighlight">
\[
T_2 ~ = ~ M\cdot\frac{n+1}{n}  -  1
\]</div>
<p>Now</p>
<div class="math notranslate nohighlight">
\[
E(T_2) ~ = ~ E(M)\cdot\frac{n+1}{n}  -  1 ~ = ~ (N+1)\frac{n}{n+1}\cdot\frac{n+1}{n} - 1 ~ = ~ N
\]</div>
<p>Thus the augmented maximum <span class="math notranslate nohighlight">\(T_2\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<div class="admonition-quick-check admonition">
<p class="admonition-title">Quick Check</p>
<p>A gardener in Berkeley has 23 blue flower pots in a row. She picks a simple random sample of 5 of them and colors the selected pots gold. What is the expected number of blue flower pots at the end of the row?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(3\)</span></p>
</div>
</section>
<section id="which-estimator-to-use">
<h2><span class="section-number">8.4.8. </span>Which Estimator to Use?<a class="headerlink" href="#which-estimator-to-use" title="Link to this heading">#</a></h2>
<p>The Allied statisticians thus had two unbiased estimators of <span class="math notranslate nohighlight">\(N\)</span> from which to choose. They went with <span class="math notranslate nohighlight">\(T_2\)</span> instead of <span class="math notranslate nohighlight">\(T_1\)</span> because <span class="math notranslate nohighlight">\(T_2\)</span> has less variability.</p>
<p>We will quantify this later in the course. For now, here is a simulation of distributions of the two estimators in the case <span class="math notranslate nohighlight">\(N = 300\)</span> and <span class="math notranslate nohighlight">\(n=30\)</span>. The simulation is based on <span class="math notranslate nohighlight">\(5000\)</span> repetitions of drawing a simple random sample of size <span class="math notranslate nohighlight">\(30\)</span> from the integers <span class="math notranslate nohighlight">\(1\)</span> through <span class="math notranslate nohighlight">\(300\)</span>.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/bd8f9a14c231e07c8ad423ef5b4cfbc913fe1ebf24d69e5bfab4a91c4bbfa447.png" src="../../_images/bd8f9a14c231e07c8ad423ef5b4cfbc913fe1ebf24d69e5bfab4a91c4bbfa447.png" />
</div>
</div>
<p>You can see why <span class="math notranslate nohighlight">\(T_2\)</span> is a better estimator than <span class="math notranslate nohighlight">\(T_1\)</span>.</p>
<ul class="simple">
<li><p>Both are unbiased. So both the empirical histograms are balanced at around <span class="math notranslate nohighlight">\(300\)</span>, the true value of <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p>The emipirical distribution of <span class="math notranslate nohighlight">\(T_2\)</span> is clustered much closer to the true value <span class="math notranslate nohighlight">\(300\)</span> than the empirical distribution of <span class="math notranslate nohighlight">\(T_1\)</span>.</p></li>
</ul>
<p>For a recap, take another look at the <a class="reference external" href="https://en.wikipedia.org/wiki/German_tank_problem#Specific_data">accuracy table</a> of the Allied statisticians’ estimator <span class="math notranslate nohighlight">\(T_2\)</span>. Not bad for an estimator based on a model that assumes nothing more complicated than simple random sampling!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/Chapter_08"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_Expectations_of_Functions.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8.3. </span>Expectations of Functions</p>
      </div>
    </a>
    <a class="right-next"
       href="05_Method_of_Indicators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.5. </span>Method of Indicators</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additivity-of-expectation">8.4.1. Additivity of Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-x-2-for-a-poisson-variable-x">8.4.2. <span class="math notranslate nohighlight">\(E(X^2)\)</span> for a Poisson Variable <span class="math notranslate nohighlight">\(X\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-sum">8.4.3. Sample Sum</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-estimator">8.4.4. Unbiased Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-estimators-of-a-population-mean">8.4.5. Unbiased Estimators of a Population Mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-unbiased-estimator-of-a-maximum-possible-value">8.4.6. First Unbiased Estimator of a Maximum Possible Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#second-unbiased-estimator-of-the-maximum-possible-value">8.4.7. Second Unbiased Estimator of the Maximum Possible Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-estimator-to-use">8.4.8. Which Estimator to Use?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ani Adhikari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>