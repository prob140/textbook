---
redirect_from:
  - "/chapter-14/04-the-sample-mean"
interact_link: content/Chapter_14/04_The_Sample_Mean.ipynb
kernel_name: python3
kernel_path: content/Chapter_14
has_widgets: false
title: |-
  The Sample Mean
pagenum: 73
prev_page:
  url: /Chapter_14/03_Central_Limit_Theorem.html
next_page:
  url: /Chapter_14/05_Confidence_Intervals.html
suffix: .ipynb
search: sample mean n x distribution law sd mu size large bar population sum frac sigma sn epsilon let sqrt gold blue numbers vert limit distributions spread factor probability increases rolls normal text square root weak small p central data confidence empirical e graph because both times therefore estimator decreases result averages case not roughly theorem us based random samples thats very saw want construct intervals used recall section show any lets well our ldots xn d xi becomes below shows die using histogram four though larger average iid just linear transformation underlying matter unbiased doesnt called infty ge le laws

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">The Sample Mean</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Sample-Mean">The Sample Mean<a class="anchor-link" href="#The-Sample-Mean"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What's central about the Central Limit Theorem? One answer is that it allows us to make inferences based on random samples when we don't know much about the distribution of the population. For data scientists, that's very valuable.</p>
<p>In Data 8 you saw that if we want estimate the mean of a population, we can construct confidence intervals for the parameter based on the mean of a large random sample. In that course you used the bootstrap to generate an empirical distribution of the sample mean, and then used the empirical distribution to create the confidence interval. You will recall that those empirical distributions were invariably bell shaped.</p>
<p>In this section we will study the probability distribution of the sample mean and show that you can use it to construct confidence intervals for the population mean without any resampling.</p>
<p>Let's start with the sample sum, which we now understand well. Recall our assumptions and notation:</p>
<p>Let $X_1, X_2, \ldots, X_n$ be an i.i.d. sample, and let each $X_i$ have mean $\mu$ and $SD$ $\sigma$. Let $S_n$ be the sample sum, that is, $S_n = \sum_{i=1}^n X_i$. We know that</p>
$$
E(S_n) = n\mu ~~~~~~~~~~  SD(S_n) = \sqrt{n}\sigma
$$<p>These results imply that as the sample size increases, the distribution of the sample sum moves to the right and becomes more spread out.</p>
<p>You can see this in the graph below. The graph shows the distributions of the sum of 5 rolls and the sum of 20 rolls of a die. The distributions are exact, calculated using the function <code>dist_sum</code> defined using pgf methods earlier in this chapter.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">die</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
<span class="n">dist_sum_5</span> <span class="o">=</span> <span class="n">dist_sum</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">die</span><span class="p">)</span>
<span class="n">dist_sum_20</span> <span class="o">=</span> <span class="n">dist_sum</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">die</span><span class="p">)</span>
<span class="n">Plots</span><span class="p">(</span><span class="s1">&#39;Sum of 5 dice&#39;</span><span class="p">,</span> <span class="n">dist_sum_5</span><span class="p">,</span> <span class="s1">&#39;Sum of 20 dice&#39;</span><span class="p">,</span> <span class="n">dist_sum_20</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_14/04_The_Sample_Mean_2_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see the normal distribution appearing already for the sum of 5 and 20 dice.</p>
<p>The gold histogram is more spread out than the blue. Because both are probability histograms, both have 1 as their total area. So the gold histogram has to be lower than the blue.</p>
<p>Now let's look at the spread more carefully.</p>
<p>You can see also that the gold distribution isn't four times as spread out as the blue, though the sample size in the gold distribution is four times that of the blue. The gold distribution is twice as spread out (and therefore half as tall) as the blue. That is because the SD of the sum is proportional to $\sqrt{n}$. It grows slower than $n$. Because the sample size is larger by a factor of 4, the SD of the gold distribution is $\sqrt{4} = 2$ times the SD of the blue.</p>
<p>The <em>average</em> of the sample behaves differently.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Mean-of-an-IID-Sample">The Mean of an IID Sample<a class="anchor-link" href="#The-Mean-of-an-IID-Sample"> </a></h3><p>Let $\bar{X}_n$ be the sample mean, that is,</p>
$$
\bar{X}_n = \frac{S_n}{n}
$$<p>Then $\bar{X}_n$ is just a linear transformation of $S_n$. So</p>
$$
E(\bar{X}_n) = \frac{E(S_n)}{n} = \frac{n\mu}{n} = \mu ~~~~ \text{for all }n
$$<p>The expectation of the sample mean is always the underlying population mean $\mu$, no matter what the sample size. Therefore, no matter what the sample size, the sample mean is an unbiased estimator of the population mean.</p>
<p>The SD of the sample mean is</p>
$$
SD(\bar{X}_n) = \frac{SD(S_n)}{n} = \frac{\sqrt{n}\sigma}{n} = \frac{\sigma}{\sqrt{n}}
$$<p>The variability of the sample mean decreases as the sample size increases. So, as the sample size increases, the sample mean becomes a more accurate estimator of the population mean.</p>
<p>The graph below shows the distributions of the means of 5 rolls of a die and of 20 rolls. Both are centered at 3.5 but the distribution of the mean of the larger sample is narrower. You saw this frequently in Data 8: as the sample size increases, the distribution of the sample mean gets more concentrated around the population mean.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_14/04_The_Sample_Mean_5_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Square-Root-Law">Square Root Law<a class="anchor-link" href="#Square-Root-Law"> </a></h3><p>Accuracy doesn't come cheap. The SD of the sample mean decreases according to the square root of the sample size. Therefore if you want to decrease the SD of the sample mean by a factor of 3, you have to increase the sample size by a factor of $3^2 = 9$.</p>
<p>The general result is usually stated in the reverse.</p>
<h4 id="Square-Root-Law">Square Root Law<a class="anchor-link" href="#Square-Root-Law"> </a></h4><p>If you multiply the sample size by a factor, then the SD of the sample mean decreases by the square root of the factor.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weak-Law-of-Large-Numbers">Weak Law of Large Numbers<a class="anchor-link" href="#Weak-Law-of-Large-Numbers"> </a></h3><p>The sample mean is an unbiased estimator of the population mean and has a small SD when the sample size is large. So the mean of a large sample is close to the population mean with high probability.</p>
<p>The formal result is called the <em>Weak Law of Large Numbers</em>.</p>
<p>Let $X_1, X_2, \ldots, X_n$ be i.i.d., each with mean $\mu$ and SD $\sigma$, and let $\bar{X}_n$ be the sample mean. Fix any number $\epsilon &gt; 0$; it is best to imagine $\epsilon$ to be very small. Then</p>
$$
P(\vert \bar{X}_n - \mu \vert &lt; \epsilon) \to 1 ~~~ \text{as } n \to \infty
$$<p>That is, for large $n$ it is almost certain that the sample average is in the range $\mu \pm \epsilon$.</p>
<p>To prove the law, we will show that $P(\vert \bar{X}_n - \mu \vert \ge \epsilon) \to 0$. This is straightforward by Chebyshev's Inequality.</p>
$$
P(\vert \bar{X}_n - \mu \vert \ge \epsilon)~ \le ~ \frac{\sigma_{\bar{X}_n}^2}{\epsilon^2} 
~ = ~ \frac{\sigma^2}{n\epsilon^2} ~ \to ~ 0 ~~~ \text{as } n \to \infty
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Related-Laws">Related Laws<a class="anchor-link" href="#Related-Laws"> </a></h3><ul>
<li><strong>Strong Law of Large Numbers.</strong> This says that if you have a large iid then sample, then with probability 1 the running sample averages converge to a limit, and that limit is the constant $\mu$. See <a href="https://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/">this blog post by Fields Medalist Terence Tao</a>. He states the laws in the case where the underlying SDs may not exist. Note that our proof of the Weak Law is not valid in that case; the result is still true but the proof needs more care.</li>
<li><strong>Law of Small Numbers.</strong> This is the title of a book by <a href="https://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz">Ladislaus Bortkiewicz</a> (1868-1931) in which he described the Poisson approximation to distributions of rare events. That's why Section 6.4 of these notes is called the Law of Small Numbers.</li>
<li><strong>Law of Averages.</strong> This is a common name for the Weak Law in the case where the population is binary and the sample mean is just the proportion of successes in the sample. In common usage, people sometimes forget that the law is a limit statement. If you are tossing a fair coin and have seen 10 heads in a row, the chance that the next toss is a head is still 1/2. The law of averages does not say that you are "due for a tail". It doesn't apply to finite sets of tosses.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Shape-of-the-Distribution">The Shape of the Distribution<a class="anchor-link" href="#The-Shape-of-the-Distribution"> </a></h3><p>The Central Limit Theorem tells us that for large samples, the distribution of the sample mean is roughly normal. The sample mean is a linear transformation of the the sample sum. So if the distribution of the sample sum is roughly normal, the distribution of the sample mean is roughly normal as well, though with different parameters. Specifically, for large $n$,</p>
$$
P(\bar{X}_n \le x) ~ \approx ~ \Phi \big{(} \frac{x - \mu}{\sigma/\sqrt{n}} \big{)} ~~~~ \text{for all } x
$$
</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_14/04_The_Sample_Mean_10_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

 


    </main>
    