---
redirect_from:
  - "/chapter-10/00-markov-chains"
interact_link: content/Chapter_10/00_Markov_Chains.ipynb
kernel_name: python3
has_widgets: false
title: 'Chapter 10: Markov Chains'
prev_page:
  url: /Chapter_09/03_Expected_Waiting_Times.html
  title: 'Expected Waiting Times'
next_page:
  url: /Chapter_10/01_Transitions.html
  title: 'Transitions'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Markov-Chains">Markov Chains<a class="anchor-link" href="#Markov-Chains"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A <em>stochastic process</em> is a collection of random variables on a probability space. We will study a kind of process that evolves over <em>discrete time</em>, that is, random variables $X_0, X_1, X_2, \ldots $. Our image is that the process starts with value $X_0$ at time 0, and then takes steps at times 1, 2, and so on, with $X_n$ representing the value at time $n$.</p>
<p>We have already seen examples of such processes. For example, an i.i.d. sequence of Bernoulli $(p)$ trials forms such a process, going back and forth between the two values 0 and 1, each move independent of all the others. But in many interesting processes, the value of the process in the future depends on its present and past values. We can use the past and present to predict the future behavior of the process.</p>
<p>Markov Chains form a class of stochastic processes. They are named after  <a href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a> (1856-1922) whom you will encounter in several sections of this course. Informally, in a Markov Chain the distribution of the process in the future depends only on its present value, not on how it arrived at its present value. This is called the <em>Markov property.</em> Formally,</p>
<ul>
<li>For each $n \ge 1$, the conditional distribution of $X_{n+1}$ given $X_0, X_1, \ldots , X_n$ depends only on $X_n$.</li>
<li>That is, for every sequence of possible values $i_0, i_1, \ldots, i_n, i_{n+1}$,</li>
</ul>
$$ P(X_{n+1} = i_{n+1} \mid X_0 = i_0, X_1 = i_1 , \ldots, X_{n-1} = i_{n-1}, X_n = i_n) = P(X_{n+1} = i_{n+1} \mid X_n = i_n) $$<p>For example, consider a <em>random walk</em> where a gambler starts with a fortune of $a$ dollars for some positive integer $a$, and bets on successive tosses of a fair coin. If the coin lands heads he gains a dollar, and if it lands tails he loses a dollar.</p>
<p>Let $X_{0} = a$, and for $n &gt; 0$ let $X_{n+1} = X_n + I_n$ where $I_1, I_2, \ldots $ is an i.i.d. sequence of increments, each taking the value $+1$ or $-1$ with chance $1/2$. The Markov property holds for this process: given the gambler's fortune at time $n$, the distribution of his fortune at time $n+1$ doesn't depend on his fortune before time $n$. So the process $X_0, X_1, X_2, \ldots $ is a Markov Chain representing the evolution of the gambler's fortune over time.</p>
<p>The <em>state space</em> of a Markov Chain is the set of possible values of the random variables in the chain. The state space of the random walk described above is the set of all integers. In this course we will restrict the state space to be discrete and typically finite.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Conditional-Independence">Conditional Independence<a class="anchor-link" href="#Conditional-Independence"> </a></h3><p>Recall that two random variables $X$ and $Y$ are independent if the conditional distribution of $X$ given $Y$ is just the unconditional distribution of $X$.</p>
<p>Random variables $X$ and $Y$ are said to be <em>conditionally independent given $Z$</em> if the conditional distribution of $X$ given both $Y$ and $Z$ is just the conditional distribution of $X$ given $Z$ alone. That is, if you know $Z$, then additional knowledge about $Y$ doesn't change your opinion about $X$.</p>
<p>In a Markov Chain, if you define time $n$ to be the present, time $n+1$ to be the future, and times $0$ through $n-1$ to be the past, then the Markov property says that the past and future are conditionally independent given the present.</p>

</div>
</div>
</div>
</div>

 

